---
title: 6 日志存储
date: 2020-04-06
categories:
    - 存储
tags:
    - Kafka
---

Kafka 的日志
<!-- more -->

## 1. 日志概述
kafka 是基于日志的消息系统，今天我们就来介绍 kafka 日志存储相关的内容，包括:
1. 日志中消息的存储格式
2. 日志索引
3. 日志清理:
    - 日志的删除
    - 日志的压缩
4. 日志的磁盘存储

## 1. 文件目录布局
### 1.1 日志目录结构
Kafka 中的消息是以主题为基本单位进行归类的，各个主题在逻辑上相互独立，从主题开始以此依次向下分为:
1. 一个或多个分区，分区是逻辑上的概念
2. 每个分区对应多个副本
3. 一个副本对应一个日志(Log)
4. 为了防止 Log 过大，Kafka 将 Log 切分为多个 LogSegment 日志分段
5. Log 和LogSegment 也不是纯粹物理意义上的概念:
    - Log 在物理上只以文件夹的形式存储
    - 而每个LogSegment 对应于磁盘上的一个日志文件和两个索引文件，以及可能的其他文件(比如以`.txnindex`为后缀的事务索引文件)


![副本日志对应关系](/images/kafka/kafka_log.png)

主题、分区、副本、Log、LogSegment 之间的关系如上图所示:
1. Log 即每个分区的副本，对应是一个命名形式为`<topic>-<partition>`的文件夹
2. 每个LogSegment 对应于磁盘上的一个日志文件和两个索引文件，以及可能的其他文件
    - 日志文件(以`.log`为文件后缀)
    - 偏移量索引文件(以`.index`为文件后缀)
    - 时间戳索引文件(以`.timeindex`为文件后缀)
    - 还可能包含
        - .deleted，.cleaned，.swap等临时文件
        - .snapshot，.txnindex(事务索引文件)
        - leader-epoch-checkpoint 等文件
3. 日志写入: 
    - 向Log 中追加消息时是顺序写入的，只有最后一个 LogSegment 才能执行写入操作
    - 最后一个 LogSegment 称为`activeSegment`，即表示当前活跃的日志分段
    - 在创建主题的时候，如果当前 broker中不止配置了一个根目录，那么会挑选分区数最少的那个根目录来完成本次创建任务
4. 日志索引命名:
    - 每个 LogSegment 都有一个基准偏移量 baseOffset，用来表示当前 LogSegment中第一条消息的offset
    - 偏移量是一个64位的长整型数，日志文件和两个索引文件都是根据基准偏移量(baseOffset)命名的，名称固定为20位数字
    - Kafka 的每个日志对象中使用了ConcurrentSkipListMap来保存各个日志分段，每个日志分段的baseOffset作为key，这样可以根据指定偏移量来快速定位到消息所在的日志分段。

### 1.2 kafka 文件目录布局
从更加宏观的视角上看，kafka 的文件目录如下图所示:

![kafka_log_dir](/images/kafka/kafka_log_dir.png)

其中
1. 当 Kafka 服务第一次启动的时候，会在默认的根目录下创建以下5个文件：
    - log-start-offset-checkpoint:
    - cleaner-offset-checkpoint: 
    - recovery-point-offset-checkpoint: 表示已经刷写到磁盘的记录，recoveryPoint: 以下的数据都是已经刷到磁盘上的了
    - replication-offset-checkpoint: 用来存储每个replica的 HW 高水位
    - meta.properties: broker.id 信息
2. 第一次有消费者消费消息时:
    - 会创建内部主题 `__consumer_offsets` 用于保存消费者提交的位移
    - 图中的 `__consumer_offsets*` 文件夹就是对应的 Log 目录 
3. 创建主题时:
    - 如果当前 broker中不止配置了一个根目录，那么会挑选分区数最少的那个根目录来完成本次创建任务


## 2. 消息格式演进
Kafka的消息格式经历了3个版本：v0版本、v1版本和v2版本(如无特殊说明，下面都是消息未压缩的情形)。与消息对应的还有消息集的概念，消息集中包含一条或多条消息，消息集不仅是存储于磁盘及在网络上传输(Produce＆Fetch)的基本形式，而且是Kafka中压缩的基本单元。接下来我们会依次介绍 v0，v1，v2 三种消息的具体格式以及相应的消息集格式。

### 2.1 消息格式 v0
v0版本的消息及消息集格式如下:
![kafka_msg_v0](/images/kafka/kafka_msg_v0.png)

其中:
1. LOG_OVERHEAD: 固定为12B
    - offset 用来标志它在分区中的偏移量，这个offset是逻辑值，而非实际物理偏移值 8B
    - message size表示消息的大小 4B
2. RECORD: 最小大小为 4B+1B+1B+4B+4B=14B，不包括 key 和 value
    -  crc32(4B)：crc32校验值。校验范围为magic至value之间
    -  magic(1B)：消息格式版本号，此版本的magic值为0
    -  attributes(1B)：消息的属性。总共占1个字节，低3位表示压缩类型，其余位保留：
        -  0表示NONE
        -  1表示GZIP
        -  2表示SNAPPY
        -  3表示LZ4(LZ4自Kafka 0.9.x引入)
    -  key length(4B)：表示消息的key的长度，如果为-1，则表示没有设置key，即key=null
    -  key：可选，如果没有key则无此字段
    -  value length(4B)：实际消息体的长度，如果为-1，则表示消息为空。
    -  value：消息体，可以为空，比如墓碑(tombstone)消息

### 2.2 消息格式 v1
v1，比v0版本就多了一个timestamp字段，表示消息的时间戳。如下图所示:

![kafka_msg_v1](/images/kafka/kafka_msg_v1.png)

其中: 
1. magic字段的值为1
2. attributes:
    - 低3位和v0版本的一样，表示压缩类型
    - 第4个位(bit)：
        - 0表示timestamp类型为CreateTime，
        - 1表示timestamp类型为LogAppendTime
    - 其他位保留

timestamp类型由broker端参数 `log.message.timestamp.type` 来配置，默认值为CreateTime，即采用生产者创建消息时的时间戳。

如果在创建 ProducerRecord 时没有显式指定消息的时间戳，那么 KafkaProducer也会在发送这条消息前自动添加上。

v1 版本的RECORD 最小长度为 14 + 8 = 22B

### 2.3 v0/v1 格式的消息压缩
常见的压缩算法是数据量越大压缩效果越好，一条消息通常不会太大，这就导致压缩效果并不是太好。而Kafka实现的压缩方式是将多条消息一起进行压缩，这样可以保证较好的压缩效果。。在一般情况下，生产者发送的压缩数据在broker中也是保持压缩状态进行存储的，消费者从服务端获取的也是压缩的消息，消费者在处理消息之前才会解压消息，这样保持了**端到端的压缩**。

参数 `compression.type` 用来配置压缩方式，可选值包括:
1. 默认值为"producer"，表示保留生产者使用的压缩方式
2. gzip、snappy、lz4，分别对应 GZIP、SNAPPY、LZ4 这 3 种压缩算法
3. uncompressed，则表示不压缩

#### 压缩消息格式
当消息压缩时是将整个消息集进行压缩作为内层消息(inner message)，内层消息整体作为外层(wrapper message)的 value，消息压缩后的格式如下图所示:

![kafka_msg_zip](/images/kafka/kafka_msg_zip.png)

其中:
1. 压缩后的外层消息(wrapper message)中的key为null，
2. value字段中保存的是多条压缩消息(inner message，内层消息)
3. Record表示的是从 crc32 到 value 的消息格式
4. 当生产者创建压缩消息的时候，压缩消息消息集中的消息offset都是从0开始的，**对 offset 的转换是在服务端进行的，客户端不需要做这个工作**
5. 外层消息保存了内层消息中最后一条消息的绝对位移(absolute offset)，绝对位移是相对于整个分区而言的。当消费者消费这个消息集的时候，首先解压缩整个消息集，然后找到内层消息中最后一条消息的inner offset，然后依次计算出消息集中每条消息的 offset

#### 数据压缩的 timestamp 字段
v1版本比v0版的消息多了一个timestamp字段。对于压缩的情形，外层消息的timestamp设置为
- 如果timestamp类型是CreateTime，那么设置的是内层消息中最大的时间戳。
- 如果timestamp类型是LogAppendTime，那么设置的是Kafka服务器当前的时间戳

内层消息的timestamp设置：
- 如果外层消息的timestamp类型是CreateTime，那么设置的是生产者创建消息时的时间戳。
- 如果外层消息的timestamp类型是LogAppendTime，那么所有内层消息的时间戳都会被忽略。

对 attributes 字段而言，它的 timestamp 位只在外层消息中设置，内层消息中的timestamp类型一直都是CreateTime
。

注意: compact message是针对日志清理策略而言的(`cleanup.policy=compact`)，是指日志压缩(Log Compaction)后的消息。本节中的压缩消息单指compress message，即采用GZIP、LZ4等压缩工具压缩的消息。

### 2.4 消息格式 v2
v2 参考 Protocol Buffer 引入了变长整型(Varints)和ZigZag编码。目的是进入降低消息所占用的大小。具体的编码方式大家可以参考其他资料，这里不再详述。

v2 的消息格式如下图所示:
![kafka_msg_v2](/images/kafka/kafka_msg_v2.png)
其中:
1. Record Batch: 消息集
2. Record Batch Header: 从first offset到records count，这些字段是不被压缩的
3. records: 表示被压缩的消息
4. Record: 表示单条消息

生产者客户端中的 ProducerBatch 对应这里的 RecordBatch ，而 ProducerRecord 对应这里的Record。下面我们分别来讲解各个部分的含义

#### Record
Record 包含如下字段:
1. key、key length、value、value length字段同v0和v1版本的一样
2. length：消息总长度
3. attributes：弃用，但还是在消息格式中占据1B的大小，以备未来的格式扩展
4. timestamp delta：时间戳增量。通常一个timestamp需要占用8个字节，如果像这里一样保存与RecordBatch的起始时间戳的差值，则可以进一步节省占用的字节数。
5. offset delta：位移增量。保存与 RecordBatch起始位移的差值，可以节省占用的字节数
6. headers：
    - 用来支持应用级别的扩展，而不需要像v0和v1版本一样不得不将一些应用级别的属性值嵌入消息体
    - Header的格式如图最右部分所示，包含key和value
    - 一个Record里面可以包含0至多个Header
7. 在 v2 版本中将 crc 的字段从 Record 中转移到了RecordBatch中

对于 v1 版本的消息，如果用户指定的 timestamp 类型是 LogAppendTime 而不是CreateTime，那么消息从生产者进入 broker 后，timestamp 字段会被更新，此时消息的 crc值将被重新计算，而此值在生产者中已经被计算过一次。再者，broker 端在进行消息格式转换时（比mp 字段会被更新，此时消息的 crc值将被重新计算，而此值在生产者中已经被计算过一次。再者，broker 端在进行消息格式转换时（比如v1版转成v0版的消息格式）也会重新计算crc的值。在这些类似的情况下，消息从生产者到消费者之间流动时，crc的值是变动的，需要计算两次crc的值，所以这个字段的设计在 v0 和 v1 版本中显得比较“鸡肋”。在 v2 版本中将 crc 的字段从 Record 中转移到了RecordBatch中。

#### Record Batch
![kafka_msg_v2](/images/kafka/kafka_msg_v2.png)
Record Batch 包含如下字段:
1. first offset：表示当前RecordBatch的起始位移
2. length：计算从partition leader epoch字段开始到末尾的长度
3. partition leader epoch：分区leader纪元，可以看作分区leader的版本号或更新次数
4. magic：消息格式的版本号，对v2版本而言，magic等于2
5. attributes：消息属性，注意这里占用了两个字节。
    - 低3位表示压缩格式，可以参考v0和v1；
    - 第4位表示时间戳类型
    - 第5位表示此RecordBatch是否处于事务中，0表示非事务，1表示事务
    - 第6位表示是否是控制消息(ControlBatch)，0表示非控制消息，而1表示是控制消息，控制消息用来支持事务功能
6. last offset delta：
    - RecordBatch中最后一个Record的offset与first offset的差值
    - 主要被broker用来确保RecordBatch中Record组装的正确性
7. first timestamp：RecordBatch中第一条Record的时间戳
8. max timestamp：RecordBatch 中最大的时间戳，一般情况下是指最后一个 Record的时间戳，和last offset delta的作用一样，用来确保消息组装的正确性。
9. producer id：PID，用来支持幂等和事务
10. producer epoch：和producer id一样，用来支持幂等和事务
11. first sequence：和 producer id、producer epoch 一样，用来支持幂等和事务
12. records count：RecordBatch中Record的个数。

v2版本的消息不仅提供了更多的功能，比如事务、幂等性等，某些情况下还减少了消息的空间占用，总体性能提升很大。使用kafka-dump-log.sh 可以查看kafka日志的内容来验证所使用的消息格式。命令如下:

```bash
> bin/kafka-dump-log.sh --files /tmp/kafka/topic-0/0000000000000.log --print-data-log
```
kafka-dump-log.sh脚本也可以用来解析.index文件(还包括.timeindex、.snapshot、.txnindex等文件)

## 2. 日志索引
kafka 日志的内容我们在上一节 kafka 消息格式已经详细介绍过了。接下来我们来看看，kafka 的两个索引文件: 
1. 偏移量索引
2. 时间戳索引

### 2.1 kafka 索引简介
偏移量索引文件用来建立消息偏移量(offset)到物理地址之间的映射关系，方便快速定位消息所在的物理文件位置；时间戳索引文件则根据指定的时间戳(timestamp)来查找对应的偏移量信息。

Kafka 中的索引文件以稀疏索引(sparse index)的方式构造消息的索引，它并不保证每个消息在索引文件中都有对应的索引项。每当写入一定量(由 broker 端参数 `log.index.interval.bytes`指定，默认值为4096，即4KB)的消息时，偏移量索引文件和时间戳索引文件分别增加一个偏移量索引项和时间戳索引项，增大或减小log.index.interval.bytes的值，对应地可以增加或缩小索引项的密度。

稀疏索引通过MappedByteBuffer将索引文件映射到内存中，以加快索引的查询速度。偏移量索引文件中的偏移量是单调递增的，查询指定偏移量时，使用二分查找法来快速定位偏移量的位置，如果指定的偏移量不在索引文件中，则会返回小于指定偏移量的最大偏移量。时间戳索引文件中的时间戳也保持严格的单调递增，查询指定时间戳时，也根据二分查找法来查找不大于该时间戳的最大偏移量，至于要找到对应的物理文件位置还需要根据偏移量索引文件来进行再次定位。稀疏索引的方式是在磁盘空间、内存空间、查找时间等多方面之间的一个折中。

### 2.2 日志分段触发点
日志分段文件达到一定的条件时需要进行切分，那么其对应的索引文件也需要进行切分。日志分段文件切分包含以下几个条件，满足其一即可。
1. 当前**日志分段文件的大小**超过了 broker 端参数 log.segment.bytes 配置的值。log.segment.bytes参数的默认值为1073741824，即1GB。
2. 当前**日志分段中消息的最大时间戳**与当前系统的时间戳的差值大于 log.roll.ms或log.roll.hours参数配置的值。如果同时配置了log.roll.ms和log.roll.hours参数，那么log.roll.ms的优先级高。默认情况下，只配置了log.roll.hours参数，其值为168，即7天。
3. 偏移量索引文件或时间戳**索引文件的大小达**到broker端参数log.index.size.max.bytes配置的值。log.index.size.max.bytes的默认值为10485760，即10MB
4. 追加的消息的偏移量与当前日志分段的**偏移量之间的差值**大于Integer.MAX_VALUE，即要追加的消息的偏移量不能转变为相对偏移量(offset-baseOffset＞Integer.MAX_VALUE)

对非当前活跃的日志分段而言，其对应的索引文件内容已经固定而不需要再写入索引项，所以会被设定为只读。而对当前活跃的日志分段(activeSegment)而言，索引文件还会追加更多的索引项，所以被设定为可读写。

在索引文件切分的时候，Kafka 会关闭当前正在写入的索引文件并置为只读模式，同时以可读写的模式创建新的索引文件，索引文件的大小由broker端参数 log.index.size.max.bytes 配置。Kafka **在创建索引文件的时候会为其预分配log.index.size.max.bytes 大小**的空间，注意这一点与日志分段文件不同，只有当索引文件进行切分的时候，Kafka 才会把该索引文件裁剪到实际的数据大小。也就是说，与当前活跃的日志分段对应的索引文件的大小固定为 log.index.size.max.bytes，而其余日志分段对应的索引文件的大小为实际的占用空间。

### 2.3 偏移量索引
```bash
relative Offset| position
    32         |    32 
```
偏移量索引项的格式如上所示。每个索引项占用8个字节，分为两个部分。
1. relativeOffset：
    - 相对偏移量，表示消息相对于baseOffset 的偏移量，占用4 个字节，当前索引文件的文件名即为baseOffset的值
    - 消息的偏移量(offset)占用8个字节，也可以称为绝对偏移量
2. position：物理地址，也就是消息在日志分段文件中对应的物理位置，占用4个字节

可以使用 kafka-dump-log.sh脚本来解析.index文件内容:

```bash
bin/kafka-dump-log.sh --files /tmp/kafka-logs/topic-log-0/000000000000000.index
    Dumping /tmp/kafka-logs/topic-log-0/000000000000000.index
    offset: 6 position 156
```

#### 消息索引过程
000000000000000.index 偏移量索引与 000000000000000.log 日志分段文件的对应关系如下:

![delete_by_time](/images/kafka/offset_index.png)


日志查找分成了两个过程:
1. 查找偏移量所在的日志分段:
    - Kafka 的每个日志对象中使用了ConcurrentSkipListMap来保存各个日志分段，每个日志分段的baseOffset作为key，这样可以根据指定偏移量来快速定位到消息所在的日志分段。
2. 根据日志分段的偏移量索引查找消息:
    - 首先通过二分法在偏移量索引文件中找到不大于偏移量的最大索引项的 position
    - 根据索引项中的position定位到具体的日志分段文件位置开始查找目标消息


Kafka 强制要求索引文件大小必须是索引项大小的整数倍，对偏移量索引文件而言，必须为8的整数倍。如果broker端参数log.index.size.max.bytes配置为67，那么Kafka在内部会将其转换为64，即不大于67，并且满足为8的整数倍的条件。

### 2.4 时间戳索引
```bash
timestamp      | relative Offset
    32         |    32 
```

每个索引项占用12个字节，分为两个部分。
1. timestamp：当前日志分段最大的时间戳
2. relativeOffset：时间戳所对应的消息的相对偏移量

每个追加的时间戳索引项中的 timestamp 必须大于之前追加的索引项的 timestamp，否则不予追加
1. 如果 broker 端参数 log.message.timestamp.type设置为LogAppendTime，那么消息的时间戳必定能够保持单调递增；
2. 相反如果是 CreateTime 类型则无法保证。生产者可以使用类似 ProducerRecord(String topic，Integer partition，Long timestamp，K key，V value)的方法来指定时间戳的值。

即使生产者客户端采用自动插入的时间戳也无法保证时间戳能够单调递增，如果两个不同时钟的生产者同时往一个分区中插入消息，那么也会造成当前分区的时间戳乱序。

与偏移量索引文件相似，时间戳索引文件大小必须是索引项大小(12B)的整数倍，如果不满足条件也会进行裁剪。同样假设broker端参数`log.index.size.max.bytes`配置为67，那么对应于时间戳索引文件，Kafka在内部会将其转换为60。

#### 索引文件的追加方式

我们已经知道每当写入一定量的消息时，就会在偏移量索引文件和时间戳索引文件中分别增加一个偏移量索引项和时间戳索引项。两个文件增加索引项的操作是同时进行的，但并不意味着偏移量索引中的relativeOffset和时间戳索引项中的relativeOffset是同一个值。

需要注意的是即便日志索引内的时间戳不是单调递增的，也不会影响日志查找的准确性，因为每个追加的时间戳索引项中的 timestamp 必须大于之前追加的索引项的 timestamp，这就保证每个时间索引内的时间戳，在日志文件内，位于它之前的日志时间都小于这个时间戳，之后的可能大于也可能小于这个时间戳。这样总是能找到大于该时间戳的所有数据。

#### 查找步骤
![kafka_time_index](/images/kafka/kafka_time_index.png)

如图所示查找 targetTimeStamp=1526384718288开始的消息，分为如下几个步骤:
1. 首先找到不小于指定时间戳的日志分段
    - 将targetTimeStamp和每个日志分段中的最大时间戳largestTimeStamp逐一对比，直到找到不小于 targetTimeStamp 的 largestTimeStamp 所对应的日志分段。
    - 日志分段中的largestTimeStamp的计算是先查询该日志分段所对应的时间戳索引文件，找到最后一条索引项，若最后一条索引项的时间戳字段值大于0，则取其值，否则取该日志分段的最近修改时间。
2. 根据时间戳索引偏移量查找消息:
    - 找到相应的日志分段之后，在时间戳索引文件中使用二分查找算法查找到不大于targetTimeStamp的最大索引项，即[1526384718283，28]，如此便找到了一个相对偏移量28
    - 在偏移量索引文件中使用二分算法查找到不大于28的最大索引项，即[26，838]
    - 从步骤1中找到日志分段文件中的838的物理位置开始查找不小于targetTimeStamp的消息


## 3. 日志清理
将日志分段就是便于日志清理，Kafka提供了两种日志清理策略:
1. 日志删除(Log Retention)：按照一定的保留策略直接删除不符合条件的日志分段
2. 日志压缩(Log Compaction)：针对每个消息的key进行整合，对于有相同key的不同value值，只保留最后一个版本

与此相关的 broker 端参数包括:

log.cleanup.policy: 
- 作用: 设置日志清理策略
- 可选值包括:
    - delete: 默认值，采用日志删除的清理策略
    - compact: 采用日志压缩的清理策略，此时还需要将log.cleaner.enable(默认值为true)设定为true
    - delete，compact: 同时支持日志删除和日志压缩两种策略
- 主题相关参数:
    - 日志清理的粒度可以控制到主题级别，比如与log.cleanup.policy 对应的主题级别的参数为 cleanup.policy

log.cleaner.enable
- 作用: 启用日志压缩

### 3.1 日志删除
在Kafka的日志管理器中会有一个专门的日志删除任务来周期性地检测和删除不符合保留条件的日志分段文件，这个周期可以通过broker端参数 `log.retention.check.interval.ms` 来配置，默认值为 300000 即 5 分钟。

当前日志分段的保留策略有3种：
1. 基于时间的保留策略
2. 基于日志大小的保留策略
3. 基于日志起始偏移量的保留策略

#### 基于时间的保留策略
日志删除任务会检查当前**日志文件中是否有保留时间超过设定的阈值**(retentionMs)来寻找可删除的日志分段文件集合(deletableSegments)。

retentionMs可以通过broker端以下参数设置:
1. log.retention.hours，优先级最高
2. log.retention.minutes
3. log.retention.ms，优先级最低
4. 默认情况下只配置了log.retention.hours参数，其值为168，默认情况下日志分段文件的保留时间为7天


![delete_by_time](/images/kafka/delete_by_time.png)

删除任务会根据日志分段中最大的时间戳 largestTimeStamp 来查找过期的日志分段，largestTimeStamp 的确定分为如下几个步骤:
1. 查询该日志分段所对应的时间戳索引文件，查找时间戳索引文件中最后一条索引项
2. 若最后一条索引项的时间戳字段值大于 0，则取其值，否则设置为最近修改时间lastModifiedTime
3. 使用文件的最后的修改时间是不准确的，因为这些文件的修改时间可能会被认为修改

日志分段删除步骤如下:
1. 如果所有的日志分段都过期，那至少要保证有一个活跃的日志分段，在此种情况下，会先切分出一个新的日志分段作为activeSegment，然后执行删除操作
2. 删除日志分段时，首先会从Log对象中所维护日志分段的跳跃表中移除待删除的日志分段，以保证没有线程对这些日志分段进行读取操作
3. 然后将日志分段所对应的所有文件添加上`.deleted`的后缀(当然也包括对应的索引文件)
4. 最后交由一个以`delete-file`命名的延迟任务来删除这些以`.deleted`为后缀的文件，这个任务的延迟执行时间可以通过file.delete.delay.ms参数来调配，此参数的默认值为60000，即1分钟

#### 基于日志大小
基于日志大小的删除策略通过**检查当前日志的大小是否超过设定的阈值**(retentionSize)来寻找可删除的日志分段的文件集合(deletableSegments)。
retentionSize 由以下参数控制:

log.retention.bytes
- 作用: 配置Log中所有日志文件的总大小，默认值为-1，表示无穷大

log.segment.bytes:
- 作用: 配置单个日志分段(确切地说应该为.log日志文件)的大小，默认值为1073741824，即1GB。

![delete_by_size](/images/kafka/delete_by_size.png)

查找可删除日志分段集合:
1. 首先计算日志文件的总大小size和retentionSize的差值diff，即计算需要删除的日志总大小
2. 然后从日志文件中的第一个日志分段开始进行查找可删除的日志分段的文件集合
3. 查找出 deletableSegments 之后就执行删除操作，删除操作同上


#### 基于偏移量
一般情况下，日志文件的起始偏移量 logStartOffset 等于第一个日志分段的 baseOffset，但这并不是绝对的，logStartOffset 的值可以通过 DeleteRecordsRequest 请求(比如使用KafkaAdminClient的deleteRecords()方法、使用kafka-delete-records.sh脚本，具体用法参考9.1.3节)、日志的清理和截断等操作进行修改。

![delete_by_offset](/images/kafka/delete_by_offset.png)

基于日志起始偏移量的保留策略的判断依据是**某日志分段的下一个日志分段**的起始偏移量baseOffset 是否小于等于logStartOffset。


### 3.2 日志压缩
Log Compaction对于有相同key的不同value值，只保留最后一个版本。如果应用只关心key对应的最新value值，则可以开启Kafka的日志清理功能，Kafka会定期将相同key的消息进行合并，只保留最新的value值。

Log Compaction会生成新的日志分段文件，日志分段中每条消息的物理位置会重新按照新文件来组织。Log Compaction执行过后的偏移量不再是连续的，不过这并不影响日志的查询。

Log Compaction是针对key的，所以在使用时应注意每个消息的key值不为null。Kafka中用于保存消费者消费位移的主题__consumer_offsets使用的就是Log Compaction策略。

#### 压缩方法
kafka 每一个日志目录下都有一个名为 `cleaner-offset-checkpoint` 的文件，这个文件就是清理检查点文件，用来记录每个主题的**每个分区中已清理的偏移量**。通过清理检查点文件中的检查点cleaner checkpoint 将日志划分为:
1. 已经清理过的clean部分，消息偏移量是断续递增的
2. 一个还未清理过的 dirty 部分，消息偏移量是逐一递增的
3. 如果客户端总能赶上dirty部分，那么它就能读取日志的所有消息，反之就不可能读到全部的消息。

![log_compaction](/images/kafka/log_compaction.png)
1. firstDirtyOffset(与cleaner checkpoint相等)表示dirty部分的起始偏移量
2. 为了避免当前活跃的日志分段activeSegment成为热点文件，activeSegment 不会参与 Log Compaction 的执行
3. firstUncleanableOffset为dirty部分的截止偏移量，整个dirty部分的偏移量范围为[firstDirtyOffset，firstUncleanableOffset)，注意这里是左闭右开区间。
4. Kafka 支持通过参数log.cleaner.min.compaction.lag.ms(默认值为0)来配置消息在被清理前的最小保留时间，默认情况下firstUncleanableOffset等于activeSegment的baseOffset。

#### 压缩策略
每个broker会启动`log.cleaner.thread`(默认值为1)个日志清理线程负责执行清理任务，这些线程会选择`污浊率`最高的日志文件进行清理:
1. cleanBytes 表示clean部分的日志占用大小
2. dirtyBytes 表示dirty部分的日志占用大小
3. 日志的污浊率(dirtyRatio)为：dirtyBytes = dirtyBytes/ (dirtyBytes + cleanBytes)

`log.cleaner.min.cleanable.ratio`(默认值为0.5)来限定可进行清理操作的最小污浊率。

#### 压缩实现
日志压缩是通过 SkimpyOffsetMap 实现的:
1. Kafka中的每个日志清理线程会使用一个名为` SkimpyOffsetMap `的对象来构建 key与offset 的映射关系的哈希表
2. 日志清理需要遍历两次日志文件，第一次遍历把每个key的哈希值和最后出现的offset都保存在SkimpyOffsetMap中
3. 第二次遍历会检查每个消息是否符合保留条件，如果符合就保留下来，否则就会被清理

SkimpyOffsetMap使用MD5来计算key的哈希值，使用线性探测解决哈希冲突，通过 broker 端参数 `log.cleaner.io.buffer.load.factor`(默认值为0.9)来调整负载因子。每个日志清理线程的SkimpyOffsetMap的内存占用大小为`log.cleaner.dedupe.buffer.size/log.cleaner.thread`，默认值为=128MB/1=128MB。

需要注意的是 SkimpyOffsetMap 中保存的是 key md5 之后的值与 offset 的对应关系。如果遇到两个不同的 key但哈希值相同的情况，那么其中一个key所对应的消息就会丢失。(如果保存的 MD5 后的哈希值何来哈希冲突检测一说？)

#### 压缩分组
Kafka 在实际清理过程中并不对单个的日志分段进行单独清理，而是将日志文件中offset从0至firstUncleanableOffset的所有日志分段进行分组，每个日志分段只属于一组，分组策略为：按照日志分段的顺序遍历，每组中日志分段的占用空间大小之和不超过segmentSize(可以通过broker端参数`log.segment.bytes`设置，默认值为1GB)，且对应的索引文件占用大小之和不超过 maxIndexSize(可以通过broker 端参数 `log.index.interval.bytes`设置，默认值为10MB)。同一个组的多个日志分段清理过后，只会生成一个新的日志分段。

![执行日志压缩](/images/kafka/log_compact_group.png)


#### 压缩过程
Log Compaction过程:
1. 首先将每个日志分组中需要保留的消息复制到一个以`.clean`为后缀的临时文件中，此临时文件以当前日志分组中第一个日志分段的文件名命名，例如 00000000000000000000.log.clean。
2. Log Compaction过后将 .clean 的文件修改为 .swap 后缀的文件，例如：00000000000000000000.log.swap
3. 然后删除原本的日志文件，最后才把文件的`.swap`后缀去掉
4. 整个过程中的索引文件的变换也是如此

## 4. 磁盘存储
kafka 高吞吐量依赖以下几个技术:
1. 顺序写: Kafka 在设计时采用了文件追加的方式来写入消息，即只能在日志文件的尾部追加新的消息，并且也不允许修改已写入的消息
2. 充分利用页缓存: 
    - 避免 java 应用层的缓存，减少内存开销和 Java GC 的影响
    - kafka 写日志只写到文件系统的页缓存中，数据落盘由操作系统刷脏页控制
    - kafka 提供了 log.flush.interval.messages、log.flush.interval.ms 参数用于控制同步刷盘的频率
    - 刷盘任务应交由操作系统去调配，消息的可靠性应该由多副本机制来保障，而不是由同步刷盘这种严重影响性能的行为来保障
    - 操作系统使用 vm.swappiness 参数用于控制内存的交换倾向，越大表示越倾向于使用 swap 分区来释放内存，swap 分区导致严重性能问题，应极力避免，建议将此参数设置为 ，这样保留了swap的机制而又最大限度地限制了它对Kafka性能的影响
    - 因为大量使用页缓存，kafka 重启和迭机并不会导致数据丢失，但是主机的迭机有可能导致数据丢失
3. sendfile: 提高数据的网络发送效率

对一个进程而言，它会在进程内部缓存处理所需的数据，然而这些数据有可能还缓存在操作系统的页缓存中，因此同一份数据有可能被缓存了两次并且，除非使用Direct I/O的方式，否则页缓存很难被禁止。此外，用过Java的人一般都知道两点事实：对象的内存开销非常大，通常会是真实数据大小的几倍甚至更多，空间使用率低下；Java的垃圾回收会随着堆内数据的增多而变得越来越慢。基于这些因素，使用文件系统并依赖于页缓存的做法明显要优于维护一个进程内缓存或其他结构，至少我们可以省去了一份进程内部的缓存消耗，同时还可以通过结构紧凑的字节码来替代使用对象的方式以节省更多的空间。如此，我们可以在 32GB 的机器上使用28GB至30GB的内存而不用担心GC所带来的性能问题。此外，即使Kafka服务重启，页缓存还是会保持有效，然而进程内的缓存却需要重建。这样也极大地简化了代码逻辑，因为维护页缓存和文件之间的一致性交由操作系统来负责，这样会比进程内维护更加安全有效。

### 4.1 操作系统的脏页控制
Linux 中有如下参数用于控制系统刷脏页
1. vm.dirty_background_ratio: 指定当脏页数量达到系统内存的百分之多少之后就会触发 pdflush/flush/kdmflush 等后台回写进程的运行来处理脏页，一般设置为小于10的值即可，但不建议设置为0
2. vm.dirty_ratio: 指定当脏页数量达到系统内存的百分之多少之后就不得不开始对脏页进行处理，在此过程中，新的 I/O 请求会被阻挡直至所有脏页被冲刷到磁盘中
3. vm.dirty_expire_centisecs
4. vm.dirty_writeback.centisecs


### 4.2 kafka 磁盘 IO 过程

![执行日志压缩](/images/kafka/disk_io.png)

从编程角度而言，一般磁盘I/O的场景有以下四种。
1. 用户调用标准C库进行I/O操作，数据流为：应用程序buffer→C库标准IObuffer→文件系统页缓存→通过具体文件系统到磁盘。
2. 用户调用文件 I/O，数据流为：应用程序 buffer→文件系统页缓存→通过具体文件系统到磁盘。
3. 用户打开文件时使用O_DIRECT，绕过页缓存直接读写磁盘
4. 用户使用类似dd工具，并使用direct参数，绕过系统cache与文件系统直接写磁盘


发起I/O请求的步骤可以表述为如下的内容（以最长链路为例）:
1. 写操作：用户调用fwrite把数据写入C库标准IObuffer后就返回，即写操作通常是异步操作；数据写入C库标准IObuffer后，不会立即刷新到磁盘，会将多次小数据量相邻写操作先缓存起来合并，最终调用write函数一次性写入（或者将大块数据分解多次write 调用）页缓存；数据到达页缓存后也不会立即刷新到磁盘，内核有 pdflush 线程在不停地检测脏页，判断是否要写回到磁盘，如果是则发起磁盘I/O请求。
2. 读操作：用户调用fread到C库标准IObuffer中读取数据，如果成功则返回，否则继续；到页缓存中读取数据，如果成功则返回，否则继续；发起 I/O 请求，读取数据后缓存buffer和C库标准IObuffer并返回。可以看出，读操作是同步请求。
3. I/O请求处理：通用块层根据I/O请求构造一个或多个bio结构并提交给调度层；调度器将 bio 结构进行排序和合并组织成队列且确保读写操作尽可能理想：将一个或多个进程的读操作合并到一起读，将一个或多个进程的写操作合并到一起写，尽可能变随机为顺序（因为随机读写比顺序读写要慢），读必须优先满足，而写也不能等太久。

### 4.3 磁盘 IO 调度策略
针对不同的应用场景，I/O调度策略也会影响I/O的读写性能，目前Linux系统中的I/O调度策略有4种，分别为NOOP、CFQ、DEADLINE和 ANTICIPATORY，默认为CFQ。

#### NOOP
该算法实现了最简单的FIFO队列，所有I/O请求大致按照先来后到的顺序进行操作。之所以说“大致”，原因是NOOP在FIFO的基础上还做了相邻I/O请求的合并，并不是完全按照先进先出的规则满足I/O请求。

#### CFQ
该算法的特点是按照I/O请求的地址进行排序，而不是按照先来后到的顺序进行响应:
1. CFQ为每个进程单独创建一个队列来管理该进程所产生的请求
2. 各队列之间的调度使用时间片进行调度，
3. I/O调度器每次执行一个进程的4次请求。

CFQ的出发点是对I/O地址进行排序，以尽量少的磁盘旋转次数来满足尽可能多的I/O请求。在CFQ算法下，SAS盘的吞吐量大大提高了。相比于NOOP的缺点是，先来的I/O请求并不一定能被满足，可能会出现“饿死”的情况。

#### DEADLINE
DEADLINE在CFQ的基础上，解决了I/O请求“饿死”的极端情况。除了CFQ本身具有的I/O排序队列，DEADLINE额外分别为读I/O和写I/O提供了FIFO队列。优先级可以表示如下

```bash
FIFO(read) > FIFO(write) > CFQ
```

读FIFO队列的最大等待时间为500ms，写FIFO队列的最大等待时间为5s。

#### ANTICIPATORY
ANTICIPATORY在DEADLINE的基础上，为每个读I/O都设置了6ms的等待时间窗口。如果在6ms内OS收到了相邻位置的读I/O请求，就可以立即满足。ANTICIPATORY算法通过增加等待时间来获得更高的性能，通过将多个随机的小写入流合并成一个大写入流（相当于将随机读写变顺序读写），通过这个原理来使用读取/写入的延时换取最大的读取/写入吞吐量。适用于大多数环境，特别是读取/写入较多的环境。

Kafka 操作的都是普通文件，并没有依赖于特定的文件系统，但是依然推荐使用EXT4或XFS。尤其是对XFS而言，它通常有更好的性能

### 4.4 零拷贝
零拷贝是指将数据直接从磁盘文件复制到网卡设备，而无需经过应用程序。零拷贝技术依赖于底层的 sendfile（）方法实现。

#### 正常的发送文件过程

![正常发送文件](/images/kafka/send_file_base.jpg)

正常我们读取一个文件并通过网络发送出去要经过如下四个过程:
1. 调用read（）时，文件A中的内容被复制到了内核模式下的Read Buffer中。
2. CPU控制将内核模式数据复制到用户模式下
3. 调用write（）时，将用户模式下的内容复制到内核模式下的Socket Buffer中
4. 将内核模式下的Socket Buffer的数据复制到

数据平白无故地从内核模式到用户模式“走了一圈”，浪费了 2次复制过程：
1. 第一次是从内核模式复制到用户模式；
2. 第二次是从用户模式再复制回内核模式，即上面4次过程中的第2步和第3步
3. 而且在上面的过程中，内核和用户模式的上下文的切换也是4次

#### 零拷贝过程
![正常发送文件](/images/kafka/send_file_direct.jpg)

零拷贝技术，那么应用程序可以直接请求内核把磁盘中的数据传输给 Socket:
1. 零拷贝技术通过DMA（Direct Memory Access）技术将文件内容复制到内核模式下的Read Buffer 中
2. 不过没有数据被复制到 Socket Buffer，相反只有包含数据的位置和长度的信息的文件描述符被加到Socket Buffer中
3. DMA引擎直接将数据从内核模式中传递到网卡设备（协议引擎）

这里数据只经历了2次复制就从磁盘中传送出去了，并且上下文切换也变成了2次。