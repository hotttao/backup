---
weight: 1
title: "LLM 的基础概念"
date: 2025-08-20T08:00:00+08:00
lastmod: 2025-08-20T08:00:00+08:00
draft: false
author: "宋涛"
authorLink: "https://hotttao.github.io/"
description: "LLM 的基础概念"
featuredImage: 

tags: ["LLM"]
categories: ["LLM"]

lightgallery: true

toc:
  auto: false
---

这个系列，我们介绍 LLM 的基础概念。

## 1. 预训练和微调
大语言模型的构建通常包括预训练（pre-training）和微调（fine-tuning）两个阶段。

预训练:
1. 大语言模型的预训练目标是在大量无标注的文本语料库（原始文本）上进行下一单词预测。
2. “预”表明它是模型训练的初始阶段，此时模型会在大规模、多样化的数据集上进行训练，以形成全面的语言理解能力。
3. 在此阶段，大语言模型使用自监督学习，模型从输入数据中生成自己的标签。
4. 预训练后的大语言模型通常称为基础模型（foundation model）。

微调:
1. 预训练完成后，可以使用较小的带标注的数据集对大语言模型进行微调
2. 以预训练模型为基础，微调阶段会在规模较小的特定任务或领域数据集上对模型进行针对性训练，以进一步提升其特定能力。
3. 微调大语言模型最流行的两种方法是指令微调和分类任务微调。
    - 在指令微调（instruction fine-tuning）中，标注数据集由“指令−答案”对（比如翻译任务中的“原文−正确翻译文本”）组成。
    - 在分类任务微调（classification fine-tuning）中，标注数据集由文本及其类别标签（比如已被标记为“垃圾邮件”或“非垃圾邮件”的电子邮件文本）组成。

![预训练和微调](/images/llm/预训练和微调.png)

## 2. Transformer 架构

![transformer](/images/llm/transformer.png)

原始 Transformer 架构是一种用于机器翻译的深度学习模型。Transformer 由两部分组成：
1. **编码器**，用于处理输入文本并生成文本嵌入（一种能够在不同维度中捕获许多不同因素的数值表示）；
2. **解码器**，用于使用这些文本嵌入逐词生成翻译后的文本。

请注意，图中展示的是翻译过程的最后阶段，此时解码器根据原始输入文本（“This is an example”）和部分翻译的句子（“Das ist ein”），生成最后一个单词（“Beispiel”）以完成翻译

以翻译任务为例，编码器将源语言的文本编码成向量，解码器则解码这些向量以生成目标语言的文本。编码器和解码器都是由多层组成，这些层通过自注意力机制连接。

### 2.1 自注意力机制
**自注意力机制**（**self-attention mechanism**），它允许模型衡量序列中不同单词或词元之间的相对重要性。这一机制使得模型能够捕捉到输入数据中长距离的依赖和上下文关系，从而提升其生成连贯且上下文相关的输出的能力。

### 2.2 变体
为了适应不同类型的下游任务，Transformer 有如下变体:
1. **BERT**: 
    - Bidirectional Encoder Representations from Transformer(双向编码预训练 Transformer)
    - 专注于**掩码预测**（masked word prediction），即预测给定句子中被掩码的词。
    - BERT 在情感预测、文档分类等文本分类任务中具有优势。
2. **GPT**: 
    - Generative Pretrained Transformer(生成式预训练 Transformer)
    - 侧重于原始 Transformer 架构的解码器部分，主要被设计和训练用于文本补全（text completion）任务
    - 类 GPT 大语言模型，主要用于生成任务和生成文本序列，但它们表现出了出色的可扩展性。这些模型擅长执行零样本学习任务和少样本学习任务

![BERT](/images/llm/BERT.png)


## 3. GPT 架构

与原始 Transformer 架构相比，GPT 的通用架构更为简洁。本质上，它只包含解码器部分，并不包含编码器。GPT 架构仅使用原始的 Transformer 解码器部分。它被设计为单向的从左到右处理，这使得它非常适合文本生成和下一单词预测任务，可以逐个词地迭代生成文本。

![GPT 架构](/images/llm/gpt.png)

GPT 完成的是下一单词预测任务:
1. 系统通过观察之前的词来学习预测句子中的下一个词。这种方法能够帮助模型理解词语和短语在语言中的常见组合，从而为应用于各种其他任务奠定基础。
2. 采用的**自监督学习**（self-supervised learning）模式，可以使用句子或文档中的下一个词作为模型的预测标签。
3. 逐词预测生成文本，因此它们被认为是一种**自回归模型**（autoregressive model）。自回归模型将之前的输出作为未来预测的输入。因此，在 GPT 中，每个新单词都是根据它之前的序列来选择的，这提高了最终文本的一致性。


我们将以 GPT 的核心原理为指导，分 3 个阶段来逐步实现一个大语言模型。

![大语言模型的实现](/images/llm/llm_train.png)

1. 第一阶段，我们将学习数据预处理的基本流程，并着手实现大语言模型的核心组件——注意力机制。
2. 第二阶段，我们将学习如何编写代码并预训练一个能够生成新文本的类 GPT 大语言模型。同时，我们还将探讨评估3. 第三阶段，我们将对一个预训练后的大语言模型进行微调，使其能够执行回答查询、文本分类等任务—