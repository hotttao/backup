---
weight: 1
title: "Rag 检索增强的方法"
date: 2025-08-20T11:00:00+08:00
lastmod: 2025-08-20T11:00:00+08:00
draft: false
author: "宋涛"
authorLink: "https://hotttao.github.io/"
description: "Rag 信息检索理论"
featuredImage: 

tags: ["RAG"]
categories: ["langchain"]

lightgallery: true

toc:
  auto: false
---

## 1. 文本分块
智能分块的方法:
1. 语义分块: 需要计算每一个句子与前一个句子的相似度，如果相似则合并为一个分块。
2. 基于大语言模型的分块: 使用大语言模型直接对文本进行分块
3. 上下文感知分块: 无论哪种分块策略，都可以使用大语言模型为每个分块添加上下文，比如给分块添加总结文本解释其在整个文档中的作用、标签信息、提问信息。

## 2. 查询语句解析
为了优化搜索，检索器可以解析提示词，以识别意图，并对其编辑、重写，或者彻底转换提示词，以优化检索效果。

查询解析有如下技术:
1. 使用 llm 重写查询语句
2. 命名实体识别（NER, Named Entity Recognition），比如模型 Gliner 是一个通用命名实体识别模型，输入文本和希望识别的实体类型列表。
3. 假设文本嵌入 HIDE（Hypothetical Document Embedding）

### 2.1 命名实体识别
在 **检索系统** 中，**命名实体识别**（NER）是非常关键的一环。它涉及从文档或查询中识别出具有特定意义的实体，例如人名、地点、组织、日期、时间、货币等信息。这些识别出来的实体对于提升检索质量和结果的相关性至关重要。

#### NER 在信息检索中的作用：

1. **提高检索精度**:

   * 在检索过程中，NER 可以帮助识别文档中的重要实体，从而提取出关键字或短语，进一步提高检索结果的精度。例如，用户查询“**Apple** CEO\*\* Tim Cook\*\*”时，NER 可以提取出**Apple**（公司）和**Tim Cook**（人物），帮助检索系统返回关于 **Apple** 公司和 **Tim Cook** 的相关信息。

2. **查询扩展**:

   * 在搜索过程中，NER 可以帮助系统理解用户查询中的具体意图，基于识别出的实体进一步扩展查询。例如，如果查询中包含“**Amazon**”这个词，NER 可以帮助区分是指 **Amazon** 公司还是亚马逊雨林，从而更准确地扩展查询。

3. **文档排序和相关性增强**:

   * 检索系统可以根据查询和文档中提取的实体之间的匹配度来对文档进行排序。通过识别和匹配实体，系统能够更精确地返回相关的结果，避免仅仅基于关键词的匹配，从而改善检索质量。

4. **多层次的语义理解**:

   * NER 在检索系统中能帮助提取更深层次的语义信息。例如，在问题回答系统中，通过识别出实体，系统不仅可以理解查询本身，还能根据实体间的关系进一步推理出更多可能的答案。


### 2.2 HIDE
HIDE（Hypothetical Document Embedding）是一种基于假设文本嵌入的方法。该方法通过创建一个假设文本或“伪文本”，以便能够捕捉文档的相关特征，从而提高下游任务的性能。

#### HIDE 方法的核心思想：

1. **假设文本生成**:

   * 在某些情况下，原始文本可能没有足够的信息来提供准确的特征表示。HIDE 通过生成一个假设文本，填补这些信息空白。
   * 这些假设文本通常是基于输入文本内容的上下文信息生成的，目的是捕获输入文本的潜在关系和潜在语义。

2. **上下文强化**:

   * HIDE 通过增强文本的上下文信息来改善模型对文本的理解。例如，如果原始文档是某个主题的讨论，假设文本可能会创建一个更加具体化的场景或背景，帮助模型更好地理解文档所涉及的内容。
   * 假设文本可以是人工生成的，也可以是基于数据的某些特征自动推导出来的。

## 3. 相似度计算优化
前面我们介绍的基于 embedding 模型，通过计算向量的相似度，来判断两个文本的相似度的架构，称为双向编码器。除了双向编码器，还有能更精确的计算相似度的方法:
1. 交叉编码器
2. Colbert

### 3.1 **双向编码器 (Bidirectional Encoder)**

双向编码器指的是在处理输入文本时，模型能够同时关注文本的**前向**（从左到右）和**后向**（从右到左）的上下文信息。这意味着模型会从整个文本的左右两个方向理解并编码信息，从而得到更加丰富的表示。

#### 工作原理：

* 双向编码器主要依赖于 Transformer 模型的结构，尤其是 BERT（Bidirectional Encoder Representations from Transformers）模型。
* BERT 使用了**掩码语言建模**（Masked Language Modeling）策略，其中一些单词被遮盖，模型需要通过上下文来预测这些缺失的单词。
* 这种方式使得模型能够同时捕捉到左右文的上下文信息，而不是像传统的语言模型那样仅依赖于前文。

#### 关键特点：

* **双向上下文**: 能同时利用文本的左侧和右侧上下文来进行文本表示，能够更准确地理解单词的含义。
* **无序列生成**: 在 BERT 等双向模型中，输入的所有单词在同一时间被编码和表示，并不是逐个生成的。
* **主要用于**: 文本分类、命名实体识别（NER）、情感分析等任务。


### 3.2 **交叉编码器 (Cross-Encoder)**

交叉编码器与双向编码器的不同之处在于它主要用于处理**两个输入文本之间的关系**，如文本匹配、问答系统等任务。交叉编码器将两个输入文本拼接成一个单一的序列输入到模型中，并通过模型的计算来理解它们之间的关系。

#### 工作原理：

* 交叉编码器的输入是两个文本的结合，它会将两个文本合并在一起并同时处理。
* 这两段文本会一起传递到一个预训练的语言模型中（如 BERT、RoBERTa），然后通过上下文的交互，模型学习并输出对两个文本的匹配结果或关系评分。

#### 关键特点：

* **对文本对的上下文建模**: 交叉编码器主要关注两个文本之间的交互，常用于判断文本对是否匹配（如句子相似度评分）或回答问题（如问答系统）。
* **精确度高**: 交叉编码器通过直接对比两个文本，通常比单独处理文本更为精确。
* **效率较低**: 由于必须同时处理两个文本，并进行全面的上下文交互，交叉编码器在大规模数据上可能会比较慢。

#### 适用场景
与双向编码器相比，交叉编码器几乎总是能提供更好的搜索结果，但是扩展性极差，只能用作改进其他搜索结果。

### 3.3 Colbert

## 4. Rerank
Rerank:  
1. 用于在检索到文档之后，发送给 LLM 之前，通过重新打分提升检索质量。
2. 因为只在少量文档上进行重排序，所以可以使用高性能但是成本较高的模型。
3. 通常重排序使用交叉编码器