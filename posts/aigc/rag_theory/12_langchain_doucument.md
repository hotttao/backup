---
weight: 1
title: "Langchain Rag æŠ½è±¡è¯¦è§£"
date: 2025-08-19T9:00:00+08:00
lastmod: 2025-08-19T9:00:00+08:00
draft: false
author: "å®‹æ¶›"
authorLink: "https://hotttao.github.io/"
description: "Langchain Rag æŠ½è±¡è¯¦è§£"
featuredImage: 

tags: ["RAG"]
categories: ["langchain"]

lightgallery: true

toc:
  auto: false
---

ä¸Šä¸€èŠ‚æˆ‘ä»¬äº†è§£äº† Langchain ä¸­æœ‰å…³ RAG çš„æŠ½è±¡ã€‚è¿™ä¸€èŠ‚æˆ‘ä»¬æ¥å­¦ä¹ è¿™äº›æŠ½è±¡çš„è¯¦ç»†å®šä¹‰ã€‚

## 1. Document
```bash
BaseMedia
 â””â”€â”€ Blob           â† åŸå§‹äºŒè¿›åˆ¶æ•°æ®
Document           â† å¯ç”¨äº LLM çš„æ–‡æ¡£
BaseDocumentTransformer â† æ–‡æ¡£çš„å¤„ç†/è½¬æ¢å™¨
```

- BaseMedia / Blob å¼ºè°ƒ åŸå§‹å†…å®¹
- Document å¼ºè°ƒ å¯å¤„ç†ã€å¯ç´¢å¼•ã€å¯å–‚ç»™ LLM çš„å†…å®¹
- BaseDocumentTransformer å¼ºè°ƒ å¯¹æ–‡æ¡£çš„åŠ å·¥æˆ–è½¬æ¢é€»è¾‘

### 1.1 `Document`

```python
class BaseMedia(Serializable):


    # The ID field is optional at the moment.
    # It will likely become required in a future major release after
    # it has been adopted by enough vectorstore implementations.
    id: Optional[str] = Field(default=None, coerce_numbers_to_str=True)
    """An optional identifier for the document.

    Ideally this should be unique across the document collection and formatted
    as a UUID, but this will not be enforced.

    .. versionadded:: 0.2.11
    """

    metadata: dict = Field(default_factory=dict)


class Blob(BaseMedia):
    data: Union[bytes, str, None] = None
    """Raw data associated with the blob."""
    mimetype: Optional[str] = None
    """MimeType not to be confused with a file extension."""
    encoding: str = "utf-8"
    """Encoding to use if decoding the bytes into a string.

    Use utf-8 as default encoding, if decoding to string.
    """
    path: Optional[PathLike] = None
    """Location where the original content was found."""

    model_config = ConfigDict(
        arbitrary_types_allowed=True,
        frozen=True,
    )


class Document(BaseMedia):
    page_content: str
    """String text."""
    type: Literal["Document"] = "Document"
```


æ¯”è¾ƒå¤æ‚çš„æ˜¯ Blobï¼Œä¸‹é¢æ˜¯å…¶**å±æ€§** å’Œ **æ–¹æ³•**çš„è¯´æ˜

#### `Blob` çš„å±æ€§åŠä½œç”¨

| å±æ€§å            | ç±»å‹                        | ä½œç”¨                                                          |
| -------------- | ------------------------- | ----------------------------------------------------------- |
| `data`         | `Union[bytes, str, None]` | å­˜æ”¾åŸå§‹æ•°æ®ï¼Œå¯ä»¥æ˜¯å­—ç¬¦ä¸²ã€äºŒè¿›åˆ¶æˆ–ä¸ºç©ºï¼ˆå¦‚æœæ˜¯ä»æ–‡ä»¶è·¯å¾„æ‡’åŠ è½½ï¼‰ã€‚                          |
| `mimetype`     | `Optional[str]`           | æ•°æ®çš„ **MIME ç±»å‹**ï¼ˆå¦‚ `text/plain`ã€`application/pdf`ï¼‰ï¼Œç”¨äºæ ‡è¯†å†…å®¹æ ¼å¼ã€‚ |
| `encoding`     | `str`ï¼ˆé»˜è®¤ `"utf-8"`ï¼‰       | æŒ‡å®šåœ¨å­—ç¬¦ä¸²å’ŒäºŒè¿›åˆ¶ä¹‹é—´è½¬æ¢æ—¶ä½¿ç”¨çš„ç¼–ç ã€‚                                       |
| `path`         | `Optional[PathLike]`      | å¦‚æœæ•°æ®æ¥è‡ªæ–‡ä»¶ï¼Œè¿™é‡Œä¿å­˜æ–‡ä»¶è·¯å¾„ã€‚                                          |
| `metadata`     | `dict`ï¼ˆç»§æ‰¿è‡ª `BaseMedia`ï¼‰   | å­˜æ”¾ä¸æ•°æ®ç›¸å…³çš„é¢å¤–ä¿¡æ¯ï¼Œä¾‹å¦‚ `source`ã€‚                                   |
| `model_config` | `ConfigDict`              | Pydantic é…ç½®ï¼šå…è®¸ä»»æ„ç±»å‹å­—æ®µï¼Œå†»ç»“å®ä¾‹é¿å…ä¿®æ”¹ã€‚                              |
| `source`ï¼ˆå±æ€§æ–¹æ³•ï¼‰ | `Optional[str]`           | æ•°æ®æ¥æºï¼ˆä¼˜å…ˆä½¿ç”¨ `metadata["source"]`ï¼Œå¦åˆ™ç”¨ `path`ï¼‰ã€‚                 |

---

#### `Blob` çš„æ–¹æ³•åŠä½œç”¨

| æ–¹æ³•å                               | ä½œç”¨                                              |
| --------------------------------- | ----------------------------------------------- |
| `check_blob_is_valid` (validator) | åœ¨å®ä¾‹åŒ–å‰éªŒè¯ï¼šå¿…é¡»æä¾› `data` æˆ– `path`ï¼Œå¦åˆ™æŠ¥é”™ã€‚              |
| `as_string()`                     | å°†æ•°æ®ä»¥ **å­—ç¬¦ä¸²** å½¢å¼è¿”å›ã€‚å¦‚æœæ˜¯äºŒè¿›åˆ¶åˆ™è§£ç ï¼›å¦‚æœæ˜¯è·¯å¾„åˆ™è¯»å–æ–‡ä»¶ã€‚         |
| `as_bytes()`                      | å°†æ•°æ®ä»¥ **å­—èŠ‚** å½¢å¼è¿”å›ã€‚å¦‚æœæ˜¯å­—ç¬¦ä¸²åˆ™ç¼–ç ï¼›å¦‚æœæ˜¯è·¯å¾„åˆ™è¯»å–æ–‡ä»¶ã€‚          |
| `as_bytes_io()`                   | å°†æ•°æ®ä»¥ **å­—èŠ‚æµï¼ˆIO å¯¹è±¡ï¼‰** çš„å½¢å¼è¿”å›ï¼Œå¯ç”¨äºéœ€è¦æ–‡ä»¶æµçš„åœºæ™¯ï¼ˆå¦‚ä¸Šä¼  APIï¼‰ã€‚ |
| `from_path(path, ...)`            | å·¥å‚æ–¹æ³•ï¼šä»æ–‡ä»¶è·¯å¾„åˆ›å»º `Blob`ï¼Œé»˜è®¤ä¸ç«‹å³åŠ è½½æ•°æ®ï¼Œè€Œæ˜¯åªä¿å­˜è·¯å¾„ã€‚          |
| `from_data(data, ...)`            | å·¥å‚æ–¹æ³•ï¼šç›´æ¥ä»å†…å­˜æ•°æ®ï¼ˆå­—ç¬¦ä¸²/å­—èŠ‚ï¼‰åˆ›å»º `Blob`ã€‚                  |
| `__repr__()`                      | å®šä¹‰å¯¹è±¡çš„å­—ç¬¦ä¸²è¡¨ç¤ºå½¢å¼ï¼Œæ‰“å°æ—¶æ˜¾ç¤º `Blob id` å’Œæ¥æºã€‚               |


### 1.2. `BaseDocumentTransformer`
BaseDocumentTransformer åªæœ‰ä¸€ä¸ªæŠ½è±¡æ–¹æ³• `transform_documents`ï¼Œç”¨äºå¯¹æ–‡æ¡£è¿›è¡Œè½¬æ¢ã€‚

```python
class BaseDocumentTransformer(ABC):
    @abstractmethod
    def transform_documents(
        self, documents: Sequence[Document], **kwargs: Any
    ) -> Sequence[Document]:

    async def atransform_documents(
        self, documents: Sequence[Document], **kwargs: Any
    ) -> Sequence[Document]:
        return await run_in_executor(
            None, self.transform_documents, documents, **kwargs
        )
```

### 1.3 BaseDocumentTransformer å…·ä½“å®ç°

BaseDocumentTransformer çš„å…·ä½“å®ç°éƒ½åœ¨ langchain_text_splitters è¿™ä¸ªåŒ…å†…ã€‚

![UML ç±»å›¾](/images/langchain/splitters.svg)


## 2. `DocumentLoader`

```bash
BaseLoader                â† æŠ½è±¡æ¥å£ï¼Œå®šä¹‰ load() æ–¹æ³•
 â”œâ”€â”€ BlobLoader            â† åŠ è½½åŸå§‹äºŒè¿›åˆ¶æ•°æ®
 â””â”€â”€ LangSmithLoader       â† ä» LangSmith å¹³å°åŠ è½½æ•°æ®

BaseBlobParser             â† å°† Blob è§£ææˆ Document
```

### 2.1 `BaseLoader`
BaseLoader å®šä¹‰ `lazy_load` æŠ½è±¡æ–¹æ³•ï¼Œå¹¶å­˜åœ¨å¦‚ä¸‹è°ƒç”¨é“¾:

```bash
load
    lazy_load
```

BaseLoader è¿˜æä¾›äº†ä¸€ä¸ª load_and_splitï¼Œç”¨äºå°† Documents åˆ†æˆå¤šä¸ª chunkã€‚

```python
class BaseLoader(ABC): 
    def load(self) -> list[Document]:
        """Load data into Document objects."""
        return list(self.lazy_load())

    async def aload(self) -> list[Document]:
        """Load data into Document objects."""
        return [document async for document in self.alazy_load()]

    def lazy_load(self) -> Iterator[Document]:
        """A lazy loader for Documents."""
        if type(self).load != BaseLoader.load:
            return iter(self.load())
        msg = f"{self.__class__.__name__} does not implement lazy_load()"
        raise NotImplementedError(msg)

    async def alazy_load(self) -> AsyncIterator[Document]:
        """A lazy loader for Documents."""
        iterator = await run_in_executor(None, self.lazy_load)
        done = object()
        while True:
            doc = await run_in_executor(None, next, iterator, done)
            if doc is done:
                break
            yield doc  # type: ignore[misc]


    def load_and_split(
        self, text_splitter: Optional[TextSplitter] = None
    ) -> list[Document]:
        """Load Documents and split into chunks. Chunks are returned as Documents.

        Do not override this method. It should be considered to be deprecated!

        Args:
            text_splitter: TextSplitter instance to use for splitting documents.
              Defaults to RecursiveCharacterTextSplitter.

        Returns:
            List of Documents.
        """
        if text_splitter is None:
            try:
                from langchain_text_splitters import RecursiveCharacterTextSplitter
            except ImportError as e:
                msg = (
                    "Unable to import from langchain_text_splitters. Please specify "
                    "text_splitter or install langchain_text_splitters with "
                    "`pip install -U langchain-text-splitters`."
                )
                raise ImportError(msg) from e

            text_splitter_: TextSplitter = RecursiveCharacterTextSplitter()
        else:
            text_splitter_ = text_splitter
        docs = self.load()
        return text_splitter_.split_documents(docs)
```

### 2.2 `BlobLoader`
BlobLoader å®šä¹‰ `yield_blobs` æŠ½è±¡æ–¹æ³•ï¼Œç”¨äºè¿”å›ä¸€ä¸ª Blob è¿­ä»£å™¨ã€‚
BaseBlobParser å®šä¹‰ `lazy_parse` æŠ½è±¡æ–¹æ³•ï¼Œç”¨äºå°† Blob è§£ææˆ Documentã€‚

BaseBlobParser ä¸­å­˜åœ¨å¦‚ä¸‹è°ƒç”¨é“¾:

```bash
parse
    lazy_parse
```

```python
class BlobLoader(ABC):
    """Abstract interface for blob loaders implementation.

    Implementer should be able to load raw content from a storage system according
    to some criteria and return the raw content lazily as a stream of blobs.
    """

    @abstractmethod
    def yield_blobs(
        self,
    ) -> Iterable[Blob]:
        """A lazy loader for raw data represented by LangChain's Blob object.

        Returns:
            A generator over blobs
        """


class BaseBlobParser(ABC):
    @abstractmethod
    def lazy_parse(self, blob: Blob) -> Iterator[Document]:

    def parse(self, blob: Blob) -> list[Document]:
        return list(self.lazy_parse(blob))
```

### 2.3 BaseLoader å…·ä½“å®ç°
BaseLoader çš„å…·ä½“å®ç°ä½äº langchain_unstructuredã€langchain_community.document_loaders ä¸­ã€‚è¿™ä¸¤ä¸ªåº“åº•å±‚éƒ½ä½¿ç”¨äº† [Unstructured](https://github.com/Unstructured-IO/unstructured)

è¿™ä¸‰ä¸ªåº“çš„å…³ç³»æœ‰ç‚¹ç»•ï¼Œåé¢æˆ‘ä»¬ä¼šæœ‰ä¸€èŠ‚ä¸“é—¨ä»‹ç» unstructured çš„ä½¿ç”¨ã€‚

#### åœ¨ RAG æµç¨‹ä¸­çš„ä½ç½®

```text
åŸå§‹æ–‡ä»¶ (PDF/Word/HTML/å›¾ç‰‡)
   â”‚
   â–¼
langchain_unstructured (Document Loaders)
   â”‚
   â–¼
æ ‡å‡† Document å¯¹è±¡
   â”‚
   â–¼
TextSplitter â†’ Embeddings â†’ VectorStore â†’ Retriever â†’ LLM
```

## 3. embedding
```
Embeddings                     â† æŠ½è±¡æ¥å£ï¼šæ–‡æœ¬ â†’ å‘é‡
 â””â”€â”€ FakeEmbeddings            â† æµ‹è¯•ç”¨ä¼ªå‘é‡
      â””â”€â”€ DeterministicFakeEmbedding  â† å¯é‡å¤ä¼ªå‘é‡
```

- Embeddingsï¼šçœŸå®æˆ–é€šç”¨å‘é‡åŒ–æ¥å£
- FakeEmbeddingsï¼šå ä½æˆ–éšæœºå‘é‡ï¼Œç”¨äºæµ‹è¯•
- DeterministicFakeEmbeddingï¼šå¯é‡å¤å‘é‡ï¼Œç”¨äºå¯é æµ‹è¯•

### 3.1 Embeddings
Embeddings å®šä¹‰äº†ä¸¤ä¸ªæŠ½è±¡æ–¹æ³•:
1. embed_documents: ç”¨äºå°†æ–‡æ¡£åˆ—è¡¨è½¬æ¢ä¸ºå‘é‡åˆ—è¡¨
2. embed_query: ç”¨äºå°†æŸ¥è¯¢æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡


```python
class Embeddings(ABC):
    @abstractmethod
    def embed_documents(self, texts: list[str]) -> list[list[float]]:
        """Embed search docs.

        Args:
            texts: List of text to embed.

        Returns:
            List of embeddings.
        """

    @abstractmethod
    def embed_query(self, text: str) -> list[float]:
        """Embed query text.

        Args:
            text: Text to embed.

        Returns:
            Embedding.
        """

    async def aembed_documents(self, texts: list[str]) -> list[list[float]]:
        """Asynchronous Embed search docs.

        Args:
            texts: List of text to embed.

        Returns:
            List of embeddings.
        """
        return await run_in_executor(None, self.embed_documents, texts)

    async def aembed_query(self, text: str) -> list[float]:
        """Asynchronous Embed query text.

        Args:
            text: Text to embed.

        Returns:
            Embedding.
        """
        return await run_in_executor(None, self.embed_query, text)
```

### 3.2 Embedding å…·ä½“å®ç°
1. å¼€æºæ¨¡å‹çš„ Embedding ä½äº `langchain_community.embeddings`
2. é—­æºæœåŠ¡æœ‰å„è‡ªç‹¬ç«‹çš„åŒ…:
    - langchain_openai.embeddings â†’ OpenAIEmbeddings
    - langchain_google_genai.embeddings â†’ Google Gemini çš„ embedding

## 4. Retriever
`BaseRetriever` æ˜¯æ•´ä¸ªæ£€ç´¢å™¨æ¨¡å—çš„åŸºç¡€æŠ½è±¡ï¼š

```
BaseRetriever
 â”œâ”€ å‘é‡æ•°æ®åº“æ£€ç´¢å™¨ï¼ˆVectorStoreRetrieverï¼‰
 â”œâ”€ å…³é”®è¯æ£€ç´¢å™¨ï¼ˆKeywordRetrieverï¼‰
 â””â”€ å…¶ä»–è‡ªå®šä¹‰æ£€ç´¢å™¨
```

* æŠ½è±¡äº†â€œå¦‚ä½•ä»æ–‡æ¡£é›†åˆä¸­æ‰¾åˆ°ç›¸å…³ä¿¡æ¯â€çš„é€šç”¨æ¥å£
* æä¾›ç»Ÿä¸€è°ƒç”¨æ–¹å¼ç»™ LLM æˆ–ä¸Šå±‚åº”ç”¨

### 4.1 BaseRetriever

BaseRetriever å®šä¹‰äº†æŠ½è±¡æ¥å£:
1. _get_relevant_documentsï¼Œç±»ä¼¼çš„ get_relevant_documents å·²ç»æ ‡æ³¨ä¸º deprecated
2. _aget_relevant_documentsï¼Œç±»ä¼¼çš„ aget_relevant_documents å·²ç»æ ‡æ³¨ä¸º deprecated

invoke æ–¹æ³•ä¼šè°ƒç”¨ _get_relevant_documents 

```basg
invoke
    _get_relevant_documents

ainvoke
    _aget_relevant_documentsï¼Œç±»ä¼¼çš„
```

```python
class BaseRetriever(RunnableSerializable[RetrieverInput, RetrieverOutput], ABC):
    model_config = ConfigDict(
        arbitrary_types_allowed=True,
    )

    _new_arg_supported: bool = False
    _expects_other_args: bool = False
    tags: Optional[list[str]] = None
    """Optional list of tags associated with the retriever. Defaults to None.
    These tags will be associated with each call to this retriever,
    and passed as arguments to the handlers defined in `callbacks`.
    You can use these to eg identify a specific instance of a retriever with its
    use case.
    """
    metadata: Optional[dict[str, Any]] = None
    """Optional metadata associated with the retriever. Defaults to None.
    This metadata will be associated with each call to this retriever,
    and passed as arguments to the handlers defined in `callbacks`.
    You can use these to eg identify a specific instance of a retriever with its
    use case.
    """

    @abstractmethod
    def _get_relevant_documents(
        self, query: str, *, run_manager: CallbackManagerForRetrieverRun
    ) -> list[Document]:
        pass

    async def _aget_relevant_documents(
        self, query: str, *, run_manager: AsyncCallbackManagerForRetrieverRun
    ) -> list[Document]:
        return await run_in_executor(
            None,
            self._get_relevant_documents,
            query,
            run_manager=run_manager.get_sync(),
        )
        pass

    @override
    def invoke(
        self, input: str, config: Optional[RunnableConfig] = None, **kwargs: Any
    ) -> list[Document]:
        """Invoke the retriever to get relevant documents.

        Main entry point for synchronous retriever invocations.

        Args:
            input: The query string.
            config: Configuration for the retriever. Defaults to None.
            kwargs: Additional arguments to pass to the retriever.
        """
        from langchain_core.callbacks.manager import CallbackManager

        config = ensure_config(config)
        inheritable_metadata = {
            **(config.get("metadata") or {}),
            **self._get_ls_params(**kwargs),
        }
        callback_manager = CallbackManager.configure(
            config.get("callbacks"),
            None,
            verbose=kwargs.get("verbose", False),
            inheritable_tags=config.get("tags"),
            # tag ä¼šè¢«ä¼ å…¥ callback
            local_tags=self.tags,
            inheritable_metadata=inheritable_metadata,
            # metadata ä¼šè¢«ä¼ å…¥ callback
            local_metadata=self.metadata,
        )
        run_manager = callback_manager.on_retriever_start(
            None,
            input,
            name=config.get("run_name") or self.get_name(),
            run_id=kwargs.pop("run_id", None),
        )
        try:
            # _expects_other_args ä¸º True æ—¶ï¼Œkwargs ä¼šè¢«ä¼ å…¥ _get_relevant_documents
            kwargs_ = kwargs if self._expects_other_args else {}
            if self._new_arg_supported:
                # æ˜¯å¦ä¼ å…¥ run_manager å–å†³äº _new_arg_supported
                result = self._get_relevant_documents(
                    input, run_manager=run_manager, **kwargs_
                )
            else:
                result = self._get_relevant_documents(input, **kwargs_)
        except Exception as e:
            run_manager.on_retriever_error(e)
            raise
        else:
            run_manager.on_retriever_end(
                result,
            )
            return result
```

### 4.2 BaseRetriever å…·ä½“å®ç°
BaseRetriever çš„å…·ä½“å®ç°ï¼Œä¸»è¦æ˜¯ä¸‹é¢è¦ä»‹ç»çš„ VectorStoreRetrieverã€‚

## 5. VectorStore
```
VectorStore                  â† å‘é‡æ•°æ®åº“æŠ½è±¡æ¥å£
 â””â”€â”€ InMemoryVectorStore      â† å†…å­˜å®ç°ç‰ˆæœ¬ï¼ˆç®€å•/æµ‹è¯•ç”¨ï¼‰

VectorStoreRetriever          â† Retriever å°è£…ï¼Œè°ƒç”¨ VectorStore è¿”å› Document
```

* `VectorStore`ï¼šå®šä¹‰å‘é‡æ•°æ®åº“æ¥å£
* `VectorStoreRetriever`ï¼šåŸºäºå‘é‡æ•°æ®çš„æ£€ç´¢å™¨å®ç°

### 5.1 VectorStore
VectorStore æä¾›çš„æ–¹æ³•æ¯”è¾ƒå¤šï¼Œè¿™é‡Œæˆ‘ä»¬å¿½ç•¥å¼‚æ­¥æ–¹æ³•ã€‚å…¶æä¾›äº†å¦‚ä¸‹æŠ½è±¡æ–¹æ³•:
1. add_texts: å‘å‘é‡æ•°æ®æ·»åŠ æ–‡æ¡£ï¼Œå¯ä»¥å€ŸåŠ© add_documents å®ç°
2. add_documents å‘å‘é‡æ•°æ®æ·»åŠ æ–‡æ¡£ï¼Œå¯ä»¥å€ŸåŠ© add_texts å®ç°ï¼Œè¿™ä¸¤ä¸ªæ–¹æ³•ä»»æ„å®ç°å…¶ä¸€å³å¯
3. embeddings: å¯é€‰å®ç° embedding
4. delete: é€šè¿‡ id åˆ é™¤æ–‡æ¡£
5. get_by_idsï¼šé€šè¿‡ id è·å–æ–‡æ¡£
6. search: æŸ¥è¯¢ç›¸ä¼¼æ–‡æ¡£

æ˜ç™½äº†ï¼Œæˆ‘å°†åˆ é™¤å‚æ•°åˆ—ï¼Œå¹¶ä¸”å‡¡æ˜¯ç›´æ¥æŠ›å‡º `NotImplementedError` çš„æ–¹æ³•éƒ½æ ‡æ³¨ä¸ºâ€œæŠ½è±¡æ–¹æ³•â€ã€‚æ•´ç†åçš„è¡¨æ ¼å¦‚ä¸‹ï¼š

| æ–¹æ³•å                                        | ç±»å‹   | ç®€è¦è¯´æ˜                                                         |
| ------------------------------------------ | ---- | ------------------------------------------------------------ |
| `add_texts`                                | æŠ½è±¡æ–¹æ³• | å°†æ–‡æœ¬é€šè¿‡ embeddings æ·»åŠ åˆ°å‘é‡å­˜å‚¨ä¸­ï¼Œè¿”å›ç”Ÿæˆçš„ ID åˆ—è¡¨ï¼Œå¯ä»¥å€ŸåŠ© add_documents å®ç°                        |
| `add_documents`                            | æ™®é€šæ–¹æ³• | å°† Document å¯¹è±¡æ·»åŠ æˆ–æ›´æ–°åˆ°å‘é‡å­˜å‚¨ä¸­ï¼Œå¯ä»¥å€ŸåŠ© add_texts å®ç°                   |
| `embeddings`                               | å±æ€§   | è¿”å›å½“å‰ VectorStore çš„ embedding å¯¹è±¡ï¼ˆå¦‚æœå®ç°äº†ï¼‰                       |
| `delete`                                   | æŠ½è±¡æ–¹æ³• | åˆ é™¤æŒ‡å®š ID çš„æ–‡æ¡£ï¼Œéœ€å­ç±»å®ç°                                            |
| `get_by_ids`                               | æŠ½è±¡æ–¹æ³• | æ ¹æ® ID è·å–æ–‡æ¡£ï¼Œéœ€å­ç±»å®ç°                                             |
| `search`                                   | æ™®é€šæ–¹æ³• | æ ¹æ®æŒ‡å®šæœç´¢ç±»å‹ï¼ˆsimilarityã€mmrã€similarity\_score\_thresholdï¼‰è¿”å›æœ€ç›¸ä¼¼æ–‡æ¡£ |
| `similarity_search`                        | æŠ½è±¡æ–¹æ³• | è¿”å›æœ€ç›¸ä¼¼æ–‡æ¡£åˆ—è¡¨ï¼Œå­ç±»å¿…é¡»å®ç°                                             |
| `similarity_search_with_relevance_scores`  | æ™®é€šæ–¹æ³• | è¿”å›æ–‡æ¡£åŠç›¸å…³æ€§åˆ†æ•°ï¼Œå¯æ ¹æ® `score_threshold` è¿‡æ»¤                          |
| `_similarity_search_with_relevance_scores` | æ™®é€šæ–¹æ³• | é»˜è®¤å®ç°çš„å¸¦ç›¸å…³æ€§åˆ†æ•°çš„ç›¸ä¼¼æ€§æœç´¢ï¼Œå¯è¢«å­ç±»ä¿®æ”¹                                     |
| `_select_relevance_score_fn`               | æŠ½è±¡æ–¹æ³• | è¿”å›ç”¨äºè®¡ç®—ç›¸å…³æ€§åˆ†æ•°çš„å‡½æ•°ï¼Œéœ€å­ç±»å®ç°                                         |
| `similarity_search_with_score`             | æŠ½è±¡æ–¹æ³• | æ ¹æ®è·ç¦»æˆ–åˆ†æ•°è¿”å›æ–‡æ¡£åŠç›¸ä¼¼åº¦åˆ†æ•°ï¼Œéœ€å­ç±»å®ç°                                      |
| `max_marginal_relevance_search`            | æŠ½è±¡æ–¹æ³• | æœ€å¤§è¾¹é™…ç›¸å…³æ€§æœç´¢ï¼Œè¿”å›å…¼é¡¾ç›¸ä¼¼æ€§å’Œå¤šæ ·æ€§çš„æ–‡æ¡£ï¼Œéœ€å­ç±»å®ç°                               |
| `similarity_search_by_vector`              | æŠ½è±¡æ–¹æ³• | æ ¹æ®å‘é‡è¿”å›æœ€ç›¸ä¼¼æ–‡æ¡£ï¼Œéœ€å­ç±»å®ç°                                            |
| `max_marginal_relevance_search_by_vector`  | æŠ½è±¡æ–¹æ³• | åŸºäºå‘é‡çš„æœ€å¤§è¾¹é™…ç›¸å…³æ€§æœç´¢ï¼Œéœ€å­ç±»å®ç°                                         |
| `from_documents`                           | ç±»æ–¹æ³•  | ä½¿ç”¨æ–‡æ¡£å’Œ embedding åˆå§‹åŒ– VectorStore                              |
| `from_texts`                               | æŠ½è±¡æ–¹æ³• | ä½¿ç”¨æ–‡æœ¬å’Œ embedding åˆå§‹åŒ– VectorStoreï¼Œå­ç±»å¿…é¡»å®ç°                       |
| `as_retriever`                             | æ™®é€šæ–¹æ³• | è¿”å›ä¸€ä¸ªåŸºäºè¯¥ VectorStore çš„ `VectorStoreRetriever` å¯¹è±¡              |


VectorStore ä¸­åŒ…å«å¦‚ä¸‹è°ƒç”¨é“¾:

```bash
add_texts
    add_documents
add_documents
    add_texts

search
    # if search_type == "similarity":
    similarity_search

    # if search_type == "similarity_score_threshold":
    similarity_search_with_relevance_scores
        _similarity_search_with_relevance_scores
            _select_relevance_score_fn
            similarity_search_with_score

    # if search_type == "mmr":
    max_marginal_relevance_search


from_documents
    from_texts
```



```python
class VectorStore(ABC):
    """Interface for vector store."""
    def add_texts(
        self,
        texts: Iterable[str],
        metadatas: Optional[list[dict]] = None,
        *,
        ids: Optional[list[str]] = None,
        **kwargs: Any,
    ) -> list[str]:
        """Run more texts through the embeddings and add to the vectorstore."""
        pass

    def add_documents(self, documents: list[Document], **kwargs: Any) -> list[str]:
        """Add or update documents in the vectorstore.
        """
        pass

    @property
    def embeddings(self) -> Optional[Embeddings]:
        """Access the query embedding object if available."""
        logger.debug(
            "The embeddings property has not been implemented for %s",
            self.__class__.__name__,
        )
        return None

    def delete(self, ids: Optional[list[str]] = None, **kwargs: Any) -> Optional[bool]:
        msg = "delete method must be implemented by subclass."
        raise NotImplementedError(msg)

    def get_by_ids(self, ids: Sequence[str], /) -> list[Document]:
        """Get documents by their IDs."""
        msg = f"{self.__class__.__name__} does not yet support get_by_ids."
        raise NotImplementedError(msg)

    def add_documents(self, documents: list[Document], **kwargs: Any) -> list[str]:
        """Add or update documents in the vectorstore.
        """
        if type(self).add_texts != VectorStore.add_texts:
            if "ids" not in kwargs:
                ids = [doc.id for doc in documents]

                # If there's at least one valid ID, we'll assume that IDs
                # should be used.
                if any(ids):
                    kwargs["ids"] = ids

            texts = [doc.page_content for doc in documents]
            metadatas = [doc.metadata for doc in documents]
            return self.add_texts(texts, metadatas, **kwargs)
        msg = (
            f"`add_documents` and `add_texts` has not been implemented "
            f"for {self.__class__.__name__} "
        )
        raise NotImplementedError(msg)

    def search(self, query: str, search_type: str, **kwargs: Any) -> list[Document]:
        """Return docs most similar to query using a specified search type.

        Args:
            query: Input text
            search_type: Type of search to perform. Can be "similarity",
                "mmr", or "similarity_score_threshold".
            **kwargs: Arguments to pass to the search method.

        """
        if search_type == "similarity":
            return self.similarity_search(query, **kwargs)
        if search_type == "similarity_score_threshold":
            docs_and_similarities = self.similarity_search_with_relevance_scores(
                query, **kwargs
            )
            return [doc for doc, _ in docs_and_similarities]
        if search_type == "mmr":
            return self.max_marginal_relevance_search(query, **kwargs)
        msg = (
            f"search_type of {search_type} not allowed. Expected "
            "search_type to be 'similarity', 'similarity_score_threshold'"
            " or 'mmr'."
        )
        raise ValueError(msg)

    @abstractmethod
    def similarity_search(
        self, query: str, k: int = 4, **kwargs: Any
    ) -> list[Document]:
        """Return docs most similar to query.

        Args:
            query: Input text.
            k: Number of Documents to return. Defaults to 4.
            **kwargs: Arguments to pass to the search method.

        Returns:
            List of Documents most similar to the query.
        """

    def similarity_search_with_relevance_scores(
        self,
        query: str,
        k: int = 4,
        **kwargs: Any,
    ) -> list[tuple[Document, float]]:
        """Return docs and relevance scores in the range [0, 1].

        0 is dissimilar, 1 is most similar.

        Args:
            query: Input text.
            k: Number of Documents to return. Defaults to 4.
            **kwargs: kwargs to be passed to similarity search. Should include:
                score_threshold: Optional, a floating point value between 0 to 1 to
                    filter the resulting set of retrieved docs.
        """
        score_threshold = kwargs.pop("score_threshold", None)

        docs_and_similarities = self._similarity_search_with_relevance_scores(
            query, k=k, **kwargs
        )
        if any(
            similarity < 0.0 or similarity > 1.0
            for _, similarity in docs_and_similarities
        ):
            warnings.warn(
                "Relevance scores must be between"
                f" 0 and 1, got {docs_and_similarities}",
                stacklevel=2,
            )

        if score_threshold is not None:
            docs_and_similarities = [
                (doc, similarity)
                for doc, similarity in docs_and_similarities
                if similarity >= score_threshold
            ]
            if len(docs_and_similarities) == 0:
                logger.warning(
                    "No relevant docs were retrieved using the "
                    "relevance score threshold %s",
                    score_threshold,
                )
        return docs_and_similarities


    def _similarity_search_with_relevance_scores(
        self,
        query: str,
        k: int = 4,
        **kwargs: Any,
    ) -> list[tuple[Document, float]]:
        """Default similarity search with relevance scores.

        Modify if necessary in subclass.
        Return docs and relevance scores in the range [0, 1].

        0 is dissimilar, 1 is most similar.

        Args:
            query: Input text.
            k: Number of Documents to return. Defaults to 4.
            **kwargs: kwargs to be passed to similarity search. Should include:
                score_threshold: Optional, a floating point value between 0 to 1 to
                    filter the resulting set of retrieved docs

        Returns:
            List of Tuples of (doc, similarity_score)
        """
        relevance_score_fn = self._select_relevance_score_fn()
        docs_and_scores = self.similarity_search_with_score(query, k, **kwargs)
        return [(doc, relevance_score_fn(score)) for doc, score in docs_and_scores]

    def _select_relevance_score_fn(self) -> Callable[[float], float]:
        """The 'correct' relevance function.

        may differ depending on a few things, including:
        - the distance / similarity metric used by the VectorStore
        - the scale of your embeddings (OpenAI's are unit normed. Many others are not!)
        - embedding dimensionality
        - etc.

        Vectorstores should define their own selection-based method of relevance.
        """
        raise NotImplementedError


    def similarity_search_with_score(
        self, *args: Any, **kwargs: Any
    ) -> list[tuple[Document, float]]:
        """Run similarity search with distance.

        Args:
            *args: Arguments to pass to the search method.
            **kwargs: Arguments to pass to the search method.

        Returns:
            List of Tuples of (doc, similarity_score).
        """
        raise NotImplementedError


    def max_marginal_relevance_search(
        self,
        query: str,
        k: int = 4,
        fetch_k: int = 20,
        lambda_mult: float = 0.5,
        **kwargs: Any,
    ) -> list[Document]:
        """Return docs selected using the maximal marginal relevance.

        Maximal marginal relevance optimizes for similarity to query AND diversity
        among selected documents.

        Args:
            query: Text to look up documents similar to.
            k: Number of Documents to return. Defaults to 4.
            fetch_k: Number of Documents to fetch to pass to MMR algorithm.
                Default is 20.
            lambda_mult: Number between 0 and 1 that determines the degree
                of diversity among the results with 0 corresponding
                to maximum diversity and 1 to minimum diversity.
                Defaults to 0.5.
            **kwargs: Arguments to pass to the search method.

        Returns:
            List of Documents selected by maximal marginal relevance.
        """
        raise NotImplementedError

    def similarity_search_by_vector(
        self, embedding: list[float], k: int = 4, **kwargs: Any
    ) -> list[Document]:
        """Return docs most similar to embedding vector.

        """
        raise NotImplementedError

    def max_marginal_relevance_search_by_vector(
        self,
        embedding: list[float],
        k: int = 4,
        fetch_k: int = 20,
        lambda_mult: float = 0.5,
        **kwargs: Any,
    ) -> list[Document]:
        """Return docs selected using the maximal marginal relevance.

        Maximal marginal relevance optimizes for similarity to query AND diversity
        among selected documents.
        """

    @classmethod
    def from_documents(
        cls,
        documents: list[Document],
        embedding: Embeddings,
        **kwargs: Any,
    ) -> Self:
        """Return VectorStore initialized from documents and embeddings.
        """
        texts = [d.page_content for d in documents]
        metadatas = [d.metadata for d in documents]

        if "ids" not in kwargs:
            ids = [doc.id for doc in documents]

            # If there's at least one valid ID, we'll assume that IDs
            # should be used.
            if any(ids):
                kwargs["ids"] = ids

        return cls.from_texts(texts, embedding, metadatas=metadatas, **kwargs)

    @classmethod
    @abstractmethod
    def from_texts(
        cls: type[VST],
        texts: list[str],
        embedding: Embeddings,
        metadatas: Optional[list[dict]] = None,
        *,
        ids: Optional[list[str]] = None,
        **kwargs: Any,
    ) -> VST:
        """Return VectorStore initialized from texts and embeddings.
        """


    def as_retriever(self, **kwargs: Any) -> VectorStoreRetriever:
        """Return VectorStoreRetriever initialized from this VectorStore.
        Returns:
            VectorStoreRetriever: Retriever class for VectorStore.
        """
        tags = kwargs.pop("tags", None) or [*self._get_retriever_tags()]
        return VectorStoreRetriever(vectorstore=self, tags=tags, **kwargs)

```

### 5.2 VectorStoreRetriever
VectorStoreRetriever æ˜¯åŸºäº VectorStore å®ç°çš„ Retrieverã€‚

```python
class VectorStoreRetriever(BaseRetriever):
    """Base Retriever class for VectorStore."""

    vectorstore: VectorStore
    """VectorStore to use for retrieval."""
    search_type: str = "similarity"
    """Type of search to perform. Defaults to "similarity"."""
    search_kwargs: dict = Field(default_factory=dict)
    """Keyword arguments to pass to the search function."""
    allowed_search_types: ClassVar[Collection[str]] = (
        "similarity",
        "similarity_score_threshold",
        "mmr",
    )

    model_config = ConfigDict(
        arbitrary_types_allowed=True,
    )

    @override
    def _get_relevant_documents(
        self, query: str, *, run_manager: CallbackManagerForRetrieverRun, **kwargs: Any
    ) -> list[Document]:
        kwargs_ = self.search_kwargs | kwargs
        if self.search_type == s"imilarity":
            docs = self.vectorstore.similarity_search(query, **kwargs_)
        elif self.search_type == "similarity_score_threshold":
            docs_and_similarities = (
                self.vectorstore.similarity_search_with_relevance_scores(
                    query, **kwargs_
                )
            )
            docs = [doc for doc, _ in docs_and_similarities]
        elif self.search_type == "mmr":
            docs = self.vectorstore.max_marginal_relevance_search(query, **kwargs_)
        else:
            msg = f"search_type of {self.search_type} not allowed."
            raise ValueError(msg)
        return docs
```

### 5.3 VectorStore å…·ä½“å®ç°

### å‘é‡æ•°æ®åº“æ¦‚è§ˆ

å¥½çš„ ğŸ‘ æˆ‘å¸®ä½ æ•´ç†äº†ä¸€äº›å¸¸è§å‘é‡æ•°æ®åº“ï¼Œå¯¹æ¯”å®ƒä»¬çš„ **å®ç°è¯­è¨€ã€é€‚ç”¨è§„æ¨¡ã€ä¸»è¦ä¼˜åŠ¿ã€æ˜¯å¦æ”¯æŒåˆ†å¸ƒå¼**ï¼š

| æ•°æ®åº“                                              | å®ç°è¯­è¨€         | é€‚ç”¨è§„æ¨¡             | ä¸»è¦ä¼˜åŠ¿                                      | æ”¯æŒåˆ†å¸ƒå¼  |
| ------------------------------------------------ | ------------ | ---------------- | ----------------------------------------- | ------ |
| **FAISS (Facebook AI)**                          | C++ / Python | å•æœºï¼Œç™¾ä¸‡ï½ä¸Šäº¿å‘é‡       | é«˜æ€§èƒ½ç›¸ä¼¼åº¦æœç´¢ï¼Œæä¾›å¤šç§ç´¢å¼•ç»“æ„ï¼ˆIVF, HNSW, PQ ç­‰ï¼‰ï¼ŒGPU åŠ é€Ÿ | âŒï¼ˆä»…å•æœºï¼‰ |
| **Annoy (Spotify)**                              | C++ / Python | å•æœºï¼Œç™¾ä¸‡çº§å‘é‡         | å†…å­˜å ç”¨å°ï¼Œæ„å»ºå’ŒæŸ¥è¯¢é€Ÿåº¦å¿«ï¼Œé€‚åˆæ¨èåœºæ™¯                     | âŒ      |
| **hnswlib**                                      | C++ / Python | å•æœºï¼Œç™¾ä¸‡ï½ä¸Šäº¿å‘é‡       | é«˜ç²¾åº¦ HNSW å›¾ç´¢å¼•ï¼Œé€Ÿåº¦å’Œå¬å›ç‡ä¼˜ç§€                     | âŒ      |
| **Milvus**                                       | C++ / Go     | åˆ†å¸ƒå¼ï¼Œåäº¿çº§å‘é‡        | å…¨åŠŸèƒ½å‘é‡æ•°æ®åº“ï¼Œæ”¯æŒå¤šç´¢å¼•ï¼Œå¼ºå¤§çš„åˆ†å¸ƒå¼èƒ½åŠ›ï¼Œäº‘åŸç”Ÿ               | âœ…      |
| **Weaviate**                                     | Go           | åˆ†å¸ƒå¼ï¼Œåäº¿çº§å‘é‡        | æ”¯æŒå‘é‡+ç»“æ„åŒ–æ··åˆæœç´¢ï¼ŒGraphQL/REST APIï¼Œæ’ä»¶åŒ–å‘é‡åŒ–æ¨¡å—    | âœ…      |
| **Pinecone**                                     | Go / Rust    | äº‘æœåŠ¡ï¼Œåäº¿çº§å‘é‡        | å…¨æ‰˜ç®¡ SaaSï¼Œå…ç»´æŠ¤ï¼Œå…¨çƒåˆ†å¸ƒï¼Œä½å»¶è¿Ÿ                     | âœ…      |
| **Qdrant**                                       | Rust         | åˆ†å¸ƒå¼ï¼Œåäº¿çº§å‘é‡        | é«˜æ€§èƒ½ ANNï¼ˆHNSWï¼‰ï¼Œæ”¯æŒå‘é‡+è¿‡æ»¤æ£€ç´¢ï¼Œå‘é‡å‹ç¼©ï¼Œé«˜æ€§ä»·æ¯”         | âœ…      |
| **Vespa (Yahoo/Oath)**                           | Java / C++   | åˆ†å¸ƒå¼ï¼Œåäº¿çº§å‘é‡        | å‘é‡+å…³é”®è¯æ··åˆæœç´¢ï¼Œé€‚åˆæœç´¢/æ¨èå¼•æ“åœºæ™¯                    | âœ…      |
| **Redis + Redis Vector Similarity (RediSearch)** | C            | åˆ†å¸ƒå¼ï¼Œç™¾ä¸‡ï½åäº¿çº§ï¼ˆä¾èµ–é›†ç¾¤ï¼‰ | å†…å­˜æ•°æ®åº“åŸºç¡€ï¼Œå‘é‡+ç»“æ„åŒ–æŸ¥è¯¢ï¼Œé€‚åˆä½å»¶è¿Ÿåº”ç”¨                  | âœ…      |
| **Elasticsearch / OpenSearch + kNN æ’ä»¶**          | Java         | åˆ†å¸ƒå¼ï¼Œåäº¿çº§          | å‘é‡+å…¨æ–‡æ£€ç´¢æ··åˆï¼Œé€‚åˆä¼ä¸šå·²æœ‰ ES ç”Ÿæ€                    | âœ…      |

ğŸ“Œ æ€»ç»“ï¼š

* **å•æœºè½»é‡çº§** â†’ FAISS, Annoy, hnswlibï¼ˆé€‚åˆå®éªŒ/å°è§„æ¨¡åº”ç”¨ï¼‰ã€‚
* **åˆ†å¸ƒå¼ä¼ä¸šçº§** â†’ Milvus, Weaviate, Qdrant, Vespaï¼ˆé€‚åˆå¤§è§„æ¨¡ç”Ÿäº§éƒ¨ç½²ï¼‰ã€‚
* **æ‰˜ç®¡æœåŠ¡** â†’ Pineconeï¼ˆå…ç»´æŠ¤ï¼‰ã€‚
* **ç”Ÿæ€æ‰©å±•** â†’ Redis, Elasticsearchï¼ˆå¦‚æœå·²æœ‰è¿™äº›ç³»ç»Ÿï¼Œå¯ä»¥æ‰©å±•ï¼‰ã€‚
