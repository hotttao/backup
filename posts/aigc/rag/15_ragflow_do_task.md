---
weight: 1
title: "RagFlow Task æ‰§è¡Œæµç¨‹"
date: 2025-08-20T11:00:00+08:00
lastmod: 2025-08-20T11:00:00+08:00
draft: false
author: "å®‹æ¶›"
authorLink: "https://hotttao.github.io/"
description: "RagFlow Task æ‰§è¡Œæµç¨‹"
featuredImage: 

tags: ["RAG", "RagFlow"]
categories: ["langchain"]

lightgallery: true

toc:
  auto: false
---

å‰ä¸¤èŠ‚æˆ‘ä»¬ä»‹ç»äº† ragflow task exector çš„å¯åŠ¨æµç¨‹ï¼Œä»¥åŠ ragflow ä¸­çš„è¡¨ä»¥åŠ ORM ç›¸å…³çš„ä»£ç ï¼Œè‡³æ­¤æˆ‘ä»¬å·²ç»å¯¹ ragflow ä¸­çš„æ•°æ®ä»¥åŠå¦‚ä½•æ“ä½œæ•°æ®æœ‰äº†ä¸€å®šçš„äº†è§£ã€‚è¿™ä¸€èŠ‚æˆ‘ä»¬æ¥çœ‹ task exector do_handle_task æ˜¯å¦‚ä½•æ‰§è¡Œ task çš„ã€‚

## 1. do_handle_task æ‰§è¡Œæµç¨‹

do_handler_task çš„ä»£ç å¾ˆé•¿ï¼Œæˆ‘ä»¬å…ˆå€ŸåŠ© ChatGpt æ¥äº†è§£ä¸€ä¸‹ do_handler_task çš„æ‰§è¡Œæµç¨‹ã€‚

---
æ•´ä½“æ¥çœ‹ï¼Œå®ƒæ˜¯ä¸€ä¸ª**æ–‡æ¡£å¤„ç†ä»»åŠ¡å¤„ç†å™¨**ï¼Œè´Ÿè´£ä»æ¥æ”¶åˆ°çš„ä»»åŠ¡å¼€å§‹åˆ°æ–‡æ¡£åˆ‡åˆ†ã€å‘é‡åŒ–ã€å­˜å‚¨çš„å…¨æµç¨‹å¤„ç†ã€‚

### 1.1 å‡½æ•°ç­¾åå’Œåˆå§‹ä¿¡æ¯

```python
async def do_handle_task(task):
    task_id = task["id"]
    task_from_page = task["from_page"]
    task_to_page = task["to_page"]
    task_tenant_id = task["tenant_id"]
    task_embedding_id = task["embd_id"]
    task_language = task["language"]
    task_llm_id = task["llm_id"]
    task_dataset_id = task["kb_id"]
    task_doc_id = task["doc_id"]
    task_document_name = task["name"]
    task_parser_config = task["parser_config"]
    task_start_ts = timer()
```

ä» `task` å¯¹è±¡é‡Œæå–ä»»åŠ¡ç›¸å…³ä¿¡æ¯ï¼Œtask çš„è·å–ä½äº collect å‡½æ•°ï¼š

```python
async def collect(self):
  task = TaskService.get_task(msg["id"])
  task["task_type"] = msg.get("task_type", "")
```


task åŒ…å«å¦‚ä¸‹å­—æ®µ:

| å­—æ®µå | ç±»å‹ | å«ä¹‰ |
|---|---|---|
| id | string | ä»»åŠ¡ IDï¼ˆ`Task.id`ï¼‰ |
| doc_id | string | æ–‡æ¡£ IDï¼ˆå…³è” `Document.id`ï¼‰ |
| from_page | int | èµ·å§‹é¡µ/è¡Œï¼ˆå«ï¼‰ |
| to_page | int | ç»“æŸé¡µ/è¡Œï¼ˆä¸å«æˆ–åŒºé—´ä¸Šé™ï¼Œä¾è§£æå™¨è¯­ä¹‰ï¼‰ |
| retry_count | int | å½“å‰ä»»åŠ¡é‡è¯•æ¬¡æ•° |
| kb_id | string | çŸ¥è¯†åº“ IDï¼ˆ`Document.kb_id`ï¼‰ |
| parser_id | string | æ–‡æ¡£è§£æå™¨ IDï¼ˆ`Document.parser_id`ï¼‰ |
| parser_config | JSON | æ–‡æ¡£çº§è§£æé…ç½®ï¼ˆ`Document.parser_config`ï¼‰ |
| name | string | æ–‡ä»¶åï¼ˆ`Document.name`ï¼‰ |
| type | string | æ–‡ä»¶ç±»å‹/æ‰©å±•åï¼ˆ`Document.type`ï¼‰ |
| location | string | æ–‡ä»¶å­˜å‚¨ä½ç½®/è·¯å¾„ï¼ˆ`Document.location`ï¼‰ |
| size | int | æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼Œ`Document.size`ï¼‰ |
| tenant_id | string | ç§Ÿæˆ· IDï¼ˆæ¥è‡ª `Knowledgebase.tenant_id`ï¼‰ |
| language | string | çŸ¥è¯†åº“è¯­è¨€ï¼ˆEnglish/Chineseï¼Œ`Knowledgebase.language`ï¼‰ |
| embd_id | string | é»˜è®¤å‘é‡æ¨¡å‹ IDï¼ˆ`Knowledgebase.embd_id`ï¼‰ |
| pagerank | int | çŸ¥è¯†åº“ PageRank å€¼ï¼ˆ`Knowledgebase.pagerank`ï¼‰ |
| kb_parser_config | JSON | çŸ¥è¯†åº“çº§è§£æé…ç½®ï¼ˆ`Knowledgebase.parser_config` çš„åˆ«åï¼‰ |
| img2txt_id | string | é»˜è®¤å›¾è½¬æ–‡æ¨¡å‹ IDï¼ˆ`Tenant.img2txt_id`ï¼‰ |
| asr_id | string | é»˜è®¤è¯­éŸ³è¯†åˆ«æ¨¡å‹ IDï¼ˆ`Tenant.asr_id`ï¼‰ |
| llm_id | string | é»˜è®¤å¤§æ¨¡å‹ IDï¼ˆ`Tenant.llm_id`ï¼‰ |
| update_time | int | ä»»åŠ¡æ›´æ–°æ—¶é—´ï¼ˆæ¯«ç§’æ—¶é—´æˆ³ï¼Œ`Task.update_time`ï¼‰ |

task ä¸­æ¯”è¾ƒç‰¹æ®Šçš„æ˜¯ task_typeï¼Œä»–è¡¨ç¤ºäº†æ–‡æ¡£å¤„ç†çš„ä»»åŠ¡ç±»å‹ã€‚

åœ¨ **RagFlow** ä¸­ï¼Œå¤„ç†æ–‡æ¡£å’ŒçŸ¥è¯†çš„ä»»åŠ¡ä¸»è¦æœ‰ä¸‰ç§ç±»å‹ï¼š**RAPTORã€Graphragã€æ ‡å‡† Chunking**ã€‚

#### 1ï¸âƒ£ æ ‡å‡† Chunkingï¼ˆStandard Chunkingï¼‰

**ç”¨é€”**ï¼š

* å°†æ–‡æ¡£æ‹†åˆ†æˆå›ºå®šå¤§å°æˆ–è§„åˆ™çš„ç‰‡æ®µï¼ˆchunkï¼‰ï¼Œç”Ÿæˆå‘é‡è¡¨ç¤ºï¼ˆembeddingï¼‰ï¼Œä¾¿äºæ£€ç´¢ã€‚

**ç‰¹ç‚¹**ï¼š

* å¤„ç†æµç¨‹ç®€å•ï¼šæ–‡æ¡£ â†’ åˆ†å— â†’ embedding â†’ å­˜å‚¨ã€‚
* ä¸ä¾èµ– LLM åšå¤æ‚ç†è§£ï¼Œåªæ˜¯æœºæ¢°æ‹†åˆ†ã€‚
* é€‚åˆå¤§éƒ¨åˆ†åŸºç¡€å‘é‡æ£€ç´¢åœºæ™¯ã€‚

**è¾“å‡º**ï¼š

* æ–‡æ¡£ chunks
* å¯¹åº”å‘é‡ embeddings

---

#### 2ï¸âƒ£ RAPTOR

**ç”¨é€”**ï¼š

* é¢å‘ **æ™ºèƒ½åˆ†å—å’ŒçŸ¥è¯†å¢å¼º** çš„ä»»åŠ¡ã€‚
* å¯¹æ–‡æ¡£è¿›è¡Œ **è¯­ä¹‰ç†è§£**ï¼Œæå–æ›´æ™ºèƒ½çš„å—å†…å®¹ã€‚

**ç‰¹ç‚¹**ï¼š

* ç»“åˆ **LLMï¼ˆChatï¼‰** å’Œ **Embedding æ¨¡å‹**ã€‚
* LLM å¯ä»¥å¯¹æ–‡æ¡£è¿›è¡Œ **æ‘˜è¦ã€å…³é”®å­—æ®µæå–ã€å†…å®¹ä¼˜åŒ–**ï¼Œç”Ÿæˆæ›´æ™ºèƒ½çš„ chunksã€‚
* è¾“å‡º chunks çš„è´¨é‡æ¯”æ ‡å‡† Chunking é«˜ï¼Œå‘é‡è¡¨ç¤ºæ›´æœ‰è¯­ä¹‰ä¿¡æ¯ã€‚

**è¾“å‡º**ï¼š

* æ™ºèƒ½åŒ–çš„æ–‡æ¡£ chunks
* å¯¹åº”å‘é‡ embeddings
* å¯èƒ½å¸¦æœ‰é¢å¤–çš„è¯­ä¹‰æ ‡ç­¾æˆ–ç»“æ„ä¿¡æ¯

---

#### 3ï¸âƒ£ Graphrag

**ç”¨é€”**ï¼š

* ç”¨äº **çŸ¥è¯†å›¾è°±æ„å»ºä»»åŠ¡**ã€‚
* å°†æ–‡æ¡£/æ•°æ®è½¬ä¸ºèŠ‚ç‚¹ã€è¾¹ç»“æ„ï¼Œä¾¿äºåšå›¾æ£€ç´¢ã€å…³ç³»æŸ¥è¯¢ã€‚

**ç‰¹ç‚¹**ï¼š

* ä¾èµ– LLM ç”ŸæˆèŠ‚ç‚¹å’Œè¾¹ï¼ˆå®ä½“å…³ç³»ï¼‰ã€‚
* å¯é€‰ `resolution` æˆ– `community` é…ç½®ï¼Œæ§åˆ¶çŸ¥è¯†å›¾è°±ç»†èŠ‚ã€‚
* ä¸ç›´æ¥ç”Ÿæˆ chunks ç”¨äºå‘é‡æ£€ç´¢ï¼Œè€Œæ˜¯ç”Ÿæˆå›¾ç»“æ„ã€‚

**è¾“å‡º**ï¼š

* çŸ¥è¯†å›¾è°±ï¼ˆèŠ‚ç‚¹ + è¾¹ï¼‰
* å¯é€‰ metadataï¼ˆå¦‚å®ä½“ç±»åˆ«ã€å…³ç³»ç±»å‹ï¼‰

---

#### 4ï¸âƒ£ æ€»ç»“å¯¹æ¯”è¡¨

| ä»»åŠ¡ç±»å‹        | æ˜¯å¦ç”¨ LLM | è¾“å‡ºå†…å®¹                  | ç‰¹ç‚¹/ç”¨é€”                   |
| ----------- | ------- | --------------------- | ----------------------- |
| æ ‡å‡† Chunking | å¦       | æ–‡æ¡£ chunks + embedding | æœºæ¢°æ‹†åˆ†æ–‡æ¡£ï¼Œç”¨äºåŸºç¡€å‘é‡æ£€ç´¢         |
| RAPTOR      | æ˜¯       | æ™ºèƒ½ chunks + embedding | LLMå¢å¼ºçš„è¯­ä¹‰åˆ†å—ï¼Œè´¨é‡æ›´é«˜ï¼Œå¯æå–å…³é”®å­—æ®µ |
| Graphrag    | æ˜¯       | çŸ¥è¯†å›¾è°±ï¼ˆèŠ‚ç‚¹/è¾¹ï¼‰            | æ„å»ºçŸ¥è¯†å›¾è°±ï¼Œé€‚åˆåšå…³ç³»æŸ¥è¯¢ã€å›¾åˆ†æ      |

### 1.2 è¿›åº¦å›è°ƒå’Œè¡¨æ ¼è§£ææ£€æŸ¥

```python
async def do_handle_task(task):
    # ä½¿ç”¨ `partial` ç»‘å®š `task_id` å’Œé¡µç ï¼Œæ–¹ä¾¿æ›´æ–°ä»»åŠ¡è¿›åº¦ã€‚
    progress_callback = partial(set_progress, task_id, task_from_page, task_to_page)

    # Infinity ä¸æ”¯æŒ table parserï¼Œç›´æ¥å¼‚å¸¸é€€å‡º
    lower_case_doc_engine = settings.DOC_ENGINE.lower()
    if lower_case_doc_engine == 'infinity' and task['parser_id'].lower() == 'table':
        error_message = "Table parsing method is not supported by Infinity, please use other parsing methods or use Elasticsearch as the document engine."
        progress_callback(-1, msg=error_message)
        raise Exception(error_message)
    # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«å–æ¶ˆï¼Œå¦‚æœæ˜¯åˆ™ç»ˆæ­¢ä»»åŠ¡å¹¶é€šçŸ¥ç”¨æˆ·ã€‚
    task_canceled = has_canceled(task_id)
    if task_canceled:
        progress_callback(-1, msg="Task has been canceled.")
        return
```


### 1.3 ç»‘å®šå‘é‡æ¨¡å‹å’Œåˆå§‹åŒ–çŸ¥è¯†åº“

```python
async def do_handle_task(task):
    try:
        # * åˆ›å»ºå‘é‡æ¨¡å‹ `embedding_model`ã€‚
        embedding_model = LLMBundle(task_tenant_id, LLMType.EMBEDDING, llm_name=task_embedding_id, lang=task_language)
        # * `is_strong_enough` æ£€æŸ¥æ¨¡å‹èƒ½åŠ›ã€‚
        await is_strong_enough(None, embedding_model)
        # * ç¼–ç æµ‹è¯•æ–‡æœ¬ `"ok"` è·å–å‘é‡ç»´åº¦ `vector_size`ã€‚
        vts, _ = embedding_model.encode(["ok"])
        vector_size = len(vts[0])
    except Exception as e:
        error_message = f'Fail to bind embedding model: {str(e)}'
        progress_callback(-1, msg=error_message)
        logging.exception(error_message)
        raise 
    # * æ ¹æ®ä»»åŠ¡ä¿¡æ¯åˆå§‹åŒ–çŸ¥è¯†åº“ç¯å¢ƒï¼ˆå¯èƒ½æ˜¯ Elasticsearch/Infinity çš„ç´¢å¼•æˆ–æœ¬åœ°å­˜å‚¨ï¼‰ã€‚
    # * `vector_size` ç”¨äºåç»­å‘é‡å­˜å‚¨ã€‚
    # * ä» index_name å¯ä»¥çœ‹åˆ°ï¼Œæ¯ä¸€ä¸ªç§Ÿæˆ·ä¼šåˆ›å»ºä¸€ä¸ªçŸ¥è¯†åº“ç´¢å¼•
    init_kb(task, vector_size)

def index_name(uid): return f"ragflow_{uid}"

def init_kb(row, vector_size: int):
    idxnm = search.index_name(row["tenant_id"])
    return settings.docStoreConn.createIdx(idxnm, row.get("kb_id", ""), vector_size)
```

LLMBundle åˆå§‹åŒ–å­˜åœ¨ä»¥ä¸‹è°ƒç”¨é“¾:

```python
LLMBundle.__init__
LLM4Tenant.__init__
    self.mdl = TenantLLMService.model_instance(tenant_id, llm_type, llm_name, lang=lang, **kwargs)
```

å¦‚ä½•æ ¹æ®ä¸åŒå‚æ•°ï¼Œå®ä¾‹åŒ–ä¸åŒçš„æ¨¡å‹ï¼Œè¿™ä¸ªæ˜¯æˆ‘ä»¬å…³æ³¨çš„é‡ç‚¹ï¼Œæ ‡è®°ä¸º 1ã€‚

### 1.4 æ ¹æ®ä»»åŠ¡ç±»å‹æ‰§è¡Œä¸åŒå¤„ç†æµç¨‹


```python
if task.get("task_type", "") == "raptor":
    ...
elif task.get("task_type", "") == "graphrag":
    ...
else:
    ...
```

#### RAPTOR ä»»åŠ¡

* ç»‘å®š Chat LLMã€‚
* åœ¨ `kg_limiter` é™æµå™¨ä¸‹è°ƒç”¨ `run_raptor` ç”ŸæˆçŸ¥è¯†ç‰‡æ®µã€‚
* è¾“å‡º `chunks` å’Œ `token_count`ã€‚

#### Graphrag ä»»åŠ¡

* æ£€æŸ¥é…ç½®æ˜¯å¦å…è®¸ Graphragã€‚
* ç»‘å®š Chat LLMã€‚
* åœ¨ `kg_limiter` ä¸‹è°ƒç”¨ `run_graphrag` æ„å»ºçŸ¥è¯†å›¾è°±ã€‚
* å®Œæˆåæ›´æ–°è¿›åº¦å¹¶è¿”å›ã€‚

#### Standard chunking ä»»åŠ¡

* ä½¿ç”¨ `build_chunks` å°†æ–‡æ¡£åˆ‡åˆ†æˆå°ç‰‡æ®µï¼ˆchunkï¼‰ã€‚
* å¦‚æœåˆ‡åˆ†å¤±è´¥ï¼Œæ›´æ–°è¿›åº¦å¹¶è¿”å›ã€‚
* å¯¹æ¯ä¸ª chunk ç”Ÿæˆå‘é‡åµŒå…¥ã€‚
* è®°å½• token æ•°é‡å’Œè€—æ—¶

---

### 1.5 å­˜å‚¨ chunks å¹¶æ›´æ–°ä»»åŠ¡çŠ¶æ€

```python
    chunk_count = len(set([chunk["id"] for chunk in chunks]))
    start_ts = timer()
    doc_store_result = ""

    async def delete_image(kb_id, chunk_id):
        try:
            # å¼‚æ­¥åˆ é™¤ MinIO ä¸­å¯¹åº” chunk çš„å›¾ç‰‡ã€‚
            async with minio_limiter:
                STORAGE_IMPL.delete(kb_id, chunk_id)
        except Exception:
            logging.exception(
                "Deleting image of chunk {}/{}/{} got exception".format(task["location"], task["name"], chunk_id))
            raise

        # * æŒ‰æ‰¹æ¬¡å°† chunks å†™å…¥æ–‡æ¡£å­˜å‚¨ï¼ˆæ¯”å¦‚ Elasticsearch æˆ– Infinityï¼‰ã€‚
        # * æ¯æ‰¹å†™å…¥åï¼š
    
    for b in range(0, len(chunks), DOC_BULK_SIZE):    
        doc_store_result = await trio.to_thread.run_sync(lambda: settings.docStoreConn.insert(chunks[b:b + DOC_BULK_SIZE], search.index_name(task_tenant_id), task_dataset_id))
        task_canceled = has_canceled(task_id)
        # * æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«å–æ¶ˆã€‚
        if task_canceled:
            progress_callback(-1, msg="Task has been canceled.")
            return
       
        if b % 128 == 0:
            progress_callback(prog=0.8 + 0.1 * (b + 1) / len(chunks), msg="")
        if doc_store_result:
            error_message = f"Insert chunk error: {doc_store_result}, please check log file and Elasticsearch/Infinity status!"
            progress_callback(-1, msg=error_message)
            raise Exception(error_message)
        chunk_ids = [chunk["id"] for chunk in chunks[:b + DOC_BULK_SIZE]]
        chunk_ids_str = " ".join(chunk_ids)
        try:
             # * è°ƒç”¨ `TaskService.update_chunk_ids` æ›´æ–° chunk IDã€‚
            TaskService.update_chunk_ids(task["id"], chunk_ids_str)
        except DoesNotExist:
            logging.warning(f"do_handle_task update_chunk_ids failed since task {task['id']} is unknown.")
            doc_store_result = await trio.to_thread.run_sync(lambda: settings.docStoreConn.delete({"id": chunk_ids}, search.index_name(task_tenant_id), task_dataset_id))
            # * å‡ºç°å¼‚å¸¸æ—¶ï¼š

            #     * åˆ é™¤å·²å†™å…¥çš„ chunk
            #     * åˆ é™¤å¯¹åº”å›¾ç‰‡ï¼ˆ`delete_image`ï¼‰ã€‚1
            async with trio.open_nursery() as nursery:
                for chunk_id in chunk_ids:
                    nursery.start_soon(delete_image, task_dataset_id, chunk_id)
            progress_callback(-1, msg=f"Chunk updates failed since task {task['id']} is unknown.")
            return

    logging.info("Indexing doc({}), page({}-{}), chunks({}), elapsed: {:.2f}".format(task_document_name, task_from_page,
                                                                                     task_to_page, len(chunks),
                                                                                     timer() - start_ts))
    # è°ƒç”¨ `DocumentService.increment_chunk_num` æ›´æ–°æ–‡æ¡£çš„ chunk æ€»æ•°å’Œ token æ€»æ•°ã€‚
    DocumentService.increment_chunk_num(task_doc_id, task_dataset_id, token_count, chunk_count, 0)

    time_cost = timer() - start_ts
    task_time_cost = timer() - task_start_ts
    progress_callback(prog=1.0, msg="Indexing done ({:.2f}s). Task done ({:.2f}s)".format(time_cost, task_time_cost))
    logging.info(
        "Chunk doc({}), page({}-{}), chunks({}), token({}), elapsed:{:.2f}".format(task_document_name, task_from_page,
                                                                                   task_to_page, len(chunks),
                                                                                   token_count, task_time_cost))
```


## 2. standard chunking ä»»åŠ¡æ‰§è¡Œæµç¨‹
ä¸åŒçš„ task_type æœ‰ä¸åŒçš„æ‰§è¡Œæµç¨‹ï¼Œè¿™é‡Œæˆ‘ä»¬å…ˆæ¥çœ‹æœ€ç®€å•çš„ standard chunking ä»»åŠ¡ã€‚

standard chunking æ‰§è¡Œäº†ä¸¤æ­¥:
1. build_chunks
2. embedding

```python
    else:
        # Standard chunking methods
        start_ts = timer()
        chunks = await build_chunks(task, progress_callback)
        logging.info("Build document {}: {:.2f}s".format(task_document_name, timer() - start_ts))
        if not chunks:
            progress_callback(1., msg=f"No chunk built from {task_document_name}")
            return
        # TODO: exception handler
        ## set_progress(task["did"], -1, "ERROR: ")
        progress_callback(msg="Generate {} chunks".format(len(chunks)))
        start_ts = timer()
        try:
            token_count, vector_size = await embedding(chunks, embedding_model, task_parser_config, progress_callback)
        except Exception as e:
            error_message = "Generate embedding error:{}".format(str(e))
            progress_callback(-1, error_message)
            logging.exception(error_message)
            token_count = 0
            raise
        progress_message = "Embedding chunks ({:.2f}s)".format(timer() - start_ts)
        logging.info(progress_message)
        progress_callback(msg=progress_message)
```

### 2.1 build_chunks
è¿™é‡Œæˆ‘ä»¬è¿˜æ˜¯å€ŸåŠ© ChatGptã€‚

`build_chunks` æ˜¯ä¸€ä¸ª **å¼‚æ­¥æ–‡æ¡£åˆ‡åˆ†ã€å­˜å‚¨ã€å…³é”®è¯/é—®é¢˜ç”Ÿæˆå’Œæ‰“æ ‡ç­¾çš„å®Œæ•´ä»»åŠ¡å¤„ç†å‡½æ•°**ã€‚å®ƒåœ¨å‰é¢ `do_handle_task` ä¸­è¢«è°ƒç”¨ï¼Œç”¨äºæŠŠæ–‡æ¡£æ‹†æˆ chunks å¹¶å¤„ç†ã€‚

---

#### 1ï¸âƒ£ å‡½æ•°ç­¾åä¸å¤§å°æ£€æŸ¥

```python
@timeout(60*80, 1)
async def build_chunks(task, progress_callback):
    if task["size"] > DOC_MAXIMUM_SIZE:
        set_progress(task["id"], prog=-1, msg="File size exceeds( <= %dMb )" % (int(DOC_MAXIMUM_SIZE / 1024 / 1024)))
        return []
```

* ä½¿ç”¨ `@timeout` è£…é¥°å™¨é™åˆ¶æœ€å¤§æ‰§è¡Œæ—¶é—´ï¼ˆè¿™é‡Œ 80 åˆ†é’Ÿï¼‰ã€‚
* æ£€æŸ¥æ–‡æ¡£å¤§å°æ˜¯å¦è¶…è¿‡ç³»ç»Ÿæœ€å¤§é™åˆ¶ï¼Œè¶…è¿‡åˆ™ç›´æ¥è¿”å›ç©ºåˆ—è¡¨ï¼Œå¹¶æ›´æ–°è¿›åº¦ã€‚

---

#### 2ï¸âƒ£ è·å–æ–‡ä»¶å†…å®¹

```python
bucket, name = File2DocumentService.get_storage_address(doc_id=task["doc_id"])
binary = await get_storage_binary(bucket, name)
```

* è·å–æ–‡æ¡£åœ¨ MinIO æˆ–å­˜å‚¨ç³»ç»Ÿä¸­çš„ä½ç½®ã€‚
* å¼‚æ­¥ä¸‹è½½æ–‡æ¡£äºŒè¿›åˆ¶æ•°æ®ã€‚
* å¼‚å¸¸å¤„ç†

    * `TimeoutError` â†’ ä¸‹è½½è¶…æ—¶ï¼Œæ›´æ–°è¿›åº¦å¹¶æŠ›å¼‚å¸¸ã€‚
    * æ–‡ä»¶ä¸å­˜åœ¨ â†’ æ›´æ–°è¿›åº¦æç¤ºç”¨æˆ·æ‰¾ä¸åˆ°æ–‡ä»¶ã€‚
    * å…¶ä»–å¼‚å¸¸ â†’ é€šç”¨æŠ¥é”™å¤„ç†ã€‚

æ–‡ä»¶å¦‚ä½•ä¸Šä¼ ï¼Œå¦‚ä½•ä¸‹è½½ï¼Œè¿™ä¸ªæ˜¯æˆ‘ä»¬å…³æ³¨çš„é‡ç‚¹ï¼Œæ ‡è®°ä¸º 2ã€‚


#### 3ï¸âƒ£ æ–‡æ¡£åˆ‡åˆ†ï¼ˆchunkingï¼‰

```python
chunker = FACTORY[task["parser_id"].lower()]
async with chunk_limiter:
    cks = await trio.to_thread.run_sync(lambda: chunker.chunk(...))
```
* æ ¹æ®ä»»åŠ¡è§£æå™¨ ID é€‰æ‹© `chunker`ï¼ˆä¸åŒç±»å‹æ–‡æ¡£ä½¿ç”¨ä¸åŒè§£ææ–¹æ³•ï¼‰ã€‚

å¦‚ä½•æ ¹æ®æ–‡æ¡£çš„ç±»å‹é€‰æ‹©ä¸åŒçš„ chunkerï¼Œä»¥åŠå¦‚ä½• chunkï¼Œè¿™ä¸ªæ˜¯æˆ‘ä»¬å…³æ³¨çš„é‡ç‚¹ï¼Œæ ‡è®°ä¸º 3ã€‚

---

#### 4ï¸âƒ£ å‡†å¤‡ chunks å…ƒæ•°æ®

```python
docs = []
doc = {
    "doc_id": task["doc_id"],
    "kb_id": str(task["kb_id"])
}
if task["pagerank"]:
    doc[PAGERANK_FLD] = int(task["pagerank"])
```

* ä¸ºæ¯ä¸ª chunk å‡†å¤‡åŸºç¡€å…ƒæ•°æ®ã€‚
* å¦‚æœä»»åŠ¡ä¸­æœ‰ `pagerank` å­—æ®µï¼Œåˆ™å­˜å‚¨ã€‚

---

#### 5ï¸âƒ£ ä¸Šä¼  chunks ä¸­çš„å›¾ç‰‡åˆ° MinIO

```python
async def upload_to_minio(document, chunk):
    ...
```


#### 6ï¸âƒ£ å…³é”®è¯ç”Ÿæˆï¼ˆå¯é€‰ï¼‰

```python
if task["parser_config"].get("auto_keywords", 0):
    ...
```

* ä¸ºæ¯ä¸ª chunk ç”Ÿæˆå…³é”®è¯ï¼š

  * æ£€æŸ¥ç¼“å­˜
  * å¦‚æœæ²¡æœ‰ç¼“å­˜ï¼Œè°ƒç”¨ LLMï¼ˆé€šè¿‡çº¿ç¨‹é˜»å¡æ‰§è¡Œï¼‰
  * æ›´æ–°ç¼“å­˜å¹¶å°†å…³é”®è¯å­˜å…¥ chunk å…ƒæ•°æ®
  * ä½¿ç”¨ `trio.open_nursery` å¹¶å‘å¤„ç†å¤šä¸ª chunk

---

#### 7ï¸âƒ£ é—®é¢˜ç”Ÿæˆï¼ˆå¯é€‰ï¼‰

```python
if task["parser_config"].get("auto_questions", 0):
    ...
```

* ç±»ä¼¼å…³é”®è¯ç”Ÿæˆï¼Œä¸ºæ¯ä¸ª chunk è‡ªåŠ¨ç”Ÿæˆé—®é¢˜ã€‚
* è°ƒç”¨ LLM ç”Ÿæˆé—®é¢˜ã€‚
* ç¼“å­˜ç»“æœå¹¶æ›´æ–° chunk å…ƒæ•°æ®ã€‚

---

#### 8ï¸âƒ£ æ–‡æ¡£å†…å®¹æ‰“æ ‡ç­¾ï¼ˆå¯é€‰ï¼‰


```python
if task["kb_parser_config"].get("tag_kb_ids", []):
    ...
```

* è·å– KB ä¸­å·²æœ‰æ ‡ç­¾ã€‚
* éå† chunksï¼š

  * è°ƒç”¨ `retrievaler.tag_content` æ ¹æ®å†…å®¹æ‰“æ ‡ç­¾
  * å¦‚æœæ²¡æœ‰æ ‡ç­¾ï¼ŒåŠ å…¥ `docs_to_tag`
* å¼‚æ­¥è°ƒç”¨ LLM è¿›è¡Œå†…å®¹æ‰“æ ‡ç­¾ï¼Œå¹¶ç¼“å­˜ç»“æœï¼š

  * ä½¿ç”¨å°‘é‡ç¤ºä¾‹ `examples` æä¾›ä¸Šä¸‹æ–‡
  * å¹¶å‘å¤„ç†æ¯ä¸ª chunk

---

### 2.2 RAG å¢å¼º
åˆ°è¿™é‡Œï¼Œå…¶å®æˆ‘ä¸å¤ªæ˜ç™½ï¼Œ**å…³é”®è¯ç”Ÿæˆã€é—®é¢˜ç”Ÿæˆã€æ–‡æ¡£å†…å®¹æ‰“æ ‡ç­¾** æ˜¯å¹²ä»€ä¹ˆçš„ï¼Œæ‰€ä»¥æˆ‘ä»¬å‘ ChatGpt æäº†ä¸€ä¸ªé—®é¢˜:

```bash
èƒ½ä¸èƒ½ç»™æˆ‘ä»‹ç»ä¸€ä¸‹ ragflow ä¸­ å…³é”®è¯ç”Ÿæˆã€é—®é¢˜ç”Ÿæˆã€æ–‡æ¡£å†…å®¹æ‰“æ ‡ç­¾ çš„èƒŒæ™¯ï¼Œæ˜¯å¹²ä»€ä¹ˆçš„
```

**å…³é”®è¯ç”Ÿæˆ**ã€**é—®é¢˜ç”Ÿæˆ**ã€**æ–‡æ¡£å†…å®¹æ‰“æ ‡ç­¾**ï¼Œå…¶å®éƒ½æ˜¯ **RAGï¼ˆRetrieval-Augmented Generationï¼‰** ç³»ç»Ÿä¸­å¾ˆé‡è¦çš„â€œå¢å¼ºæ£€ç´¢ä¸é—®ç­”ä½“éªŒâ€çš„åŠŸèƒ½ã€‚

---

#### èƒŒæ™¯ï¼šRAG çš„æ ¸å¿ƒé—®é¢˜

RAG çš„ç›®æ ‡æ˜¯ï¼š

1. ç”¨æˆ·æé—®ï¼ˆQueryï¼‰
2. ç³»ç»Ÿåœ¨çŸ¥è¯†åº“é‡Œæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ç‰‡æ®µï¼ˆChunksï¼‰
3. æŠŠè¿™äº›ç‰‡æ®µå–‚ç»™å¤§æ¨¡å‹ï¼Œç”Ÿæˆå›ç­”

ä½†æ˜¯è¿™é‡Œé¢æœ‰ä¸¤ä¸ªæŒ‘æˆ˜ï¼š

* **æ£€ç´¢ç²¾åº¦**ï¼šå¦‚ä½•ä¿è¯ query èƒ½å‡†ç¡®åŒ¹é…åˆ°åˆé€‚çš„æ–‡æ¡£ç‰‡æ®µï¼Ÿ
* **è¡¨è¾¾å·®å¼‚**ï¼šç”¨æˆ·çš„æé—®æ–¹å¼å’Œæ–‡æ¡£åŸæ–‡ä¸ä¸€æ ·ï¼Œå•é å‘é‡ç›¸ä¼¼åº¦æœ‰æ—¶åŒ¹é…ä¸åˆ°ã€‚

æ‰€ä»¥ï¼ŒRAGFlow é‡Œå¼•å…¥äº† **å…³é”®è¯ç”Ÿæˆã€é—®é¢˜ç”Ÿæˆã€æ ‡ç­¾ç”Ÿæˆ** è¿™ç±» **è¾…åŠ©å¢å¼ºç­–ç•¥**ã€‚

---

#### ğŸ”‘ (1) å…³é”®è¯ç”Ÿæˆï¼ˆauto_keywordsï¼‰

* **åšä»€ä¹ˆ**ï¼šå¯¹æ¯ä¸ªæ–‡æ¡£ç‰‡æ®µè‡ªåŠ¨æå–ä¸€ç»„å…³é”®çŸ­è¯­/å…³é”®è¯ã€‚
* **ç›®çš„**ï¼š

  * ä½œä¸ºè¾…åŠ©ç´¢å¼•ï¼šé™¤äº† embeddingï¼Œè¿˜å¯ä»¥ç”¨å…³é”®è¯ï¼ˆå€’æ’ç´¢å¼•ï¼‰å¿«é€Ÿç­›é€‰å€™é€‰æ–‡æ¡£ã€‚
  * æå‡å¬å›ç‡ï¼šå½“ç”¨æˆ· query é‡ŒåŒ…å«å…³é”®è¯æ—¶ï¼Œå¯ä»¥ç›´æ¥å¿«é€ŸåŒ¹é…ã€‚
  * å¸®åŠ©ä¸»é¢˜èšç±»æˆ–å¯è§†åŒ–ã€‚

ğŸ‘‰ ä¸¾ä¾‹ï¼š
æ–‡æ¡£ç‰‡æ®µï¼š

> â€œRAGFlow æ”¯æŒä» MinIO åŠ è½½æ–‡æ¡£ï¼Œå¹¶è¿›è¡Œåˆ†å—å¤„ç†ã€‚â€
> è‡ªåŠ¨ç”Ÿæˆå…³é”®è¯ï¼š
> `["RAGFlow", "MinIO", "æ–‡æ¡£åˆ†å—"]`

---

#### â“ (2) é—®é¢˜ç”Ÿæˆï¼ˆauto_questionsï¼‰

* **åšä»€ä¹ˆ**ï¼šåŸºäºæ–‡æ¡£ç‰‡æ®µï¼Œè‡ªåŠ¨ç”Ÿæˆä¸€ç»„â€œå¯èƒ½çš„é—®é¢˜â€ã€‚
* **ç›®çš„**ï¼š

  * æ¨¡æ‹Ÿç”¨æˆ·æé—®ï¼Œæå‰å‡†å¤‡å¥½ Queryâ€“Chunk çš„æ˜ å°„ï¼Œæå‡ **queryâ€“chunk åŒ¹é…èƒ½åŠ›**ã€‚
  * ä½œä¸ºè®­ç»ƒæ•°æ®ï¼Œç”¨äºè®­ç»ƒ / å¾®è°ƒ embedding æ¨¡å‹æˆ– rerankerã€‚
  * è®©ç³»ç»Ÿæ›´å¥½æ”¯æŒå¤šæ ·åŒ–çš„é—®æ³•ã€‚

ğŸ‘‰ ä¸¾ä¾‹ï¼š
æ–‡æ¡£ç‰‡æ®µï¼š

> â€œRAGFlow å¯ä»¥è‡ªåŠ¨æå–å…³é”®è¯æ¥å¢å¼ºæ£€ç´¢ã€‚â€
> è‡ªåŠ¨ç”Ÿæˆé—®é¢˜ï¼š

* â€œRAGFlow å¦‚ä½•å¢å¼ºæ£€ç´¢ï¼Ÿâ€
* â€œå®ƒæ”¯æŒå…³é”®è¯æå–å—ï¼Ÿâ€

è¿™æ ·ï¼Œç”¨æˆ·é—® â€œRAGFlow æ€ä¹ˆåšæ£€ç´¢å¢å¼ºï¼Ÿâ€ æ—¶ï¼Œå³ä½¿ embedding æ²¡æœ‰å®Œå…¨åŒ¹é…ï¼Œç³»ç»Ÿä¹Ÿèƒ½é€šè¿‡é—®é¢˜æ‰©å±•æ‰¾åˆ°ç›¸å…³ç‰‡æ®µã€‚

---

#### ğŸ· (3) æ–‡æ¡£å†…å®¹æ‰“æ ‡ç­¾ï¼ˆcontent taggingï¼‰

* **åšä»€ä¹ˆ**ï¼šå¯¹æ¯ä¸ªæ–‡æ¡£ç‰‡æ®µï¼Œè‡ªåŠ¨æ‰“ä¸Šä¸»é¢˜æ ‡ç­¾ï¼ˆTagï¼‰ï¼Œå¯èƒ½æ˜¯ç±»åˆ«ã€é¢†åŸŸã€åŠŸèƒ½ç­‰ã€‚
* **ç›®çš„**ï¼š

  * è®©æ–‡æ¡£æœ‰æ›´æ¸…æ™°çš„åˆ†ç±»ä½“ç³»ï¼Œæ”¯æŒ **æŒ‰ä¸»é¢˜è¿‡æ»¤/æ£€ç´¢**ã€‚
  * æå‡çŸ¥è¯†åº“å¯è§£é‡Šæ€§ï¼ˆç”¨æˆ·/è¿è¥èƒ½çœ‹åˆ°æ–‡æ¡£çš„è¯­ä¹‰åˆ†ç±»ï¼‰ã€‚
  * åœ¨å¤šçŸ¥è¯†åº“ã€å¤šé¢†åŸŸåœºæ™¯ä¸‹ï¼Œç”¨æ ‡ç­¾åŒºåˆ†è¯­å¢ƒã€‚

ğŸ‘‰ ä¸¾ä¾‹ï¼š
æ–‡æ¡£ç‰‡æ®µï¼š

> â€œRAGFlow æ”¯æŒå‘é‡æ£€ç´¢å’Œå…³é”®è¯æ£€ç´¢æ··åˆã€‚â€
> æ‰“æ ‡ç­¾ç»“æœï¼š
> `{"æ£€ç´¢": 0.9, "RAGFlowåŠŸèƒ½": 0.8}`

è¿™æ ·ï¼Œåœ¨ç”¨æˆ· query â€œæ··åˆæ£€ç´¢â€ æ—¶ï¼Œå³ä½¿æ–‡æœ¬ embedding ä¸å®Œå…¨å¯¹é½ï¼Œä¹Ÿå¯ä»¥å€ŸåŠ©æ ‡ç­¾åŒ¹é…ã€‚

---

#### åº”ç”¨åœºæ™¯

* **çŸ¥è¯†åº“å»ºè®¾**ï¼šå¤§æ‰¹æ–‡æ¡£è‡ªåŠ¨å¤„ç†ï¼Œç”Ÿæˆå…³é”®è¯/æ ‡ç­¾/é—®é¢˜ï¼Œå‡å°‘äººå·¥æ ‡æ³¨ã€‚
* **æ£€ç´¢å¢å¼º**ï¼šç»“åˆå‘é‡æœç´¢ï¼ˆè¯­ä¹‰ï¼‰+ å…³é”®è¯æœç´¢ï¼ˆç²¾ç¡®ï¼‰+ æ ‡ç­¾æœç´¢ï¼ˆåˆ†ç±»ï¼‰ï¼Œæå‡å‡†ç¡®ç‡ã€‚
* **å¯è§†åŒ–ç®¡ç†**ï¼šè®©è¿è¥äººå‘˜èƒ½æŒ‰å…³é”®è¯ã€æ ‡ç­¾ã€é—®é¢˜æ¥æŸ¥çœ‹å’Œç»´æŠ¤çŸ¥è¯†åº“ã€‚
* **å¤šæ¨¡æ€æ‰©å±•**ï¼šæ¯”å¦‚å›¾ç‰‡å—ã€è¡¨æ ¼å—ä¹Ÿèƒ½ç”Ÿæˆå…³é”®è¯/æ ‡ç­¾ï¼Œæ–¹ä¾¿åç»­æ£€ç´¢ã€‚

---

âœ… **æ€»ç»“**ï¼š
åœ¨ RAGFlow ä¸­ï¼š

* **å…³é”®è¯ç”Ÿæˆ** â†’ æå‡æ£€ç´¢è¦†ç›–é¢
* **é—®é¢˜ç”Ÿæˆ** â†’ æå‡ queryâ€“chunk åŒ¹é…èƒ½åŠ›
* **å†…å®¹æ‰“æ ‡ç­¾** â†’ æå‡æ–‡æ¡£ç»„ç»‡å’Œæ£€ç´¢ç²¾åº¦

å®ƒä»¬çš„æ ¸å¿ƒä»·å€¼å°±æ˜¯ï¼š**è®©ç”¨æˆ·æé—®å’ŒçŸ¥è¯†åº“é‡Œçš„æ–‡æ¡£ä¹‹é—´çš„è¯­ä¹‰é¸¿æ²Ÿæ›´å°ï¼Œæ£€ç´¢æ›´å‡†ï¼Œå›ç­”æ›´å¥½ã€‚**

æ˜¾ç„¶**å…³é”®è¯ç”Ÿæˆ**ã€**é—®é¢˜ç”Ÿæˆ**ã€**æ–‡æ¡£å†…å®¹æ‰“æ ‡ç­¾**ï¼Œè¿™ä¸ªæ˜¯æˆ‘ä»¬å…³æ³¨çš„é‡ç‚¹ï¼Œæ ‡è®°ä¸º 4ã€‚

### 2.3 embedding
å¦‚ä½•æ ¹æ®å‰é¢ç”Ÿæˆçš„æ‰€æœ‰çš„å†…å®¹åš embeddingï¼Œè¿™ä¸ªå¿…é¡»ç­‰åˆ°æˆ‘ä»¬äº†è§£äº†å‰é¢çš„å†…å®¹æ‰èƒ½ç†è§£ã€‚è¿™ä¸ªæ˜¯æˆ‘ä»¬å…³æ³¨çš„é‡ç‚¹ï¼Œæ ‡è®°ä¸º 5ã€‚

è‡³æ­¤ï¼Œæˆ‘ä»¬æ ‡è®°äº† 5 ä¸ªé—®é¢˜:
1. `TenantLLMService.model_instance` å¦‚ä½•å®ä¾‹åŒ–æ¨¡å‹
2. å¦‚ä½•æ ¹æ®æ–‡æ¡£çš„ç±»å‹é€‰æ‹©ä¸åŒçš„ chunker
3. **å…³é”®è¯ç”Ÿæˆ**ã€**é—®é¢˜ç”Ÿæˆ**ã€**æ–‡æ¡£å†…å®¹æ‰“æ ‡ç­¾** çš„æ‰§è¡Œæµç¨‹
4. å¦‚ä½• embedding
2. æ–‡ä»¶å¦‚ä½•ä¸Šä¼ ï¼Œå¦‚ä½•ä¸‹è½½ã€‚æ›´ä¸€æ­¥æ˜¯è§£æåçš„ chunk å¦‚ä½•åœ¨ MinIO å’Œ ES ä¸­å­˜å‚¨


ä¸‹é¢æ˜¯æˆ‘é€šè¿‡æ–­ç‚¹ï¼Œè·å–çš„ä¸€ä¸ª task å‚æ•°ï¼Œä»¥åŠæ•´ç†çš„æ ¸å¿ƒè°ƒç”¨é“¾çš„æ‰§è¡Œè¿‡ç¨‹:

```python
{
	'id': '4d60226c7fde11f08e4c99abb39df9de',
	'doc_id': '6f4757087f7311f088ac639e5785ef57',
	'from_page': 0,
	'to_page': 100000000,
	'retry_count': 0,
	'kb_id': 'a37e77f87f6f11f088ac639e5785ef57',
	'parser_id': 'naive',
	'parser_config': {
		'pages': [...]
	},
	'name': '47_langgraph_summary.md',
	'type': 'doc',
	'location': '47_langgraph_summary.md',
	'size': 35828,
	'tenant_id': '4d6e8fd47f6e11f088ac639e5785ef57',
	'language': 'Chinese',
	'embd_id': 'BAAI/bge-large-zh-v1.5@BAAI',
	'pagerank': 0,
	'kb_parser_config': {
		'pages': [...]
	},
	'img2txt_id': '',
	'asr_id': '',
	'llm_id': '',
	'update_time': 1755925320629,
	'task_type': ''
}


```

è°ƒç”¨é“¾:

```python
# å®ä¾‹åŒ– embedding_model
embedding_model = LLMBundle(
            tenant_id="4d6e8fd47f6e11f088ac639e5785ef57",
            llm_type="embedding",
            llm_name="BAAI/bge-large-zh-v1.5@BAAI",
            lang="Chinese",
        )
    LLM4Tenant.__init___
        self.mdl = TenantLLMService.model_instance(tenant_id, llm_type, llm_name, lang=lang, **kwargs)
            # è·å–æ¨¡å‹é…ç½®
            model_config = TenantLLMService.get_model_config(tenant_id, llm_type, llm_name)
            # åŠ è½½æ¨¡å‹
            EmbeddingModel[model_config["llm_factory"]](model_config["api_key"], model_config["llm_name"], base_url=model_config["api_base"])
        model_config = TenantLLMService.get_model_config(tenant_id, llm_type, llm_name)

# åˆ†å—
chunks = await build_chunks(task, progress_callback)
    # bucket=kb_id, name=47_langgraph_summary.md
    bucket, name = File2DocumentService.get_storage_address(doc_id=task["doc_id"])
    # ä» minio ä¸­ä¸‹è½½ 47_langgraph_summary.md
    binary = await get_storage_binary(bucket, name)

    # è·å–æ–‡ä»¶è§£æå™¨
    chunker = FACTORY[task["parser_id"].lower()]
    # naive.chunk ä¼šæ ¹æ®æ–‡ä»¶åä¸­çš„æ–‡ä»¶ç±»å‹ï¼Œå®ä¾‹åŒ–ä¸åŒçš„ parser è¿›è¡Œchunk
    chunker.chunk(
                    task["name"],
                    binary=binary,
                    from_page=task["from_page"],
                    to_page=task["to_page"],
                    lang=task["language"],
                    callback=progress_callback,
                    kb_id=task["kb_id"],
                    parser_config=task["parser_config"],
                    tenant_id=task["tenant_id"],
                )
    upload_to_minio
    doc_question_proposal
    doc_content_tagging
embedding
```
