---
weight: 1
title: "RagFlow NLP Tokenizer"

date: 2025-08-20T11:00:00+08:00
lastmod: 2025-08-20T11:00:00+08:00
draft: false
author: "å®‹æ¶›"
authorLink: "https://hotttao.github.io/"
description: "RagFlow NLP Tokenizer"
featuredImage: 

tags: ["RAG", "RagFlow"]
categories: ["langchain"]

lightgallery: true

toc:
  auto: false
---

RagTokenizer æ˜¯ Ragflow å®ç°çš„åˆ†è¯å™¨ï¼Œè¿™ä¸€èŠ‚æˆ‘ä»¬æ¥ä»‹ç»å®ƒçš„å®ç°ã€‚

## 1. Tokenizer
NLP æˆ‘äº†è§£çš„ä¸å¤šï¼Œæ‰€ä»¥æˆ‘å‘ ChatGPT æäº†ä»¥ä¸‹é—®é¢˜:

```bash
1. ä»€ä¹ˆæ˜¯ Tokenizer 
2. ä¼ ç»Ÿçš„ Tokenizer ä¸ åŸºäº LLM çš„ Tokenizer æœ‰ä»€ä¹ˆåŒºåˆ« 
3. å¼€æºé¡¹ç›®ä¸­ï¼Œä½¿ç”¨æœ€å¤šçš„ä¼ ç»Ÿ Tokenizer å’Œ LLM Tokenizer åˆ†åˆ«æ˜¯ä»€ä¹ˆï¼Œè¯·ç»™å‡ºä½¿ç”¨ç¤ºä¾‹
```

ä¸‹é¢æ˜¯ ChatGPT çš„å›ç­”ï¼ŒåŸºæœ¬ä¸Šèƒ½å¸®åŠ©æˆ‘ä»¬ç†è§£åˆ†è¯å™¨åˆ°åº•æ˜¯ä»€ä¹ˆã€‚

### 1ï¸âƒ£ Tokenizer æ˜¯ä»€ä¹ˆ
Tokenizerï¼ˆåˆ†è¯å™¨ï¼‰**æ˜¯å°†æ–‡æœ¬æ‹†åˆ†ä¸º**æœ€å°å•å…ƒï¼ˆtokensï¼‰çš„å·¥å…·ï¼Œè¿™äº›å•å…ƒå¯ä»¥æ˜¯å•è¯ã€å­è¯ã€å­—ç¬¦æˆ–å…¶ä»–ç¬¦å·ã€‚

* **ä½œç”¨**ï¼š

  1. å°†åŸå§‹æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯ä»¥ç†è§£çš„æ•°å­—åºåˆ—ï¼ˆé€šå¸¸æ˜¯ token idï¼‰ã€‚
  2. æ”¯æŒæ¨¡å‹è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­çš„æ–‡æœ¬å¤„ç†ã€‚

* **ä¸¾ä¾‹**ï¼š

```python
text = "æˆ‘çˆ±è‡ªç„¶è¯­è¨€å¤„ç†"
tokens = ["æˆ‘", "çˆ±", "è‡ªç„¶", "è¯­è¨€", "å¤„ç†"]
```

åœ¨ NLP æ¨¡å‹ä¸­ï¼Œè¿™äº› tokens ä¼šè¿›ä¸€æ­¥æ˜ å°„æˆ id ä¾›æ¨¡å‹ä½¿ç”¨ã€‚

---

### 2ï¸âƒ£ ä¼ ç»Ÿ Tokenizer ä¸ åŸºäº LLM çš„ Tokenizer çš„åŒºåˆ«

| ç‰¹æ€§    | ä¼ ç»Ÿ Tokenizer                                | åŸºäº LLM çš„ Tokenizer                          |
| ----- | ------------------------------------------- | ------------------------------------------- |
| åˆ†è¯ç­–ç•¥  | é€šå¸¸è§„åˆ™æˆ–ç»Ÿè®¡æ–¹æ³•ï¼Œå¦‚ç©ºæ ¼åˆ†è¯ã€BPEã€WordPieceã€SentencePiece | ç”± LLM è®­ç»ƒä¸­å­¦ä¹ åˆ°çš„ subword æˆ– context-aware token |
| ä¸Šä¸‹æ–‡æ„è¯† | æ— ï¼Œå®Œå…¨æ ¹æ®è§„åˆ™æˆ–å­—å…¸                                 | æœ‰ï¼Œèƒ½æ ¹æ®ä¸Šä¸‹æ–‡è°ƒæ•´ token åˆ’åˆ†                         |
| é€‚åº”æ€§   | å›ºå®šå­—å…¸ï¼Œæ–°å¢è¯éœ€è¦é‡æ–°è®­ç»ƒæˆ–æ‰©å……                           | å¯åŠ¨æ€é€‚åº”è¾“å…¥æ–‡æœ¬å’Œæ¨¡å‹éœ€æ±‚                              |
| ç²¾åº¦    | å¯¹å¸¸è§è¯­è¨€è¡¨ç°å¥½ï¼Œä½†å¯¹æ–°è¯ã€é”™åˆ«å­—ã€å¤–æ–‡å¯èƒ½å·®                     | å¯¹å¤æ‚ã€å¤šè¯­ç§ã€æ··åˆæ–‡æœ¬è¡¨ç°æ›´å¥½                            |
| è®¡ç®—å¼€é”€  | è¾ƒä½                                          | è¾ƒé«˜ï¼ˆå› ä¸ºå¯èƒ½è°ƒç”¨ LLM æœ¬èº«è¿›è¡Œç¼–ç ï¼‰                       |

**ä¸¾ä¾‹ï¼š**

ä¼ ç»Ÿ BPE åˆ†è¯å™¨å¯¹è‹±æ–‡å¥å­ï¼š

```python
from transformers import RobertaTokenizer

tokenizer = RobertaTokenizer.from_pretrained("roberta-base")
tokens = tokenizer.tokenize("I love NLP")
print(tokens)
# è¾“å‡º: ['I', 'Ä love', 'Ä N', 'L', 'P']
```

LLM Tokenizerï¼ˆå¦‚ ChatGLM ä½¿ç”¨çš„ tokenizerï¼Œå¯ä»¥å¯¹ä¸­æ–‡æ›´æ™ºèƒ½åˆ†è¯ï¼‰ï¼š

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True)
tokens = tokenizer.tokenize("æˆ‘çˆ±è‡ªç„¶è¯­è¨€å¤„ç†")
print(tokens)
# è¾“å‡ºå¯èƒ½ä¸º: ['æˆ‘', 'çˆ±', 'è‡ªç„¶è¯­è¨€', 'å¤„ç†']
```

åŒºåˆ«åœ¨äº LLM Tokenizer æ›´èƒ½ç†è§£è¯è¯­ç»„åˆçš„è¯­ä¹‰ï¼Œä¸ä»…ä»…æ˜¯å­—ç¬¦æ‹†åˆ†ã€‚

---

### 3ï¸âƒ£ å¼€æºé¡¹ç›®ä¸­æœ€å¸¸ç”¨çš„ Tokenizer

#### ğŸ”¹ ä¼ ç»Ÿ Tokenizer

* **BPEï¼ˆByte-Pair Encodingï¼‰**: GPT ç³»åˆ—æ—©æœŸä½¿ç”¨
* **WordPiece**: BERT ç³»åˆ—ä½¿ç”¨
* **SentencePiece**: å¤šè¯­ç§æ¨¡å‹å¸¸ç”¨ï¼ˆå¦‚ mBERT, T5ï¼‰

**ç¤ºä¾‹ï¼ˆBERT WordPieceï¼‰ï¼š**

```python
from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained("bert-base-chinese")
text = "æˆ‘çˆ±è‡ªç„¶è¯­è¨€å¤„ç†"
tokens = tokenizer.tokenize(text)
ids = tokenizer.convert_tokens_to_ids(tokens)
print(tokens, ids)
```

---

#### ğŸ”¹ LLM Tokenizer

* **HuggingFace çš„ `AutoTokenizer` + trust_remote_code=True** å¯ä»¥åŠ è½½å¤§éƒ¨åˆ†åŸºäº LLM çš„ tokenizerï¼ˆå¦‚ ChatGLM, LLaMA, MPTï¼‰
* ä¼˜åŠ¿ï¼šèƒ½æ›´å¥½å¤„ç†é•¿æ–‡æœ¬ã€å¤šè¯­ç§ã€ç‰¹æ®Šç¬¦å·å’Œä¸Šä¸‹æ–‡ç›¸å…³åˆ†è¯

**ç¤ºä¾‹ï¼ˆChatGLM tokenizerï¼‰ï¼š**

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True)
text = "æˆ‘çˆ±è‡ªç„¶è¯­è¨€å¤„ç†"
tokens = tokenizer.tokenize(text)
ids = tokenizer.encode(text)
print(tokens, ids)
```

---

æ€»ç»“ï¼š

* **ä¼ ç»Ÿ tokenizer**ï¼šè§„åˆ™/ç»Ÿè®¡é©±åŠ¨ï¼Œå¿«é€Ÿã€ç¨³å®šï¼Œä½†å¯¹ä¸Šä¸‹æ–‡å’Œæ–°è¯èƒ½åŠ›æœ‰é™ã€‚
* **LLM tokenizer**ï¼šä¸Šä¸‹æ–‡æ•æ„Ÿï¼Œèƒ½å¤„ç†å¤æ‚ã€å¤šè¯­ç§åœºæ™¯ï¼Œä½†å¼€é”€è¾ƒå¤§ã€‚
* **å¼€æºæœ€å¸¸ç”¨**ï¼š

  * ä¼ ç»Ÿï¼šBPEã€WordPieceã€SentencePiece
  * LLMï¼šHuggingFace AutoTokenizer + æ¨¡å‹è‡ªå¸¦ tokenizer

RagFlow å®ç°çš„ RagTokenizer æ˜¯ä¼ ç»Ÿ tokenizerã€‚åœ¨ RagTokenizer çš„å®ç°ä¸­ç”¨åˆ°äº† NLTK å’Œ datrieï¼Œæˆ‘ä»¬å…ˆæ¥äº†è§£ä¸€ä¸‹è¿™ä¸¤ä¸ªå·¥å…·ã€‚


## 2. NLTK

### 2.1 NLTK æ˜¯ä»€ä¹ˆï¼Ÿ

**NLTK (Natural Language Toolkit)** æ˜¯ä¸€ä¸ªç»å…¸çš„ Python è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰åº“ã€‚
å®ƒæä¾›äº†å¾ˆå¤š NLP å·¥å…·ï¼Œæ¯”å¦‚ï¼š

* åˆ†è¯ï¼ˆæŠŠå¥å­æ‹†æˆå•è¯/æ ‡ç‚¹ï¼‰
* è¯å¹²æå–ï¼ˆæå–å•è¯çš„â€œè¯æ ¹â€ï¼‰
* è¯å½¢è¿˜åŸï¼ˆæŠŠå•è¯å˜æˆæ ‡å‡†è¯å…¸é‡Œçš„å½¢å¼ï¼‰
* è¯­æ–™åº“ï¼ˆå¸¸è§çš„è‹±æ–‡æ–‡æœ¬æ•°æ®ï¼‰
* è¯æ€§æ ‡æ³¨ã€å¥æ³•åˆ†æç­‰ç­‰

---

### 2.2 ä»£ç ä¸­çš„ä¸‰ä¸ªæ¨¡å—

#### (1) `word_tokenize`

* åŠŸèƒ½ï¼šæŠŠè‹±æ–‡å¥å­åˆ†å‰²æˆå•è¯å’Œæ ‡ç‚¹ã€‚
* ä¾‹å­ï¼š

  ```python
  from nltk import word_tokenize
  sentence = "Cats are running faster than dogs."
  tokens = word_tokenize(sentence)
  print(tokens)
  ```

  è¾“å‡ºï¼š

  ```python
  ['Cats', 'are', 'running', 'faster', 'than', 'dogs', '.']
  ```

  ğŸ‘‰ å°±æ˜¯ä¸€ä¸ªä¸ªâ€œè¯å…ƒâ€ã€‚

---

#### (2) `PorterStemmer`

* åŠŸèƒ½ï¼š**è¯å¹²æå– (stemming)**ï¼ŒæŠŠå•è¯â€œå‰Šå‡â€æˆä¸€ä¸ªåŸºç¡€å½¢å¼ï¼ˆä¸ä¸€å®šæ˜¯çœŸæ­£çš„è¯ï¼Œåªæ˜¯ç»Ÿä¸€æˆä¸€ä¸ªæ ¹å½¢å¼ï¼‰ã€‚
* ä¾‹å­ï¼š

  ```python
  from nltk.stem import PorterStemmer
  stemmer = PorterStemmer()
  print(stemmer.stem("running"))   # run
  print(stemmer.stem("better"))    # better ï¼ˆæ²¡æœ‰å˜åŒ–ï¼‰
  ```

  ğŸ‘‰ `running â†’ run`ï¼Œä½† `better` è¿˜æ˜¯ `better`ï¼Œå› ä¸ºå®ƒåªæ˜¯ç®€å•è§„åˆ™ï¼Œä¸æŸ¥å­—å…¸ã€‚

---

#### (3) `WordNetLemmatizer`

* åŠŸèƒ½ï¼š**è¯å½¢è¿˜åŸ (lemmatization)**ï¼ŒæŠŠå•è¯è½¬åŒ–æˆå­—å…¸é‡Œæ ‡å‡†çš„å½¢å¼ï¼ˆéœ€è¦è¯å…¸æ”¯æŒï¼‰ã€‚
* ä¾‹å­ï¼š

  ```python
  from nltk.stem import WordNetLemmatizer
  lemmatizer = WordNetLemmatizer()
  print(lemmatizer.lemmatize("running", pos="v"))  # run
  print(lemmatizer.lemmatize("better", pos="a"))   # good
  ```

  ğŸ‘‰ è¿™ä¸ªæ¯” stemming æ›´æ™ºèƒ½ï¼Œæ¯”å¦‚ `better â†’ good`ã€‚

---

### 2.3 åŒºåˆ«æ€»ç»“

| å·¥å…·                  | ä½œç”¨         | ç¤ºä¾‹                                            |
| ------------------- | ---------- | --------------------------------------------- |
| `word_tokenize`     | åˆ†è¯         | `"I like cats." â†’ ['I', 'like', 'cats', '.']` |
| `PorterStemmer`     | è¯å¹²åŒ–ï¼ˆç®€å•è§„åˆ™ï¼‰  | `"running" â†’ "run"`ï¼Œ`"studies" â†’ "studi"`     |
| `WordNetLemmatizer` | è¯å½¢è¿˜åŸï¼ˆåŸºäºè¯å…¸ï¼‰ | `"running" â†’ "run"`ï¼Œ`"better" â†’ "good"`       |


## 3. datrie

`datrie` æ˜¯ Python çš„ä¸€ä¸ª **åŒæ•°ç»„ Trieï¼ˆDouble-Array Trieï¼‰** å®ç°ï¼Œç”¨äºé«˜æ•ˆçš„**å‰ç¼€åŒ¹é…**å’Œ**å­—ç¬¦ä¸²æŸ¥æ‰¾**ã€‚

* **Trie**ï¼ˆå­—å…¸æ ‘ï¼‰æ˜¯ä¸€ç§æ ‘å½¢ç»“æ„ï¼Œé€‚åˆå­˜å‚¨å¤§é‡å­—ç¬¦ä¸²ï¼Œæ”¯æŒï¼š

  * å¿«é€ŸæŸ¥æ‰¾å­—ç¬¦ä¸²æ˜¯å¦å­˜åœ¨
  * æŸ¥æ‰¾æ‰€æœ‰ä»¥æŸä¸ªå‰ç¼€å¼€å¤´çš„å­—ç¬¦ä¸²
  * å¯ä»¥å­˜å‚¨å­—ç¬¦ä¸²å¯¹åº”çš„å€¼ï¼ˆæƒé‡ã€ID ç­‰ï¼‰

* **åŒæ•°ç»„ Trie** æ˜¯ Trie çš„ä¸€ç§é«˜æ•ˆå­˜å‚¨å®ç°ï¼š

  * å†…å­˜å ç”¨å°
  * æŸ¥æ‰¾é€Ÿåº¦éå¸¸å¿«
  * ç‰¹åˆ«é€‚åˆä¸­æ–‡åˆ†è¯è¯å…¸ã€æ‹¼å†™æ£€æŸ¥ã€æœç´¢å¼•æ“è¯è¡¨ç­‰åœºæ™¯

---

### 3.1 `datrie` çš„ç‰¹ç‚¹

1. **æ”¯æŒ Unicode**

   * å¯ä»¥ç›´æ¥å¤„ç†ä¸­æ–‡ã€æ—¥æ–‡ã€è‹±æ–‡ç­‰å¤šè¯­è¨€å­—ç¬¦

2. **æ”¯æŒå‰ç¼€æœç´¢**

   * å¯ä»¥å¿«é€Ÿæ‰¾å‡ºæ‰€æœ‰ä»¥æŸä¸ªå‰ç¼€å¼€å¤´çš„è¯æ¡

3. **æ”¯æŒé”®å€¼å­˜å‚¨**

   * æ¯ä¸ªå­—ç¬¦ä¸²å¯ä»¥å…³è”ä¸€ä¸ªæ•´æ•°æˆ–å¯¹è±¡
   * å¸¸ç”¨äºè¯å…¸å­˜å‚¨è¯é¢‘ã€è¯æ€§ã€ID ç­‰

4. **é«˜æ•ˆ**

   * æŸ¥æ‰¾æ—¶é—´å¤æ‚åº¦æ¥è¿‘ O(1)ï¼ˆç›¸æ¯”æ™®é€š Python dict è¿˜æ˜¯æœ‰ä¼˜åŠ¿ï¼Œå°¤å…¶æ˜¯å‰ç¼€åŒ¹é…ï¼‰

---

### 3.2 å¸¸ç”¨æ–¹æ³•

```python
import datrie

# æ„å»ºä¸€ä¸ª Trieï¼Œåªæ”¯æŒ a-z å­—ç¬¦
trie = datrie.Trie("abcdefghijklmnopqrstuvwxyz")

# æ’å…¥
trie["apple"] = 5
trie["app"] = 10
trie["banana"] = 3

# æŸ¥æ‰¾
print(trie["apple"])  # 5

# åˆ¤æ–­é”®æ˜¯å¦å­˜åœ¨
print("app" in trie)  # True

# å‰ç¼€æœç´¢
print(list(trie.keys(prefix="ap")))  # ['app', 'apple']

# åˆ é™¤
del trie["app"]
```

> âš  æ³¨æ„ï¼š
>
> * `datrie` æ„å»ºæ—¶å¿…é¡»æŒ‡å®šå­—ç¬¦é›†ï¼ˆ`alphabet`ï¼‰ï¼Œæ¯”å¦‚ `"abcdefghijklmnopqrstuvwxyz"`
> * å¦‚æœæ˜¯ä¸­æ–‡ï¼Œå¯ä»¥ç”¨ Unicode èŒƒå›´ï¼Œä¾‹å¦‚ï¼š
>
>   ```python
>   trie = datrie.Trie(ranges=[(u'\u4e00', u'\u9fff')])  # ä¸­æ–‡æ±‰å­—
>   ```
> * string.printable æ˜¯ Python å†…ç½®çš„å­—ç¬¦ä¸²ï¼ŒåŒ…å«äº†æ‰€æœ‰â€œå¯æ‰“å°å­—ç¬¦â€ï¼š
---

### 3.3 ä½¿ç”¨åœºæ™¯

* **ä¸­æ–‡åˆ†è¯è¯å…¸**ï¼šå­˜å‚¨è¯æ¡å’Œæƒé‡ï¼Œå¿«é€ŸæŸ¥æ‰¾å’ŒåŒ¹é…å‰ç¼€
* **æ‹¼å†™æ£€æŸ¥**ï¼šæ£€æŸ¥è¾“å…¥æ˜¯å¦åœ¨è¯å…¸ä¸­
* **æœç´¢å¼•æ“**ï¼šå‰ç¼€åŒ¹é…ã€è‡ªåŠ¨è¡¥å…¨
* **RAG / NLP ç³»ç»Ÿ**ï¼šå­˜å‚¨è¯å…¸ IDã€è¯é¢‘ã€è¯æ€§


## 4. RagTokenizer

```python
# -*- coding: utf-8 -*-
# è¿™ä»½æ–‡ä»¶å®ç°äº†ä¸€ä¸ªä¸­è‹±æ–‡æ··åˆåˆ†è¯å™¨ RagTokenizerï¼š
# - é€šè¿‡ datrieï¼ˆåŒæ•°ç»„ Trieï¼‰åŠ è½½è¯å…¸ï¼Œæ”¯æŒå‰ç¼€/åå‘å‰ç¼€åŒ¹é…
# - ä¸­æ–‡ï¼šæ­£å‘/åå‘æœ€å¤§åŒ¹é… + DFS ç©·ä¸¾ + æ‰“åˆ† è§£å†³æ­§ä¹‰
# - è‹±æ–‡ï¼šNLTK åˆ†è¯ + Lemmatize + Stem
# - é¢„å¤„ç†ï¼šå…¨è§’è½¬åŠè§’ã€ç¹ä½“è½¬ç®€ä½“ã€ç»Ÿä¸€å°å†™ã€å»æ‰éå•è¯å­—ç¬¦
# - æ”¯æŒåŠ è½½ç”¨æˆ·è¯å…¸ï¼ˆå¯ç¼“å­˜ä¸º .trie æ–‡ä»¶ä»¥åŠ é€Ÿä¸‹æ¬¡å¯åŠ¨ï¼‰

import logging         # æ ‡å‡†åº“æ—¥å¿—ï¼Œç”¨äºè®°å½•ä¿¡æ¯/è°ƒè¯•/å¼‚å¸¸
import copy            # æä¾› deepcopy ç­‰æ·±æ‹·è´æ“ä½œï¼ŒDFS æ—¶å¤åˆ¶è·¯å¾„ç”¨
import datrie          # åŒæ•°ç»„ trieï¼Œæ”¯æŒå¿«é€Ÿå‰ç¼€æŸ¥è¯¢/æŒä¹…åŒ–åˆ°ç£ç›˜
import math            # æ•°å­¦åº“ï¼Œç”¨äº log/exp/å››èˆäº”å…¥ç­‰
import os              # ä¸æ–‡ä»¶è·¯å¾„/å­˜åœ¨æ€§æ£€æŸ¥ç›¸å…³
import re              # æ­£åˆ™è¡¨è¾¾å¼ï¼Œå¹¿æ³›ç”¨äºæ¸…æ´—/åˆ‡åˆ†
import string          # å¸¸è§å­—ç¬¦é›†åˆï¼ˆå¦‚ string.printableï¼‰
import sys             # ç³»ç»Ÿæ¥å£ï¼ˆæ­¤å¤„æœªç›´æ¥ä½¿ç”¨ï¼‰
from hanziconv import HanziConv                        # ç”¨äºç¹ä½“è½¬ç®€ä½“
from nltk import word_tokenize                         # è‹±æ–‡åˆ†è¯
from nltk.stem import PorterStemmer, WordNetLemmatizer # è‹±æ–‡è¯å¹²åŒ–/è¯å½¢è¿˜åŸ
from api.utils.file_utils import get_project_base_directory  # è·å–é¡¹ç›®æ ¹ç›®å½•ï¼ˆé¡¹ç›®å†…å·¥å…·ï¼‰


class RagTokenizer:
    """
    RagTokenizerï¼šé¢å‘ RAG çš„ä¸­è‹±æ–‡æ··åˆåˆ†è¯å™¨ã€‚

    ä¸»è¦èŒè´£ï¼š
    - ç»´æŠ¤ä¸€ä¸ª datrie.Trie ä½œä¸ºè¯å…¸å®¹å™¨ï¼ˆå«æ­£å‘ key å’Œåå‘ keyï¼‰
    - å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œæ¸…æ´—ï¼ˆå…¨è§’->åŠè§’ã€ç¹->ç®€ã€å°å†™ã€å»éå•è¯å­—ç¬¦ï¼‰
    - æ ¹æ®ä¸­/è‹±ä¸åŒè¯­è¨€æ®µè½é€‰æ‹©ä¸åŒåˆ†è¯ç­–ç•¥
      * ä¸­æ–‡ï¼šæœ€å¤§æ­£/åå‘åŒ¹é… -> è‹¥å†²çªåˆ™ DFS ç©·ä¸¾æ‰€æœ‰å¯åˆ‡åˆ†è·¯å¾„å¹¶æ‰“åˆ†æ‹©ä¼˜
      * è‹±æ–‡ï¼šnltk åˆ†è¯ + è¯å½¢è¿˜åŸ + è¯å¹²åŒ–
    - æ”¯æŒç”¨æˆ·è¯å…¸åŠ è½½/è¿½åŠ  + Trie æ–‡ä»¶ç¼“å­˜
    """

    def key_(self, line):
        """
        å°†å­—ç¬¦ä¸²æ ‡å‡†åŒ–ä¸º trie çš„æ­£å‘ keyï¼š
        - è½¬ä¸ºå°å†™
        - ä»¥ UTF-8 ç¼–ç å¾—åˆ° bytes
        - å†è½¬ä¸ºå½¢å¦‚ "b'xxx'" çš„å­—ç¬¦ä¸²å¹¶å»æ‰å‰å "b'" å’Œ "'"ï¼Œå¾—åˆ°è£¸å­—èŠ‚è¡¨ç¤º
        """
        return str(line.lower().encode("utf-8"))[2:-1]

    def rkey_(self, line):
        """
        æ„é€ åå‘ keyï¼ˆç”¨äºåå‘æœ€å¤§åŒ¹é…ï¼‰ï¼š
        - å°†å­—ç¬¦ä¸²åè½¬å¹¶å°å†™
        - åœ¨å‰é¢åŠ ä¸Šå‰ç¼€ "DD"ï¼ˆé¿å…ä¸æ­£å¸¸æ­£å‘ key å†²çªï¼›ä¹Ÿå¯ä½œä¸ºå‘½åç©ºé—´ï¼‰
        - åŒæ ·åš UTF-8 ç¼–ç å¹¶å»æ‰ "b'...'" åŒ…è£…
        """
        return str(("DD" + (line[::-1].lower())).encode("utf-8"))[2:-1]

    def loadDict_(self, fnm):
        """
        ä»æ–‡æœ¬è¯å…¸æ–‡ä»¶åŠ è½½è¯æ¡åˆ° trieï¼Œå¹¶è½ç›˜ä¸º .trie ç¼“å­˜æ–‡ä»¶ã€‚
        è¯å…¸æ¯è¡Œæ ¼å¼çº¦å®šï¼šword<ç©ºç™½>freq<ç©ºç™½>tag

        å­˜å‚¨ç­–ç•¥ï¼š
        - æ­£å‘ keyï¼šå­˜ (F, tag)ï¼Œå…¶ä¸­ F = round(log(freq / DENOMINATOR))
        - åå‘ keyï¼šå­˜ä¸€ä¸ªå­˜åœ¨æ€§æ ‡è®° 1ï¼ˆç”¨äº has_keys_with_prefix çš„åå‘æ£€æŸ¥ï¼‰
        """
        logging.info(f"[HUQIE]:Build trie from {fnm}")
        try:
            of = open(fnm, "r", encoding='utf-8')  # ä»¥ UTF-8 æ‰“å¼€è¯å…¸æ–‡ä»¶
            while True:
                line = of.readline()               # é€è¡Œè¯»å–
                if not line:                       # EOF é€€å‡º
                    break
                line = re.sub(r"[\r\n]+", "", line)          # å»æ‰æ¢è¡Œç¬¦
                line = re.split(r"[ \t]", line)              # æŒ‰ç©ºæ ¼/Tab åˆ†åˆ— -> [word, freq, tag]
                k = self.key_(line[0])                       # æ ‡å‡†åŒ–æ­£å‘ key
                # å¯¹è¯é¢‘åšå¯¹æ•°å˜æ¢å¹¶å››èˆäº”å…¥ -> å‹ç¼©é¢‘ç‡èŒƒå›´ï¼Œé¿å…æç«¯å€¼å½±å“
                F = int(math.log(float(line[1]) / self.DENOMINATOR) + .5)
                # æ­£å‘ key è‹¥ä¸å­˜åœ¨æˆ–å·²æœ‰ freq æ›´å°ï¼Œåˆ™æ›´æ–°ä¸ºæ›´å¤§çš„ freqï¼ˆä¿ç•™æ›´å¯ä¿¡çš„é¢‘ç‡ï¼‰
                if k not in self.trie_ or self.trie_[k][0] < F:
                    self.trie_[self.key_(line[0])] = (F, line[2])  # å­˜ (å¯¹æ•°é¢‘ç‡, è¯æ€§/æ ‡ç­¾)
                # åŒæ—¶å†™å…¥åå‘ key æ ‡è®°ï¼Œç”¨äºåå‘æœ€å¤§åŒ¹é…æ—¶çš„å‰ç¼€æ£€æŸ¥
                self.trie_[self.rkey_(line[0])] = 1

            dict_file_cache = fnm + ".trie"                  # ç¼“å­˜æ–‡ä»¶å
            logging.info(f"[HUQIE]:Build trie cache to {dict_file_cache}")
            self.trie_.save(dict_file_cache)                 # å°† trie æŒä¹…åŒ–ä¸ºäºŒè¿›åˆ¶æ–‡ä»¶
            of.close()
        except Exception:
            # è‹¥æœ‰å¼‚å¸¸ï¼ˆå¦‚æ–‡ä»¶ä¸å­˜åœ¨/æ ¼å¼ä¸å¯¹ï¼‰ï¼Œæ‰“å°å †æ ˆæ–¹ä¾¿æ’æŸ¥
            logging.exception(f"[HUQIE]:Build trie {fnm} failed")

    def __init__(self, debug=False):
        """
        åˆå§‹åŒ–ï¼š
        - è®¾ç½® DEBUG å¼€å…³ã€é¢‘ç‡å½’ä¸€åŒ–åˆ†æ¯ã€è¯å…¸ç›®å½•
        - åˆå§‹åŒ–è‹±æ–‡è¯å¹²åŒ–/è¯å½¢è¿˜åŸå™¨
        - å®šä¹‰ç”¨äºè¯­è¨€åˆ‡æ®µçš„åˆ†éš”æ­£åˆ™ï¼ˆæ ‡ç‚¹/ç©ºç™½/è‹±æ•°ç­‰ï¼‰
        - è‹¥å­˜åœ¨ .trie ç¼“å­˜åˆ™ç›´æ¥åŠ è½½ï¼Œå¦åˆ™åˆ›å»ºç©º trie å¹¶ä» .txt è¯å…¸æ„å»º
        """
        self.DEBUG = debug
        self.DENOMINATOR = 1000000
        # è¯å…¸åŸºæœ¬è·¯å¾„ï¼š<project>/rag/res/huqie
        self.DIR_ = os.path.join(get_project_base_directory(), "rag/res", "huqie")

        # è‹±æ–‡è¯å½¢å¤„ç†ï¼šlemmatize å½’ä¸€åŒ–è¯å½¢ï¼Œå†ç”¨ stemmer å–è¯å¹²ï¼ˆäºŒè€…ç»„åˆé¿å…æ¼å¬ï¼‰
        self.stemmer = PorterStemmer()
        self.lemmatizer = WordNetLemmatizer()

        # è¯­è¨€åˆ‡æ®µ/åˆ†éš”æ­£åˆ™ï¼š
        # - ç¬¬ä¸€éƒ¨åˆ†ï¼šå„ç§æ ‡ç‚¹/ç©ºç™½/ä¸­æ–‡æ ‡ç‚¹
        # - ç¬¬äºŒéƒ¨åˆ†ï¼šæˆæ®µçš„è‹±æ•°/è¿å­—ç¬¦/ç‚¹å·ç­‰ï¼ˆä½œä¸ºä¸å¯å†ç»†åˆ†çš„ä¸€æ®µï¼‰
        self.SPLIT_CHAR = r"([ ,\.<>/?;:'\[\]\\`!@#$%^&*\(\)\{\}\|_+=ã€Šã€‹ï¼Œã€‚ï¼Ÿã€ï¼›â€˜â€™ï¼šâ€œâ€ã€ã€‘~ï¼ï¿¥%â€¦â€¦ï¼ˆï¼‰â€”â€”-]+|[a-zA-Z0-9,\.-]+)"

        trie_file_name = self.DIR_ + ".txt.trie"  # è¯å…¸ç¼“å­˜æ–‡ä»¶å…¨è·¯å¾„
        # ä¼˜å…ˆå°è¯•åŠ è½½å·²æœ‰ç¼“å­˜ï¼Œä»¥é¿å…æ¯æ¬¡å¯åŠ¨éƒ½é‡å»º Trieï¼ˆæ€§èƒ½æ›´å¥½ï¼‰
        if os.path.exists(trie_file_name):
            try:
                # datrie.Trie.load ä¼šæ ¹æ®æ„å»ºæ—¶çš„ alphabet è‡ªåŠ¨è¿˜åŸ
                self.trie_ = datrie.Trie.load(trie_file_name)
                return  # åŠ è½½æˆåŠŸåˆ™ç›´æ¥è¿”å›
            except Exception:
                # è‹¥ç¼“å­˜æŸåæˆ–ç‰ˆæœ¬ä¸å…¼å®¹ï¼Œåˆ™è®°å½•å¼‚å¸¸å¹¶å›é€€åˆ°ç©º Trieï¼ˆä½¿ç”¨ string.printable ä½œä¸º alphabetï¼‰
                logging.exception(f"[HUQIE]:Fail to load trie file {trie_file_name}, build the default trie file")
                self.trie_ = datrie.Trie(string.printable)
        else:
            # é¦–æ¬¡è¿è¡Œæˆ–ç¼“å­˜æœªç”Ÿæˆï¼šåˆ›å»ºç©º Trieï¼ˆalphabet æŒ‡å®šå¯æ¥å—çš„å­—ç¬¦é›†ï¼›æ­¤å¤„å– printableï¼‰
            logging.info(f"[HUQIE]:Trie file {trie_file_name} not found, build the default trie file")
            self.trie_ = datrie.Trie(string.printable)

        # è‹¥ä¸Šé¢æ²¡æœ‰ returnï¼Œè¯´æ˜éœ€è¦ä»åŸå§‹ .txt æ„å»º Trieï¼Œå¹¶åœ¨æ„å»ºå®Œè½ç›˜ .trie
        self.loadDict_(self.DIR_ + ".txt")

    def loadUserDict(self, fnm):
        """
        åŠ è½½ç”¨æˆ·è¯å…¸ï¼š
        - ä¼˜å…ˆå°è¯•ç›´æ¥åŠ è½½ fnm.trieï¼ˆç¼“å­˜ï¼‰
        - è‹¥å¤±è´¥ï¼Œåˆ™æ–°å»ºç©º Trie å¹¶ä» fnm æ–‡æœ¬è¯å…¸æ„å»º
        """
        try:
            self.trie_ = datrie.Trie.load(fnm + ".trie")  # ç›´æ¥è¯»ç¼“å­˜
            return
        except Exception:
            # ç¼“å­˜ä¸å­˜åœ¨æˆ–æŸåï¼Œåˆ™åˆ›å»ºç©º trie å¹¶ç”¨æ–‡æœ¬è¯å…¸æ„å»º
            self.trie_ = datrie.Trie(string.printable)
        self.loadDict_(fnm)

    def addUserDict(self, fnm):
        """
        åœ¨å½“å‰ Trie åŸºç¡€ä¸Šè¿½åŠ åŠ è½½æ–°çš„ç”¨æˆ·è¯å…¸ï¼ˆå¯å¤šæ¬¡è°ƒç”¨å åŠ æ–°è¯ï¼‰ã€‚
        """
        self.loadDict_(fnm)

    def _strQ2B(self, ustring):
        """Convert full-width characters to half-width charactersï¼ˆå…¨è§’->åŠè§’ï¼‰"""
        rstring = ""
        for uchar in ustring:
            inside_code = ord(uchar)           # è·å–å­—ç¬¦çš„ Unicode ç ä½
            if inside_code == 0x3000:          # å…¨è§’ç©ºæ ¼ç‰¹æ®Šå¤„ç†
                inside_code = 0x0020
            else:
                inside_code -= 0xfee0          # å…¨è§’å­—ç¬¦ä¸åŠè§’å­—ç¬¦é—´çš„å›ºå®šåç§»
            # å¦‚æœè½¬æ¢åä¸åœ¨å¯æ‰“å° ASCII èŒƒå›´ï¼Œåˆ™ä¿æŒåŸå­—ç¬¦ï¼ˆé¿å…è¯¯è½¬ï¼‰
            if inside_code < 0x0020 or inside_code > 0x7e:
                rstring += uchar
            else:
                rstring += chr(inside_code)    # å¦åˆ™å†™å…¥è½¬æ¢åçš„åŠè§’å­—ç¬¦
        return rstring

    def _tradi2simp(self, line):
        """ç¹ä½“ -> ç®€ä½“ï¼ˆå€ŸåŠ© hanziconvï¼‰"""
        return HanziConv.toSimplified(line)

    def dfs_(self, chars, s, preTks, tkslist, _depth=0, _memo=None):
        """
        å¯¹ç»™å®šå­—ç¬¦æ•°ç»„ chars ä»ä½ç½® s å¼€å§‹åš DFS åˆ‡åˆ†ï¼Œå°†æ‰€æœ‰å¯èƒ½åˆ‡åˆ†è·¯å¾„åŠ å…¥ tkslistã€‚

        å‚æ•°ï¼š
        - chars: list[str]ï¼Œå­—ç¬¦åˆ—è¡¨
        - s: intï¼Œå½“å‰æœç´¢èµ·ç‚¹
        - preTks: list[(token, (freq, tag))]ï¼Œåˆ°ç›®å‰ä¸ºæ­¢çš„åˆ‡åˆ†åºåˆ—ï¼ˆè·¯å¾„ï¼‰
        - tkslist: list[list[(token, (freq, tag))]]ï¼Œæ”¶é›†æ‰€æœ‰å®Œæ•´åˆ‡åˆ†çš„å®¹å™¨
        - _depth: å½“å‰é€’å½’æ·±åº¦ï¼ˆç”¨äºè®¾å®šä¸Šé™é˜²æ­¢çˆ†æ ˆï¼‰
        - _memo: è®°å¿†åŒ–ç¼“å­˜ï¼Œkey=(s, tuple(å·²æœ‰ token åºåˆ—)) -> æœ€è¿œå¯è¾¾ä½ç½®ï¼›é¿å…é‡å¤å­é—®é¢˜

        é€»è¾‘ï¼š
        - è¶…è¿‡ MAX_DEPTHï¼šå°†å‰©ä½™å­—ç¬¦è§†ä½œä¸€ä¸ªæ•´ä½“ tokenï¼ˆæ‰“ä½åˆ† freq=-12ï¼‰ï¼Œæ”¶æŸå¹¶è¿”å›
        - è‹¥ s å·²åˆ°ç»“å°¾ï¼šæŠŠå·²æœ‰è·¯å¾„åŠ å…¥ç»“æœï¼Œè¿”å›
        - é‡å¤å­—ç¬¦å¿«é€Ÿè·¯å¾„ï¼šè¿ç»­ç›¸åŒå­—ç¬¦ï¼ˆå¦‚ "â€”â€”" æˆ– "......"ï¼‰åˆå¹¶å¤„ç†ï¼Œå‡å°‘åˆ†æ”¯
        - ä¸»æµç¨‹ï¼šå°è¯•ä» s å‘åæ‰©å±•ï¼Œä¼˜å…ˆä½¿ç”¨ trie çš„ has_keys_with_prefix åšå‰ªæ
        - å¦‚æ— æ³•æ‰©å±•ï¼ˆres == sï¼‰ï¼Œåˆ™æŒ‰å•å­—åˆ‡åˆ†æ¨è¿›ï¼ˆä¿åº•ï¼‰
        """
        if _memo is None:
            _memo = {}
        MAX_DEPTH = 10
        if _depth > MAX_DEPTH:
            if s < len(chars):
                copy_pretks = copy.deepcopy(preTks)     # å¤åˆ¶è·¯å¾„ä»¥å…åç»­æ±¡æŸ“
                remaining = "".join(chars[s:])          # å°†å‰©ä½™éƒ¨åˆ†åˆæˆä¸€ä¸ª token
                copy_pretks.append((remaining, (-12, '')))  # é¢‘ç‡ç»™å¾ˆä½å€¼è¡¨ç¤ºé€€åŒ–è·¯å¾„
                tkslist.append(copy_pretks)             # æ”¶é›†ä¸ºä¸€ç§åˆ‡åˆ†
            return s
    
        # è®°å¿†åŒ– keyï¼šå½“å‰ä½ç½® + è·¯å¾„çš„ token ä¸²ï¼ˆåªå– token å­—ç¬¦ï¼Œä¸å–å…ƒä¿¡æ¯ï¼‰
        state_key = (s, tuple(tk[0] for tk in preTks)) if preTks else (s, None)
        if state_key in _memo:
            return _memo[state_key]
        
        res = s
        if s >= len(chars):
            # åˆ°è¾¾æœ«å°¾ï¼Œå½“å‰è·¯å¾„æ˜¯ä¸€ä¸ªå®Œæ•´åˆ‡åˆ†ï¼Œè®°å…¥ tkslist
            tkslist.append(preTks)
            _memo[state_key] = s
            return s

        # å¿«é€Ÿè·¯å¾„ï¼šæ£€æµ‹æ˜¯å¦æœ‰ >=5 ä¸ªç›¸åŒå­—ç¬¦è¿ä¸²ï¼Œè‹¥æœ‰åˆ™æ‰¹é‡åæ‰ï¼ˆå‡å°‘é€’å½’æ·±åº¦ï¼‰
        if s < len(chars) - 4:
            is_repetitive = True
            char_to_check = chars[s]
            for i in range(1, 5):
                if s + i >= len(chars) or chars[s + i] != char_to_check:
                    is_repetitive = False
                    break
            if is_repetitive:
                # end æŒ‡å‘é‡å¤æ®µå°¾ï¼Œmid å–æœ€å¤š 10 ä¸ªå­—ç¬¦ä½œä¸º token
                end = s
                while end < len(chars) and chars[end] == char_to_check:
                    end += 1
                mid = s + min(10, end - s)
                t = "".join(chars[s:mid])               # é‡å¤æ®µçš„å‰ç¼€ä½œä¸ºä¸€ä¸ª token
                k = self.key_(t)
                copy_pretks = copy.deepcopy(preTks)
                if k in self.trie_:
                    copy_pretks.append((t, self.trie_[k]))
                else:
                    copy_pretks.append((t, (-12, '')))
                # ä¼ å…¥ midï¼Œå‡å°‘é€’å½’
                next_res = self.dfs_(chars, mid, copy_pretks, tkslist, _depth + 1, _memo)
                res = max(res, next_res)                # è®°å½•æœ€è¿œæ¨è¿›ä½ç½®
                _memo[state_key] = res
                return res
    
        # S æ˜¯èµ·å§‹å¯å°è¯•çš„æœ€çŸ­ç»ˆç‚¹ï¼ˆç”¨äºå‰ªæï¼Œå°½é‡é¿å… 1 å­—èŠ‚æ­¥è¿›çš„çˆ†ç‚¸ï¼‰
        S = s + 1
        if s + 2 <= len(chars):
            t1 = "".join(chars[s:s + 1])                                # å– 1 å­—ç¬¦
            t2 = "".join(chars[s:s + 2])                                # å– 2 å­—ç¬¦
            # å¦‚æœå‰ç¼€æœ‰ 1 å­—ç¬¦è¯å‰ç¼€ï¼Œä½†æ²¡æœ‰ 2 å­—ç¬¦å‰ç¼€ï¼Œåˆ™ç›´æ¥è·³è¿‡ä¸€ä¸ªå­—ç¬¦
            if self.trie_.has_keys_with_prefix(self.key_(t1)) and not self.trie_.has_keys_with_prefix(self.key_(t2)):
                S = s + 2
        # é™ä½è¿ç»­ 1 å­—ç¬¦ token çš„æ¦‚ç‡ï¼šè‹¥å‰é¢è¿ç»­ 3 ä¸ª 1 å­—ç¬¦ token ä¸”ä¸‹ä¸€ä¸ªä¸å…¶å¯å‰ç¼€æ‹¼æ¥ï¼Œåˆ™ S è·³ 2
        if len(preTks) > 2 and len(preTks[-1][0]) == 1 and len(preTks[-2][0]) == 1 and len(preTks[-3][0]) == 1:
            t1 = preTks[-1][0] + "".join(chars[s:s + 1])
            if self.trie_.has_keys_with_prefix(self.key_(t1)):
                S = s + 2
    
        # ä¸»å¾ªç¯ï¼šå°è¯•ä» s å‘å³æ‰©å±•åˆ° eï¼Œå°½é‡èµ°å‰ç¼€å­˜åœ¨çš„è·¯å¾„ï¼ˆå¦åˆ™ breakï¼‰
        for e in range(S, len(chars) + 1):
            t = "".join(chars[s:e])                 # å€™é€‰ token
            k = self.key_(t)
            if e > s + 1 and not self.trie_.has_keys_with_prefix(k):
                break                               # ä¸€æ—¦å‰ç¼€ä¸å­˜åœ¨ï¼Œç›´æ¥åœæ­¢æ‰©å±•
            if k in self.trie_:                     # å‘½ä¸­è¯å…¸åˆ™ä½œä¸ºä¸€ä¸ªå¤‡é€‰åˆ†æ”¯
                pretks = copy.deepcopy(preTks)
                pretks.append((t, self.trie_[k]))
                # é€’å½’ç»§ç»­å‘åæœç´¢ï¼Œå¹¶å°è¯•æ›´æ–°æœ€è¿œæ¨è¿›ä½ç½®
                res = max(res, self.dfs_(chars, e, pretks, tkslist, _depth + 1, _memo))
        
        # è‹¥è‡³å°‘æœ‰ä¸€ä¸ªåˆ†æ”¯æ¨è¿›äº†ï¼ˆres > sï¼‰ï¼Œè®°å½•è®°å¿†åŒ–å¹¶è¿”å›
        if res > s:
            _memo[state_key] = res
            return res
    
        # å¦åˆ™é€€åŒ–ä¸ºæŒ‰å•å­—åˆ‡åˆ†ï¼ˆä¿è¯èƒ½å‰è¿›ï¼‰
        t = "".join(chars[s:s + 1])
        k = self.key_(t)
        copy_pretks = copy.deepcopy(preTks)
        if k in self.trie_:
            copy_pretks.append((t, self.trie_[k]))
        else:
            copy_pretks.append((t, (-12, '')))
        result = self.dfs_(chars, s + 1, copy_pretks, tkslist, _depth + 1, _memo)
        _memo[state_key] = result
        return result

    def freq(self, tk):
        """
        æŸ¥è¯¢è¯é¢‘ï¼ˆå°†å­˜å‚¨çš„å¯¹æ•°é¢‘ç‡è¿˜åŸä¸ºè¿‘ä¼¼åŸé¢‘ç‡ï¼‰ï¼š
        - è‹¥ä¸åœ¨ trieï¼Œè¿”å› 0
        - åœ¨ trieï¼šexp(F) * DENOMINATORï¼Œå››èˆäº”å…¥ä¸º int
        """
        k = self.key_(tk)
        if k not in self.trie_:
            return 0
        return int(math.exp(self.trie_[k][0]) * self.DENOMINATOR + 0.5)

    def tag(self, tk):
        """è¿”å›è¯æ€§/æ ‡ç­¾ï¼›ä¸åœ¨ trie è¿”å›ç©ºä¸²"""
        k = self.key_(tk)
        if k not in self.trie_:
            return ""
        return self.trie_[k][1]

    def score_(self, tfts):
        """
        å¯¹ä¸€ä¸ªåˆ‡åˆ†åºåˆ—ï¼ˆå«æ¯ä¸ª token çš„ (freq, tag)ï¼‰æ‰“åˆ†ï¼š
        - Fï¼šç´¯åŠ æ‰€æœ‰ token çš„å¯¹æ•°é¢‘ç‡
        - Lï¼šé•¿åº¦>1 çš„ token å æ¯”ï¼ˆåå¥½æ›´é•¿çš„è¯ï¼‰
        - æƒ©ç½šé¡¹ B=30ï¼šåºåˆ—è¶Šé•¿ B/len è¶Šå°ï¼ˆé¼“åŠ±è¾ƒå°‘çš„åˆ‡åˆ†ï¼‰
        è¿”å›ï¼š(token åˆ—è¡¨, åˆ†æ•°)
        """
        B = 30
        F, L, tks = 0, 0, []
        for tk, (freq, tag) in tfts:
            F += freq
            L += 0 if len(tk) < 2 else 1
            tks.append(tk)
        #F /= len(tks)   # å¦‚éœ€å¹³å‡é¢‘ç‡ï¼Œå¯æ‰“å¼€ï¼›å½“å‰ç­–ç•¥ç›´æ¥ç´¯åŠ 
        L /= len(tks)
        logging.debug("[SC] {} {} {} {} {}".format(tks, len(tks), L, F, B / len(tks) + L + F))
        return tks, B / len(tks) + L + F

    def sortTks_(self, tkslist):
        """
        å¯¹å¤šæ¡å€™é€‰åˆ‡åˆ†ï¼ˆlist of token-with-metaï¼‰æ‰“åˆ†æ’åºï¼Œåˆ†æ•°é«˜çš„åœ¨å‰ã€‚
        è¿”å›ï¼š[ (token_str_list, score), ... ]
        """
        res = []
        for tfts in tkslist:
            tks, s = self.score_(tfts)
            res.append((tks, s))
        return sorted(res, key=lambda x: x[1], reverse=True)

    def merge_(self, tks):
        """
        å°†ç©ºæ ¼åˆ†å‰²çš„ token ä¸²åšåˆå¹¶ä¼˜åŒ–ï¼š
        - å…ˆå½’ä¸€è¿ç»­ç©ºæ ¼
        - ä»¥æœ€å¤š 5 ä¸ª token çš„æ»‘çª—ï¼Œå°è¯•æŠŠ [s:e] åˆå¹¶ä¸ºæ›´é•¿çš„ token
          * ä»…å½“åˆå¹¶åå­—ç¬¦ä¸²åŒ¹é… SPLIT_CHARï¼ˆå³â€œåƒä¸€ä¸ªåˆç†ç‰‡æ®µâ€ï¼‰
            ä¸”åœ¨è¯å…¸ä¸­ freq(tk) > 0 æ—¶æ‰åˆå¹¶
        - è¿”å›åˆå¹¶åçš„å­—ç¬¦ä¸²ï¼ˆä»ä»¥ç©ºæ ¼åˆ†å‰² tokenï¼‰
        """
        res = []
        tks = re.sub(r"[ ]+", " ", tks).split()  # å½’ä¸€ç©ºæ ¼åæŒ‰ç©ºæ ¼åˆ‡
        s = 0
        while True:
            if s >= len(tks):
                break
            E = s + 1
            # æœ€å¤šå°è¯•åˆå¹¶åˆ° s+5
            for e in range(s + 2, min(len(tks) + 2, s + 6)):
                tk = "".join(tks[s:e])                    # å°è¯•åˆå¹¶ä¸ºæ— ç©ºæ ¼å­—ç¬¦ä¸²
                if re.search(self.SPLIT_CHAR, tk) and self.freq(tk):
                    E = e                                 # æ»¡è¶³æ¡ä»¶åˆ™å»¶é•¿å¯åˆå¹¶çª—å£
            res.append("".join(tks[s:E]))                 # å°† [s:E) åˆå¹¶åŠ å…¥ç»“æœ
            s = E

        return " ".join(res)

    def maxForward_(self, line):
        """
        æ­£å‘æœ€å¤§åŒ¹é…ï¼š
        - ä»å·¦åˆ°å³æ‰©å±• eï¼Œç›´åˆ°å‰ç¼€ä¸å­˜åœ¨å†å›é€€
        - å‘½ä¸­è¯å…¸åˆ™å– (token, meta)ï¼Œå¦åˆ™ freq=0 ä½œä¸ºæœªçŸ¥è¯
        - æœ€ç»ˆå¯¹å¾—åˆ°çš„åºåˆ—æ‰“åˆ†ï¼Œè¿”å› (token_list, score)
        """
        res = []
        s = 0
        while s < len(line):
            e = s + 1
            t = line[s:e]
            # åªè¦å‰ç¼€å­˜åœ¨å°±æŒç»­æ‰©å±•
            while e < len(line) and self.trie_.has_keys_with_prefix(
                    self.key_(t)):
                e += 1
                t = line[s:e]

            # å›é€€åˆ°æœ€åä¸€ä¸ªåœ¨è¯å…¸ä¸­çš„ä½ç½®ï¼ˆæˆ–å•å­—ç¬¦ï¼‰
            while e - 1 > s and self.key_(t) not in self.trie_:
                e -= 1
                t = line[s:e]

            # è®°å½•å‘½ä¸­æˆ–æœªçŸ¥ token
            if self.key_(t) in self.trie_:
                res.append((t, self.trie_[self.key_(t)]))
            else:
                res.append((t, (0, '')))

            s = e

        return self.score_(res)

    def maxBackward_(self, line):
        """
        åå‘æœ€å¤§åŒ¹é…ï¼š
        - ä»å³åˆ°å·¦æ‰©å±• sï¼Œç›´åˆ°åå‘å‰ç¼€ä¸å­˜åœ¨å†å›é€€
        - å‘½ä¸­è¯å…¸åˆ™å– (token, meta)ï¼Œå¦åˆ™ freq=0
        - è¿”å›æ—¶å°† token åºåˆ—åè½¬ä¸ºä»å·¦åˆ°å³ï¼Œå¹¶æ‰“åˆ†
        """
        res = []
        s = len(line) - 1
        while s >= 0:
            e = s + 1
            t = line[s:e]
            # ä½¿ç”¨åå‘ key åšå‰ç¼€æ£€æŸ¥ï¼ˆrkey_ï¼‰
            while s > 0 and self.trie_.has_keys_with_prefix(self.rkey_(t)):
                s -= 1
                t = line[s:e]

            # å›é€€åˆ°å­—å…¸ä¸­å­˜åœ¨çš„ä½ç½®
            while s + 1 < e and self.key_(t) not in self.trie_:
                s += 1
                t = line[s:e]

            # è®°å½•å‘½ä¸­æˆ–æœªçŸ¥ token
            if self.key_(t) in self.trie_:
                res.append((t, self.trie_[self.key_(t)]))
            else:
                res.append((t, (0, '')))

            s -= 1

        return self.score_(res[::-1])  # è¿˜åŸä¸ºä»å·¦åˆ°å³

    def english_normalize_(self, tks):
        """
        å¯¹è‹±æ–‡ token åšè¯å½¢å½’ä¸€ï¼šå…ˆ lemmatize å† stemã€‚
        ä»…å¯¹åŒ¹é… [a-zA-Z_-]+ çš„ token åº”ç”¨ï¼Œå…¶ä½™åŸæ ·è¿”å›ã€‚
        """
        return [self.stemmer.stem(self.lemmatizer.lemmatize(t)) if re.match(r"[a-zA-Z_-]+$", t) else t for t in tks]

    def _split_by_lang(self, line):
        """
        å°†è¾“å…¥å­—ç¬¦ä¸²æŒ‰è¯­è¨€å—æ‹†åˆ†ä¸º (ç‰‡æ®µæ–‡æœ¬, æ˜¯å¦ä¸­æ–‡) çš„åˆ—è¡¨ï¼š
        - å…ˆæŒ‰ SPLIT_CHAR åˆ‡å‡ºâ€œå—â€ï¼ˆæ ‡ç‚¹/è‹±æ•°æ®µä¼šç‹¬ç«‹æˆå—ï¼‰
        - åœ¨æ¯ä¸ªå—å†…éƒ¨ï¼Œå†æŒ‰ä¸­æ–‡/éä¸­æ–‡çš„åˆ‡æ¢æ‹†åˆ†æˆæ›´ç»†ç²’åº¦çš„ (txt, zh_flag)
        eg: line = "ä»Šå¤©å¤©æ°”å¾ˆå¥½, hello world! 3.14"
            arr = ['ä»Šå¤©å¤©æ°”å¾ˆå¥½', ',', 'hello', 'world', '!', '3.14']
        """
        txt_lang_pairs = []
        arr = re.split(self.SPLIT_CHAR, line)  # ä½¿ç”¨æ•è·ç»„ï¼Œåˆ†éš”ç¬¦ä¹Ÿä¼šå‡ºç°åœ¨ç»“æœä¸­ï¼ˆä¿ç•™ç»“æ„ï¼‰
        for a in arr:
            if not a:
                continue
            s = 0
            e = s + 1
            zh = is_chinese(a[s])  # ä»¥é¦–å­—ç¬¦åˆ¤æ–­ä¸­æ–‡/éä¸­æ–‡
            while e < len(a):
                _zh = is_chinese(a[e])
                if _zh == zh:
                    e += 1
                    continue
                txt_lang_pairs.append((a[s: e], zh))  # è¯­è¨€çŠ¶æ€åˆ‡æ¢å¤„æˆªæ–­
                s = e
                e = s + 1
                zh = _zh
            if s >= len(a):
                continue
            txt_lang_pairs.append((a[s: e], zh))
        return txt_lang_pairs

    def tokenize(self, line):
        """
        å¯¹è¾“å…¥å­—ç¬¦ä¸²è¿›è¡Œå®Œæ•´åˆ†è¯æµç¨‹ï¼š
        1) æ¸…æ´—ï¼šå»éå•è¯å­—ç¬¦ -> å…¨è§’è½¬åŠè§’ -> å°å†™ -> ç¹è½¬ç®€
        2) è¯­è¨€åˆ‡æ®µï¼šä¸­æ–‡/è‹±æ–‡åˆ†åˆ«å¤„ç†
           - è‹±æ–‡ï¼šnltk.word_tokenize + lemmatize + stem
           - ä¸­æ–‡ï¼šæ­£å‘/åå‘æœ€å¤§åŒ¹é…ï¼›è‹¥å‡ºç°ä¸ä¸€è‡´åŒºé—´ï¼Œåˆ™å¯¹è¯¥åŒºé—´ç”¨ DFS ç©·ä¸¾å¹¶æ‰“åˆ†é€‰æœ€ä¼˜
        3) åˆå¹¶ç­–ç•¥ï¼šmerge_ å¯¹è‹±æ–‡ç²˜è¿/å¯åˆå¹¶ç‰‡æ®µè¿›è¡Œåˆå¹¶
        4) è¿”å›ä»¥ç©ºæ ¼åˆ†éš”çš„ä¸€ä¸² token
        """
        line = re.sub(r"\W+", " ", line)   # å°†éå•è¯å­—ç¬¦æ›¿æ¢ä¸ºç©ºæ ¼ï¼ˆå…ˆç²—æ¸…æ´—ï¼‰
        line = self._strQ2B(line).lower()  # å…¨è§’->åŠè§’ + å…¨éƒ¨å°å†™
        line = self._tradi2simp(line)      # ç¹ä½“->ç®€ä½“

        arr = self._split_by_lang(line)    # æ‹†æˆè‹¥å¹² (æ–‡æœ¬ç‰‡æ®µ, æ˜¯å¦ä¸­æ–‡)
        res = []
        for L,lang in arr:
            if not lang:
                # è‹±æ–‡/éä¸­æ–‡è·¯å¾„ï¼šç›´æ¥ç”¨ nltk åˆ†è¯ï¼Œå†åšè¯å½¢å½’ä¸€
                res.extend([self.stemmer.stem(self.lemmatizer.lemmatize(t)) for t in word_tokenize(L)])
                continue
            # ä¸­æ–‡è·¯å¾„ï¼šå¯¹å¾ˆçŸ­æˆ–æ˜æ˜¾æ˜¯è‹±æ•°çš„ï¼Œç›´æ¥åŸæ ·åŠ å…¥
            if len(L) < 2 or re.match(
                    r"[a-z\.-]+$", L) or re.match(r"[0-9\.-]+$", L):
                res.append(L)
                continue

            # å…ˆåšä¸€æ¬¡æ­£å‘/åå‘æœ€å¤§åŒ¹é…
            tks, s = self.maxForward_(L)
            tks1, s1 = self.maxBackward_(L)
            if self.DEBUG:
                logging.debug("[FW] {} {}".format(tks, s))
                logging.debug("[BW] {} {}".format(tks1, s1))

            # åˆå¹¶ä¸¤ç§åˆ‡åˆ†ï¼šä»ä¸¤è€…å…±åŒçš„å‰ç¼€å¼€å§‹ï¼Œé‡åˆ°ä¸ä¸€è‡´åŒºé—´æ—¶ä½¿ç”¨ DFS ç©·ä¸¾å¹¶é€‰é«˜åˆ†æ–¹æ¡ˆ
            i, j, _i, _j = 0, 0, 0, 0
            same = 0
            while i + same < len(tks1) and j + same < len(tks) and tks1[i + same] == tks[j + same]:
                same += 1
            if same > 0:
                # ä¸¤è€…å¼€å¤´çš„ä¸€æ®µæ˜¯ç›¸åŒçš„ï¼Œå…ˆæ”¾å…¥ç»“æœ
                res.append(" ".join(tks[j: j + same]))
            _i = i + same
            _j = j + same
            j = _j + 1
            i = _i + 1

            # åœ¨ä¸ä¸€è‡´çš„åŒºé—´é‡Œï¼Œæ»‘åŠ¨çª—å£æ¯”è¾ƒï¼Œç›´åˆ°å¯¹é½
            while i < len(tks1) and j < len(tks):
                tk1, tk = "".join(tks1[_i:i]), "".join(tks[_j:j])
                if tk1 != tk:
                    # å°šæœªå¯¹é½ï¼šä¼¸é•¿è¾ƒçŸ­ä¸€æ–¹ä»¥å°è¯•å¯¹é½
                    if len(tk1) > len(tk):
                        j += 1
                    else:
                        i += 1
                    continue

                if tks1[i] != tks[j]:
                    # è™½ç„¶æ‹¼æ¥å­—ç¬¦ä¸²ä¸€è‡´ï¼Œä½†å•ä¸ª token è¾¹ç•Œä¸åŒ -> ç»§ç»­ä¼¸é•¿å°è¯•æ›´ç¨³å®šå¯¹é½
                    i += 1
                    j += 1
                    continue

                # åˆ°æ­¤è¯´æ˜ [ _i:i ) ä¸ [ _j:j ) èŒƒå›´å¯¹åº”åŒä¸€ä¸²å­—ç¬¦ï¼Œä½† token è¾¹ç•Œä¸åŒ
                # å¯¹è¯¥ä¸ä¸€è‡´åŒºé—´ç”¨ DFS ç©·ä¸¾æ‰€æœ‰åˆ‡åˆ†å¹¶æ‰“åˆ†ï¼Œé€‰æœ€ä¼˜åŠ å…¥ç»“æœ
                tkslist = []
                self.dfs_("".join(tks[_j:j]), 0, [], tkslist)
                res.append(" ".join(self.sortTks_(tkslist)[0][0]))

                # ä¹‹åå†æ¬¡è·³è¿‡ä¸¤è€…æ–°çš„ç›¸åŒå‰ç¼€
                same = 1
                while i + same < len(tks1) and j + same < len(tks) and tks1[i + same] == tks[j + same]:
                    same += 1
                res.append(" ".join(tks[j: j + same]))
                _i = i + same
                _j = j + same
                j = _j + 1
                i = _i + 1
            
            # _i è¡¨ç¤ºå·²ç»è¾¾æˆåŒ¹é…çš„å¼€å§‹ç‚¹
            # è‹¥è¿˜æœ‰å°¾éƒ¨æœªå¤„ç†ï¼ˆä¸¤è€…ä» _i/_j èµ·åº”ç­‰é•¿ä¸”å­—ç¬¦ç›¸åŒï¼‰ï¼Œå¯¹å°¾éƒ¨å†åšä¸€æ¬¡ DFS ä¼˜åŒ–
            if _i < len(tks1):
                assert _j < len(tks)
                assert "".join(tks1[_i:]) == "".join(tks[_j:])
                tkslist = []
                self.dfs_("".join(tks[_j:]), 0, [], tkslist)
                res.append(" ".join(self.sortTks_(tkslist)[0][0]))

        res = " ".join(res)                 # å°†æ‰€æœ‰ç‰‡æ®µæ‹¼æˆç©ºæ ¼åˆ†éš”çš„ token ä¸²
        logging.debug("[TKS] {}".format(self.merge_(res)))
        return self.merge_(res)             # æœ€ååšä¸€æ¬¡åˆå¹¶ä¼˜åŒ–å¹¶è¿”å›

    def fine_grained_tokenize(self, tks):
        """
        å¯¹ tokenize çš„ç»“æœå†åšç»†ç²’åº¦å¤„ç†ï¼š
        - è‹¥ä¸­æ–‡ token å æ¯”è¾ƒä½ï¼ˆ<20%ï¼‰ï¼Œåˆ™å¯¹è‹±æ–‡ token è¿›ä¸€æ­¥ä»¥ "/" åˆ‡åˆ†å­è¯
        - å¯¹ä¸­æ–‡ tokenï¼š
          * çŸ­é•¿åº¦ï¼ˆ<3ï¼‰æˆ–çº¯æ•°å­—ç›´æ¥ä¿ç•™
          * å¦åˆ™ç”¨ DFS å°è¯•æ›´ç»†åˆ‡åˆ†ï¼ˆé•¿åº¦>10 çš„ç›´æ¥è§†ä¸ºæ•´ä½“ï¼Œé¿å…çˆ†ç‚¸ï¼‰
        - æœ€ç»ˆå¯¹è‹±æ–‡åš english_normalize_ï¼ˆlemma+stemï¼‰
        """
        tks = tks.split()
        zh_num = len([1 for c in tks if c and is_chinese(c[0])])
        if zh_num < len(tks) * 0.2:
            # è‹±æ–‡å å¤šï¼šæŠŠåŒ…å« '/' çš„ token è¿›ä¸€æ­¥æ‹†å¼€
            res = []
            for tk in tks:
                res.extend(tk.split("/"))
            return " ".join(res)

        res = []
        for tk in tks:
            if len(tk) < 3 or re.match(r"[0-9,\.-]+$", tk):
                # çŸ­ token æˆ–æ•°å­—ï¼šç›´æ¥ä¿ç•™
                res.append(tk)
                continue
            tkslist = []
            if len(tk) > 10:
                # å¾ˆé•¿çš„ token ä¸ DFSï¼ˆæ§åˆ¶å¤æ‚åº¦ï¼‰ï¼Œç›´æ¥ä¿ç•™åŸæ ·
                tkslist.append(tk)
            else:
                # å¯¹è¾ƒçŸ­çš„ä¸­æ–‡ token ç”¨ DFS å°è¯•æ›´ç»†ç²’åº¦åˆ‡åˆ†
                self.dfs_(tk, 0, [], tkslist)
            if len(tkslist) < 2:
                # æ²¡æœ‰äº§ç”Ÿå¯å¯¹æ¯”çš„å€™é€‰ï¼šåŸæ ·ä¿ç•™
                res.append(tk)
                continue
            # é€‰ç¬¬ 2 é«˜åˆ†ï¼ˆç´¢å¼• 1ï¼‰çš„æ–¹æ¡ˆï¼ˆå®ç°è€…ä¸»è§‚ç­–ç•¥ï¼Œé¿å…è¿‡åº¦åˆå¹¶ï¼‰
            stk = self.sortTks_(tkslist)[1][0]
            if len(stk) == len(tk):
                # è‹¥åˆ‡åˆ†åé•¿åº¦ç­‰äºåŸé•¿åº¦ï¼ˆæœ¬è´¨æ²¡åˆ‡å¼€ï¼‰ï¼Œé€€å›åŸ token
                stk = tk
            else:
                if re.match(r"[a-z\.-]+$", tk):
                    # è‹±æ–‡ tokenï¼šè‹¥æœ‰çŸ­å­è¯ï¼ˆ<3ï¼‰ï¼Œåˆ™é€€å›åŸ tokenï¼›å¦åˆ™ä»¥ç©ºæ ¼è¿æ¥å­è¯
                    for t in stk:
                        if len(t) < 3:
                            stk = tk
                            break
                    else:
                        stk = " ".join(stk)
                else:
                    # ä¸­æ–‡ tokenï¼šå­è¯ç›´æ¥ç”¨ç©ºæ ¼è¿æ¥
                    stk = " ".join(stk)

            res.append(stk)

        # æœ€åå¯¹è‹±æ–‡ token åšè¯å½¢å½’ä¸€ï¼ˆlemma+stemï¼‰
        return " ".join(self.english_normalize_(res))


def is_chinese(s):
    """
    åˆ¤æ–­å•ä¸ªå­—ç¬¦æ˜¯å¦æ˜¯å¸¸ç”¨ CJK æ±‰å­—åŒºé—´ï¼ˆ\u4e00-\u9fa5ï¼‰ã€‚
    æ³¨æ„ï¼šè¿™ä¸æ˜¯ä¸¥æ ¼æ„ä¹‰ä¸Šæ‰€æœ‰ä¸­æ–‡å­—ç¬¦çš„å…¨é›†ï¼Œä»…è¦†ç›–å¸¸è§æ±‰å­—ã€‚
    """
    if s >= u'\u4e00' and s <= u'\u9fa5':
        return True
    else:
        return False


def is_number(s):
    """åˆ¤æ–­å•ä¸ªå­—ç¬¦æ˜¯å¦æ˜¯é˜¿æ‹‰ä¼¯æ•°å­—ï¼ˆ0-9ï¼Œå¯¹åº” \u0030-\u0039ï¼‰"""
    if s >= u'\u0030' and s <= u'\u0039':
        return True
    else:
        return False


def is_alphabet(s):
    """åˆ¤æ–­å•ä¸ªå­—ç¬¦æ˜¯å¦æ˜¯è‹±æ–‡å­—æ¯ï¼ˆA-Z æˆ– a-zï¼‰"""
    if (s >= u'\u0041' and s <= u'\u005a') or (
            s >= u'\u0061' and s <= u'\u007a'):
        return True
    else:
        return False


def naiveQie(txt):
    """
    æœ´ç´ åˆ‡è¯å™¨ï¼š
    - å…ˆæŒ‰ç©ºæ ¼åˆ‡ä¸ºè‹¥å¹²ç‰‡æ®µ
    - è‹¥ç›¸é‚»ä¸¤ä¸ªç‰‡æ®µéƒ½ä»¥è‹±æ–‡å­—æ¯ç»“å°¾/å¼€å¤´ï¼ˆå®é™…ä¸Šæ˜¯â€œæœ« token æœ«å°¾æ˜¯å­—æ¯ + å½“å‰ token æœ«å°¾æ˜¯å­—æ¯â€çš„åˆ¤æ–­ï¼‰ï¼Œ
      åœ¨å®ƒä»¬ä¹‹é—´æ’å…¥ä¸€ä¸ªç‹¬ç«‹çš„ç©ºæ ¼ tokenï¼Œç¡®ä¿å±•ç¤ºæ—¶è‹±æ–‡ä¹‹é—´æœ‰ç©ºæ ¼
    - è¿”å›ä¸€ä¸ªåŒ…å«å­—ç¬¦ä¸²å’Œç©ºæ ¼ token çš„åˆ—è¡¨ï¼ˆå¦‚ ["Deep", " ", "Learning"]ï¼‰
    """
    tks = []
    for t in txt.split():
        # å¦‚æœä¸Šä¸€ä¸ª token å’Œå½“å‰ token éƒ½ä»¥è‹±æ–‡å­—æ¯ç»“å°¾ï¼Œåˆ™åœ¨å®ƒä»¬ä¹‹é—´æ’å…¥ç©ºæ ¼ token
        if tks and re.match(r".*[a-zA-Z]$", tks[-1]
                            ) and re.match(r".*[a-zA-Z]$", t):
            tks.append(" ")
        tks.append(t)
    return tks

```

### 4.1 key_/rkey_

ä¸ºä»€ä¹ˆè¦å•ç‹¬å®šä¹‰ `self.key_` å’Œ `self.rkey_` ä¸¤ä¸ªæ–¹æ³•ï¼Œå¹¶æŠŠè½¬æ¢ç»“æœå­˜åˆ° trie é‡Œã€‚

`datrie.Trie` æ˜¯ **åŒæ•°ç»„ Trie**ï¼Œå®ƒæœ‰å‡ ä¸ªç‰¹ç‚¹ï¼š

1. **åªèƒ½å­˜å‚¨å­—ç¬¦ä¸² key**ï¼Œä¸èƒ½ç›´æ¥å­˜å‚¨ Unicode æˆ–å­—èŠ‚æ•°ç»„ã€‚
2. **å‰ç¼€åŒ¹é…æ•ˆç‡é«˜**ï¼Œå¯ä»¥ç”¨ `has_keys_with_prefix` åšå¿«é€Ÿæœç´¢ã€‚
3. å› ä¸ºåº•å±‚æ˜¯æ•°ç»„ç´¢å¼•å®ç°ï¼Œ**key ä¸­çš„å­—ç¬¦å¿…é¡»åœ¨ alphabet èŒƒå›´å†…**ï¼ˆæ¯”å¦‚ `string.printable`ï¼‰ã€‚

æ‰€ä»¥æˆ‘ä»¬ä¸èƒ½ç›´æ¥æŠŠä¸­æ–‡ã€å…¨è§’å­—ç¬¦æˆ–è€… UTF-8 å­—èŠ‚åºåˆ—ç›´æ¥å½“ key å­˜ã€‚å¿…é¡»å…ˆâ€œç¼–ç â€æˆå¯ä»¥æ”¾è¿› trie çš„ ASCII-safe å­—ç¬¦ä¸²ã€‚


### 4.2 dfs_ 

```python
def dfs_(self, chars, s, preTks, tkslist, _depth=0, _memo=None):
    pass
```
å‚æ•°:
* **chars**ï¼šå¾…åˆ‡åˆ†çš„å­—ç¬¦åˆ—è¡¨æˆ–å­—ç¬¦ä¸²ï¼Œä¾‹å¦‚ `"ä»Šå¤©å¤©æ°”å¾ˆå¥½"` â†’ `['ä»Š','å¤©','å¤©','æ°”','å¾ˆ','å¥½']`
* **s**ï¼šå½“å‰åˆ‡åˆ†èµ·å§‹ä½ç½®ï¼ˆç´¢å¼•ï¼‰
* **preTks**ï¼šå½“å‰å·²ç»åˆ‡å¥½çš„ token åˆ—è¡¨ï¼ˆä¾‹å¦‚ `[("ä»Šå¤©", freq_tag)]`ï¼‰
* **tkslist**ï¼šDFS æœç´¢å¾—åˆ°çš„åˆ‡åˆ†ç»“æœåˆ—è¡¨ï¼ˆæœ€ç»ˆè¾“å‡ºï¼‰
* **_depth**ï¼šé€’å½’æ·±åº¦ï¼Œé˜²æ­¢æ— é™é€’å½’
* **_memo**ï¼šè®°å¿†åŒ–ç¼“å­˜ï¼Œé¿å…é‡å¤è®¡ç®—

**æ ¸å¿ƒç›®æ ‡**ï¼š

* å¯¹ç»™å®šå­—ç¬¦ä¸²åš **å¤šç§å¯èƒ½çš„åˆ‡åˆ†ç»„åˆ**ã€‚
* ä½¿ç”¨ trie åˆ¤æ–­å“ªäº›å­ä¸²æ˜¯è¯å…¸ä¸­çš„åˆæ³• tokenã€‚
* å¯¹æ­§ä¹‰åŒºåŸŸï¼ˆä¾‹å¦‚ `"ä»Šå¤©å¤©"`ï¼‰å°è¯•æ‰€æœ‰å¯èƒ½åˆ‡åˆ†ï¼Œä¿å­˜åˆ° `tkslist`ã€‚

---

### 2ï¸âƒ£ ä¸¾ä¾‹è¯´æ˜

å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ª trie ä¸­å­˜äº†è¿™äº›è¯ï¼š

```text
ä»Šå¤©
å¤©æ°”
å¾ˆå¥½
å¤©
```

æ–‡æœ¬æ˜¯ï¼š

```
chars = list("ä»Šå¤©å¤©æ°”å¾ˆå¥½")  # ['ä»Š','å¤©','å¤©','æ°”','å¾ˆ','å¥½']
```

è°ƒç”¨ï¼š

```python
tkslist = []
tokenizer.dfs_(chars, 0, [], tkslist)
```

---

DFS é€’å½’è¿‡ç¨‹:

1. **ä»ç´¢å¼• 0 å¼€å§‹**ï¼š

   * å°è¯•åˆ‡ `"ä»Š"` â†’ ä¸åœ¨ trie ä¸­ï¼Œè®°ä½œ (`ä»Š`, (-12, ''))
   * å°è¯•åˆ‡ `"ä»Šå¤©"` â†’ åœ¨ trie ä¸­ï¼Œè®°ä½œ (`ä»Šå¤©`, freq_tag)
   * å°è¯•åˆ‡ `"ä»Šå¤©å¤©"` â†’ `"ä»Šå¤©å¤©"` ä¸åœ¨ trie ä¸­ï¼Œåœæ­¢ç»§ç»­æ‰©å±•
   * é€’å½’ä¸‹ä¸€æ­¥ä»ç´¢å¼• 2ï¼ˆ`å¤©`ï¼‰å¼€å§‹

2. **ç´¢å¼• 2**ï¼š

   * å°è¯•åˆ‡ `"å¤©"` â†’ åœ¨ trie ä¸­ (`å¤©`, freq_tag)
   * å°è¯•åˆ‡ `"å¤©æ°”"` â†’ åœ¨ trie ä¸­ (`å¤©æ°”`, freq_tag)
   * å°è¯•åˆ‡ `"å¤©æ°”å¾ˆ"` â†’ ä¸åœ¨ trie ä¸­ â†’ åœæ­¢

3. **ç´¢å¼• 4**ï¼š

   * `"å¾ˆ"` â†’ ä¸åœ¨ trie ä¸­ â†’ è®°ä½œ (`å¾ˆ`, (-12,''))
   * `"å¾ˆå¥½"` â†’ åœ¨ trie ä¸­ â†’ (`å¾ˆå¥½`, freq_tag)

4. **DFS ä¼šç©·ä¸¾æ‰€æœ‰å¯èƒ½åˆ‡åˆ†ç»„åˆ**ï¼š

   * `[("ä»Šå¤©", "å¤©æ°”", "å¾ˆå¥½")]`
   * `[("ä»Šå¤©", "å¤©", "æ°”", "å¾ˆå¥½")]`
   * `[("ä»Š", "å¤©", "å¤©", "æ°”", "å¾ˆå¥½")]`

æœ€ç»ˆ `tkslist` å°±æ˜¯åŒ…å«å¤šç§åˆ‡åˆ†æ–¹æ¡ˆçš„åˆ—è¡¨ã€‚
