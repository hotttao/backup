---
weight: 1
title: "RagFlow Task Executor"
date: 2025-08-20T10:00:00+08:00
lastmod: 2025-08-20T10:00:00+08:00
draft: false
author: "å®‹æ¶›"
authorLink: "https://hotttao.github.io/"
description: "RagFlow Task Executor"
featuredImage: 

tags: ["RAG", "RagFlow"]
categories: ["langchain"]

lightgallery: true

toc:
  auto: false
---

å‰é¢æˆ‘ä»¬ä»‹ç»äº† RagFlow çš„æ¡†æ¶ï¼Œå¯åŠ¨æµç¨‹ï¼Œè¿™ä¸€èŠ‚æˆ‘ä»¬æ¥è¯¦ç»†ä»‹ç» RagFlow çš„æ ¸å¿ƒè¿›ç¨‹ Task Executorã€‚

## 1. Task Executor
task executor å¯åŠ¨çš„å…¥å£åœ¨ `rag/svr/task_executor.py`ï¼Œå¯åŠ¨è¿‡ç¨‹æœ‰å¦‚ä¸‹æ ¸å¿ƒæ­¥éª¤:
1. åˆå§‹åŒ–é…ç½®
2. æ³¨å†Œä¿¡å·é‡
3. ä½¿ç”¨ trio ç®¡ç†å¼‚æ­¥ä»»åŠ¡

åœ¨æˆ‘ä»¬è¯¦ç»†ä»‹ç»æ¯ä¸€æ­¥çš„ä½¿ç”¨ä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆæ¥ç®€å•äº†è§£ä¸€ä¸‹ trioã€‚

```python
async def main():
    settings.init_settings()
    print_rag_settings()
    if sys.platform != "win32":
        # å¦‚æœä¸æ˜¯ Windows ç³»ç»Ÿï¼Œåˆ™æ³¨å†Œä¸¤ä¸ªè‡ªå®šä¹‰ä¿¡å·å¤„ç†å‡½æ•°
        # SIGUSR1 -> å¯åŠ¨ tracemalloc å¹¶ç”Ÿæˆå†…å­˜å¿«ç…§
        # SIGUSR2 -> åœæ­¢ tracemalloc
        signal.signal(signal.SIGUSR1, start_tracemalloc_and_snapshot)
        signal.signal(signal.SIGUSR2, stop_tracemalloc)

    # ä»ç¯å¢ƒå˜é‡ TRACE_MALLOC_ENABLED ä¸­è¯»å–æ˜¯å¦å¯ç”¨å†…å­˜è·Ÿè¸ª (é»˜è®¤ 0ï¼Œå³ä¸å¼€å¯)
    TRACE_MALLOC_ENABLED = int(os.environ.get('TRACE_MALLOC_ENABLED', "0"))
    if TRACE_MALLOC_ENABLED:
        # å¦‚æœå¯ç”¨ï¼Œåˆ™ç«‹å³å¯åŠ¨ tracemalloc å¹¶ç”Ÿæˆä¸€æ¬¡å¿«ç…§
        start_tracemalloc_and_snapshot(None, None)

    # æ³¨å†Œç³»ç»Ÿä¿¡å·å¤„ç†å‡½æ•°
    # SIGINT  -> ä¸€èˆ¬æ˜¯ Ctrl+C ç»ˆæ­¢ç¨‹åº
    # SIGTERM -> kill å‘½ä»¤å‘é€çš„ç»ˆæ­¢ä¿¡å·
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)

    # ä½¿ç”¨ Trio å¼‚æ­¥åº“å¯åŠ¨å¹¶å‘ä»»åŠ¡
    async with trio.open_nursery() as nursery:
        # å¯åŠ¨ä¸€ä¸ªåå°ä»»åŠ¡ï¼šå‘¨æœŸæ€§ä¸ŠæŠ¥ç¨‹åºçŠ¶æ€
        nursery.start_soon(report_status)
        
        # ä¸»å¾ªç¯ï¼šåªè¦ stop_event æœªè¢«è§¦å‘ï¼Œå°±æŒç»­æ‰§è¡Œä»»åŠ¡
        while not stop_event.is_set():
            # ä»»åŠ¡è°ƒåº¦æ§åˆ¶ï¼šè·å–ä¸€ä¸ªä»»åŠ¡æ‰§è¡Œè®¸å¯ï¼ˆé˜²æ­¢ä»»åŠ¡è¿‡å¤šï¼‰
            await task_limiter.acquire()
            # å¯åŠ¨ä¸€ä¸ªæ–°çš„åå°ä»»åŠ¡ï¼štask_managerï¼ˆè´Ÿè´£å¤„ç†å…·ä½“ä»»åŠ¡ï¼‰
            nursery.start_soon(task_manager)

    # å¦‚æœèƒ½èµ°åˆ°è¿™é‡Œï¼Œè¯´æ˜é€»è¾‘å¼‚å¸¸ï¼ˆç†è®ºä¸Šä¸åº”è¯¥åˆ°è¾¾ï¼‰
    logging.error("BUG!!! You should not reach here!!!")

if __name__ == "__main__":
    # å¯ç”¨ faulthandlerï¼Œåœ¨ Python å´©æºƒæ—¶è‡ªåŠ¨æ‰“å° tracebackï¼Œæ–¹ä¾¿è°ƒè¯•
    faulthandler.enable()
    # åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿï¼Œä½¿ç”¨æŒ‡å®šçš„æ¶ˆè´¹è€…å
    init_root_logger(CONSUMER_NAME)
    # ä½¿ç”¨ Trio äº‹ä»¶å¾ªç¯è¿è¡Œ main åç¨‹
    trio.run(main)
```

## 2. Trio ç®€ä»‹

### 2.1 Trio æ˜¯ä»€ä¹ˆ

* **å®šä¹‰**ï¼šTrio æ˜¯ä¸€ä¸ªåŸºäº Python `async/await` è¯­æ³•çš„å¼‚æ­¥ I/O åº“ã€‚
* **è®¾è®¡ç†å¿µ**ï¼šæä¾› *ç®€å•*ã€*å¯é *ã€*ç¬¦åˆäººç±»ç›´è§‰* çš„å¹¶å‘æ¨¡å‹ã€‚
* **ç›®æ ‡**ï¼šè®©å¼€å‘è€…åœ¨å†™å¼‚æ­¥ä»£ç æ—¶ **åƒå†™åŒæ­¥ä»£ç ä¸€æ ·è‡ªç„¶**ï¼ŒåŒæ—¶ä¿æŒå¥å£®æ€§å’Œå¯ç»´æŠ¤æ€§ã€‚

Trio çš„ä½œè€…æ˜¯ Nathaniel J. Smithï¼Œä»–çš„ç†å¿µæ˜¯ï¼š

> â€œæ­£ç¡®çš„æŠ½è±¡æ¯”è¿½æ±‚æé™æ€§èƒ½æ›´é‡è¦ï¼ŒTrio è¦è®©å¹¶å‘ç¼–ç¨‹å˜å¾—ä¸å†ç—›è‹¦ã€‚â€

---

### 2.2 Trio çš„ä½œç”¨

Trio ä¸»è¦ç”¨äºï¼š

1. **é«˜å¹¶å‘ä»»åŠ¡ç®¡ç†**ï¼šæ¯”å¦‚åŒæ—¶å¤„ç†å¤§é‡ç½‘ç»œè¯·æ±‚ã€I/O æ“ä½œã€‚
2. **åå°ä»»åŠ¡è¿è¡Œ**ï¼šå¯åŠ¨/å–æ¶ˆä»»åŠ¡æ›´åŠ ç›´è§‚ã€‚
3. **å—æ§é€€å‡ºæœºåˆ¶**ï¼šä¿è¯ä»»åŠ¡å¯ä»¥ä¼˜é›…åœ°ç»“æŸï¼Œä¸ä¼šç•™â€œåƒµå°¸ä»»åŠ¡â€ã€‚
4. **è°ƒè¯•/ç›‘æ§**ï¼šTrio åœ¨é”™è¯¯ä¼ æ’­å’Œå¼‚å¸¸å¤„ç†ä¸Šè®¾è®¡å¾—æ¯” asyncio æ›´æ¸…æ™°ã€‚

---

### 2.3 Trio ç›¸æ¯”å…¶ä»–å¼‚æ­¥åº“çš„ä¼˜åŠ¿

| ç‰¹æ€§       | `asyncio`                     | `gevent`     | `curio`         | **Trio**                     |
| -------- | ----------------------------- | ------------ | --------------- | ---------------------------- |
| **å¹¶å‘æ¨¡å‹** | äº‹ä»¶å¾ªç¯ + ä»»åŠ¡è°ƒåº¦                   | åç¨‹ï¼ˆåŸºäºç»¿è‰²çº¿ç¨‹ï¼‰   | ç±»ä¼¼ asyncioï¼Œä½†æ›´ç®€æ´ | **ç»“æ„åŒ–å¹¶å‘ï¼ˆnurseryï¼‰**           |
| **é”™è¯¯å¤„ç†** | é”™è¯¯å¯èƒ½è¢«åæ‰ï¼Œéœ€è¦å°å¿ƒ                  | å¼‚å¸¸ä¼ æ’­ä¸è‡ªç„¶      | æ”¹è¿›äº†å¼‚å¸¸å¤„ç†         | **å¼‚å¸¸ä¼šè‡ªåŠ¨ä¼ æ’­ç»™çˆ¶ä»»åŠ¡ï¼Œé˜²æ­¢é—æ¼**         |
| **ä»»åŠ¡ç®¡ç†** | `create_task`/`gather` å®¹æ˜“æ³„éœ²ä»»åŠ¡ | åç¨‹è°ƒåº¦éšå¼ï¼Œä¸é€æ˜   | ä»»åŠ¡ç®¡ç†ç®€å•          | **æ˜¾å¼çš„ä»»åŠ¡ä½œç”¨åŸŸï¼ˆnurseryï¼‰ï¼Œé˜²æ­¢ä»»åŠ¡æ³„éœ²** |
| **æ˜“ç”¨æ€§**  | åŠŸèƒ½å¤šä½†å¤æ‚                        | è¯­æ³•çœ‹ä¼¼åŒæ­¥ï¼Œä½†è°ƒè¯•éš¾  | æ›´è½»é‡ï¼Œä½†ä¸æ´»è·ƒ        | **API ç®€æ´ç›´è§‚ï¼Œå¼‚å¸¸å¤„ç†æ›´å®‰å…¨**         |
| **ç”Ÿæ€**   | å†…ç½®åº“ï¼Œç”Ÿæ€æœ€å¼º                      | åœ¨ web æ¡†æ¶ä¸­ç”¨å¾—å¤š | åœæ›´è¶‹åŠ¿            | ç¤¾åŒºå°ä¼—ï¼Œä½†è¶Šæ¥è¶Šè¢«å…³æ³¨                 |

ğŸ‘‰ **æœ€å¤§äº®ç‚¹**ï¼š

* Trio æå‡ºäº† **â€œç»“æ„åŒ–å¹¶å‘ (Structured Concurrency)â€** çš„æ¦‚å¿µï¼š

  * ä»»åŠ¡å¿…é¡»è¿è¡Œåœ¨ä¸€ä¸ªæ˜ç¡®çš„ä½œç”¨åŸŸï¼ˆnurseryï¼‰é‡Œã€‚
  * å½“ä½œç”¨åŸŸé€€å‡ºæ—¶ï¼Œæ‰€æœ‰å­ä»»åŠ¡å¿…é¡»å®Œæˆæˆ–è¢«å–æ¶ˆã€‚
  * é¿å…äº† `asyncio.create_task` è¿™ç§â€œæ‚¬ç©ºä»»åŠ¡â€å¯¼è‡´çš„ bugã€‚

---

### 2.4 Trio çš„æ ¸å¿ƒç”¨æ³•

#### åŸºç¡€ï¼šè¿è¡Œä¸€ä¸ªå¼‚æ­¥å‡½æ•°

```python
import trio

async def hello():
    print("Hello, Trio!")

trio.run(hello)  # ç±»ä¼¼ asyncio.run
```

---

#### Nurseryï¼šç»“æ„åŒ–å¹¶å‘

Trio çš„æ ¸å¿ƒæ˜¯ `nursery`ï¼ˆä»»åŠ¡æ‰˜ç®¡åŒºï¼‰ï¼š

```python
import trio

async def child(name, delay):
    await trio.sleep(delay)
    print(f"{name} done")

async def main():
    async with trio.open_nursery() as nursery:
        # å¯åŠ¨ä¸¤ä¸ªå­ä»»åŠ¡ï¼Œå®ƒä»¬ç”± nursery ç®¡ç†
        nursery.start_soon(child, "task1", 2)
        nursery.start_soon(child, "task2", 1)
    # é€€å‡º with æ—¶ï¼Œä¿è¯æ‰€æœ‰å­ä»»åŠ¡å®Œæˆæˆ–è¢«å–æ¶ˆ
    print("All tasks finished")

trio.run(main)
```

âœ… **ä¼˜ç‚¹**ï¼š

* å­ä»»åŠ¡çš„ç”Ÿå‘½å‘¨æœŸå’Œ `with` å—ç»‘å®šï¼Œé˜²æ­¢ä»»åŠ¡æ³„éœ²ã€‚
* å¦‚æœä¸€ä¸ªå­ä»»åŠ¡æŠ¥é”™ï¼Œé”™è¯¯ä¼šä¼ é€’åˆ° nurseryï¼Œå¹¶å–æ¶ˆæ‰€æœ‰å…¶ä»–å­ä»»åŠ¡ã€‚

---

#### å¹¶å‘é€šä¿¡ï¼ˆé€šé“ï¼‰

Trio æä¾›å†…å»ºçš„ **é«˜æ•ˆé€šé“ (Channel)** æ¥åšä»»åŠ¡é—´é€šä¿¡ã€‚

```python
import trio

async def sender(send_channel):
    for i in range(3):
        await send_channel.send(i)
    send_channel.close()

async def receiver(receive_channel):
    async for value in receive_channel:
        print("Received:", value)

async def main():
    send_channel, receive_channel = trio.open_memory_channel(0)
    async with trio.open_nursery() as nursery:
        nursery.start_soon(sender, send_channel)
        nursery.start_soon(receiver, receive_channel)

trio.run(main)
```

---

#### å–æ¶ˆå’Œè¶…æ—¶æ§åˆ¶

Trio çš„å–æ¶ˆæœºåˆ¶éå¸¸ç›´è§‚ï¼Œç»“åˆ **CancelScope** ä½¿ç”¨ï¼š

```python
import trio

async def slow_task():
    await trio.sleep(5)
    print("Done")

async def main():
    with trio.move_on_after(2):  # è¶…è¿‡ 2 ç§’è‡ªåŠ¨å–æ¶ˆ
        await slow_task()
    print("Task was cancelled")

trio.run(main)
```

---

#### é”™è¯¯ä¼ æ’­

Trio çš„ nursery ä¼šè‡ªåŠ¨æŠŠå­ä»»åŠ¡é”™è¯¯ä¼ æ’­ç»™çˆ¶ä»»åŠ¡ï¼Œä¿è¯ä½ ä¸ä¼šé—æ¼å¼‚å¸¸ï¼š

```python
import trio

async def broken():
    raise RuntimeError("Boom!")

async def main():
    async with trio.open_nursery() as nursery:
        nursery.start_soon(broken)
        # è¿™é‡Œä¸ä¼šæ— é™æŒ‚èµ·ï¼Œè€Œæ˜¯ç«‹å³æ”¶åˆ°å¼‚å¸¸
    print("This will not run")

trio.run(main)  # ä¼šæŠ¥é”™ RuntimeError: Boom!
```

---

### 2.5 Trio çš„é€‚ç”¨åœºæ™¯

* éœ€è¦ **é«˜å¯é æ€§** çš„å¼‚æ­¥ä»»åŠ¡ç®¡ç†ï¼ˆæ¯”å¦‚æœåŠ¡ç«¯ã€åå° workerï¼‰
* å¯¹ **è°ƒè¯•ã€å¼‚å¸¸å¤„ç†ã€å®‰å…¨é€€å‡º** æœ‰è¦æ±‚çš„é¡¹ç›®
* ä¸æƒ³è¢« asyncio çš„â€œéšå¼ä»»åŠ¡â€å‘åˆ°
* å­¦ä¹ ç»“æ„åŒ–å¹¶å‘çš„æœ€ä½³é€‰æ‹©

---

ğŸ“Œ **ä¸€å¥è¯æ€»ç»“**ï¼š

> Trio çš„æ ¸å¿ƒä¼˜åŠ¿æ˜¯ **ç»“æ„åŒ–å¹¶å‘ + ç®€æ´ API + å¼ºå¼‚å¸¸å®‰å…¨**ï¼Œæ¯” asyncio æ›´ç›´è§‚ã€æ›´å®‰å…¨ï¼Œé€‚åˆéœ€è¦é•¿æœŸç»´æŠ¤ã€ç¨³å®šå¯é çš„å¼‚æ­¥åº”ç”¨ã€‚


## 3. Trio ç®¡ç†çš„ä»»åŠ¡
task scheduler ä¸­ä½¿ç”¨ trio å¯åŠ¨äº†å¦‚ä¸‹ä»»åŠ¡:
1. `report_status`: ä¸ŠæŠ¥ä»»åŠ¡æ‰§è¡ŒçŠ¶æ€
2. `task_manager`: ä» redis é˜Ÿåˆ—è·å– taskï¼Œæ‰§è¡Œ task çš„å¤„ç†

ragflow ä½¿ç”¨ redis ä½œä¸ºæ¶ˆæ¯ä¸­é—´ä»¶:
1. ä½¿ç”¨ stream é˜Ÿåˆ—æ¥ä¼ é€’ task
2. ä½¿ç”¨ zset æ”¶é›†å’Œç»Ÿè®¡ä»»åŠ¡çŠ¶æ€
3. ä½¿ç”¨ redis åˆ†å¸ƒå¼é”ï¼Œä¿è¯åªæœ‰ä¸€ä¸ªèŠ‚ç‚¹ï¼Œå®Œæˆè¿‡æœŸçŠ¶æ€çš„æ¸…ç†

### 3.1 redis é˜Ÿåˆ—
æˆ‘ä»¬å…ˆå›é¡¾ä¸€ä¸‹ redis streamã€zsetã€list çš„æ“ä½œ

#### åŸºäº List çš„é˜Ÿåˆ—æ“ä½œï¼ˆValkeyï¼‰

| Redis å‘½ä»¤                      | ä½œç”¨     | è¯´æ˜            | Valkey Redis client å¯¹åº”æ–¹æ³•     |
| ----------------------------- | ------ | ------------- | ---------------------------- |
| `LPUSH key value`             | å·¦ç«¯å…¥é˜Ÿ   | å°†å…ƒç´ æ’å…¥åˆ—è¡¨å·¦ç«¯ï¼ˆé˜Ÿå¤´ï¼‰ | `r.lpush(key, value)`        |
| `RPUSH key value`             | å³ç«¯å…¥é˜Ÿ   | å°†å…ƒç´ æ’å…¥åˆ—è¡¨å³ç«¯ï¼ˆé˜Ÿå°¾ï¼‰ | `r.rpush(key, value)`        |
| `LPOP key`                    | å·¦ç«¯å‡ºé˜Ÿ   | ç§»é™¤å¹¶è¿”å›å·¦ç«¯å…ƒç´      | `r.lpop(key)`                |
| `RPOP key`                    | å³ç«¯å‡ºé˜Ÿ   | ç§»é™¤å¹¶è¿”å›å³ç«¯å…ƒç´      | `r.rpop(key)`                |
| `BLPOP key [key ...] timeout` | é˜»å¡å·¦ç«¯å‡ºé˜Ÿ | é˜»å¡ç­‰å¾…å…ƒç´         | `r.blpop(keys, timeout)`     |
| `BRPOP key [key ...] timeout` | é˜»å¡å³ç«¯å‡ºé˜Ÿ | é˜»å¡ç­‰å¾…å…ƒç´         | `r.brpop(keys, timeout)`     |
| `LLEN key`                    | è·å–é•¿åº¦   | åˆ—è¡¨å…ƒç´ æ•°é‡        | `r.llen(key)`                |
| `LRANGE key start stop`       | è·å–èŒƒå›´   | æŸ¥çœ‹é˜Ÿåˆ—å†…å®¹        | `r.lrange(key, start, stop)` |

---

#### åŸºäº Stream çš„é˜Ÿåˆ—æ“ä½œï¼ˆValkeyï¼‰

| Redis å‘½ä»¤                                                     | ä½œç”¨        | è¯´æ˜            | Valkey Redis client å¯¹åº”æ–¹æ³•                                        |
| ------------------------------------------------------------ | --------- | ------------- | --------------------------------------------------------------- |
| `XADD key * field value`                                     | å†™å…¥æ¶ˆæ¯      | å‘ Stream è¿½åŠ æ¶ˆæ¯ | `r.xadd(key, message_dict)`                                     |
| `XREAD COUNT n STREAMS key id`                               | è¯»å–æ¶ˆæ¯      | ä»æŒ‡å®š ID ä¹‹åè¯»å–   | `r.xread(streams={key: id}, count=n)`                           |
| `XREAD BLOCK milliseconds STREAMS key id`                    | é˜»å¡è¯»å–      | é˜»å¡ç­‰å¾…æ–°æ¶ˆæ¯       | `r.xread(streams={key: id}, block=ms)`                          |
| `XGROUP CREATE key groupname id`                             | åˆ›å»ºæ¶ˆè´¹è€…ç»„    | ç”¨äºå¤šæ¶ˆè´¹è€…å…±äº«      | `r.xgroup_create(key, groupname, id)`                           |
| `XREADGROUP GROUP groupname consumer COUNT n STREAMS key id` | æ¶ˆè´¹æ¶ˆæ¯      | æ¶ˆè´¹è€…ç»„è¯»å–        | `r.xreadgroup(groupname, consumer, streams={key: id}, count=n)` |
| `XACK key groupname id [id ...]`                             | ç¡®è®¤æ¶ˆæ¯      | æ¶ˆè´¹å®Œæˆç¡®è®¤        | `r.xack(key, groupname, ids)`                                   |
| `XPENDING key groupname [start end count] [consumer]`        | æŸ¥çœ‹å¾…ç¡®è®¤     | æœªç¡®è®¤æ¶ˆæ¯çŠ¶æ€       | `r.xpending(key, groupname)`                                    |
| `XDEL key id [id ...]`                                       | åˆ é™¤æ¶ˆæ¯      | åˆ é™¤ Stream æ¶ˆæ¯  | `r.xdel(key, *ids)`                                             |
| `XTRIM key MAXLEN ~ n`                                       | ä¿®å‰ª Stream | ä¿æŒé•¿åº¦ä¸Šé™        | `r.xtrim(key, maxlen=n, approximate=True)`                      |

---

#### åŸºäº Sorted Set çš„å»¶è¿Ÿé˜Ÿåˆ—ï¼ˆValkeyï¼‰

| Redis å‘½ä»¤                             | ä½œç”¨      | è¯´æ˜            | Valkey Redis client å¯¹åº”æ–¹æ³•                          |
| ------------------------------------ | ------- | ------------- | ------------------------------------------------- |
| `ZADD key score member`              | æ·»åŠ ä»»åŠ¡    | score ä½œä¸ºæ‰§è¡Œæ—¶é—´æˆ³ | `r.zadd(key, {member: score})`                    |
| `ZRANGE key start stop [WITHSCORES]` | æŸ¥è¯¢ä»»åŠ¡    | è·å–åŒºé—´å†…ä»»åŠ¡       | `r.zrange(key, start, stop, withscores=True)`     |
| `ZRANGEBYSCORE key min max`          | è·å–å¯æ‰§è¡Œä»»åŠ¡ | score â‰¤ å½“å‰æ—¶é—´  | `r.zrangebyscore(key, min, max, withscores=True)` |
| `ZREM key member`                    | ç§»é™¤ä»»åŠ¡    | æ‰§è¡Œå®Œæˆååˆ é™¤       | `r.zrem(key, member)`                             |

---

#### æ€»ç»“

| é˜Ÿåˆ—ç±»å‹       | ä¼˜ç‚¹              | é€‚ç”¨åœºæ™¯       | Valkey client ç‰¹ç‚¹                     |
| ---------- | --------------- | ---------- | ------------------------------------ |
| List       | ç®€å•ã€ä½å»¶è¿Ÿã€é˜»å¡æ“ä½œ     | å•ç”Ÿäº§è€…-å•æ¶ˆè´¹è€…  | ç›´æ¥è°ƒç”¨ `lpush/rpush/lpop` ç­‰            |
| Stream     | æ¶ˆæ¯ç¡®è®¤ã€å¤šæ¶ˆè´¹è€…ç»„ã€å†å²è®°å½• | é«˜å¯é æ¶ˆæ¯é˜Ÿåˆ—    | æä¾›å®Œæ•´ `xadd/xread/xack/xgroup` æ–¹æ³•     |
| Sorted Set | å¯å»¶è¿Ÿã€æŒ‰ä¼˜å…ˆçº§æ¶ˆè´¹      | å»¶è¿Ÿä»»åŠ¡ã€ä¼˜å…ˆçº§é˜Ÿåˆ— | `zadd/zrangebyscore/zrem` æ”¯æŒæ—¶é—´/ä¼˜å…ˆçº§æ“ä½œ |



### 3.1 report_status

è¿™æ®µä»£ç çš„ä¸»è¦åŠŸèƒ½æ˜¯ï¼š

1. **å¿ƒè·³ä¸ŠæŠ¥**
   * ä½¿ç”¨ redis zset ä¿å­˜å¿ƒè·³ä¿¡æ¯
   * å‘¨æœŸæ€§æŠŠæ¶ˆè´¹è€…çŠ¶æ€ï¼ˆå¾…å¤„ç†ã€è½åã€å½“å‰ä»»åŠ¡ç­‰ï¼‰å†™å…¥ Redisï¼Œæ–¹ä¾¿ç›‘æ§ã€‚

2. **è¿‡æœŸæ•°æ®æ¸…ç†**
   * ä½¿ç”¨ RedisDistributedLock å»æŠ¥åªæœ‰ä¸€ä¸ªè¿›ç¨‹æ‰§è¡Œæ¸…ç†æ“ä½œ
   * åˆ é™¤è¶…è¿‡ 30 åˆ†é’Ÿæœªæ›´æ–°çš„å¿ƒè·³ä¿¡æ¯ã€‚
   * åˆ é™¤å·²ç»ä¸æ´»è·ƒçš„ä»»åŠ¡æ‰§è¡Œå™¨ï¼ˆåŸºäºåˆ†å¸ƒå¼é”ä¿è¯ä¸ä¼šå†²çªï¼‰ã€‚

```python
async def report_status():
    # å£°æ˜ä½¿ç”¨çš„ä¸€äº›å…¨å±€å˜é‡
    global CONSUMER_NAME, BOOT_AT, PENDING_TASKS, LAG_TASKS, DONE_TASKS, FAILED_TASKS

    # å°†å½“å‰æ¶ˆè´¹è€…åç§°åŠ å…¥ Redis çš„é›†åˆ(set) "TASKEXE"ï¼Œè¡¨ç¤ºè¿™æ˜¯æ´»è·ƒçš„ä»»åŠ¡æ‰§è¡Œå™¨ä¹‹ä¸€
    REDIS_CONN.sadd("TASKEXE", CONSUMER_NAME)

    # åˆ›å»ºä¸€ä¸ªåˆ†å¸ƒå¼é”ï¼Œç”¨äºåœ¨å¤šè¿›ç¨‹/å¤šæœºå™¨ç¯å¢ƒä¸‹æ¸…ç†è¿‡æœŸä»»åŠ¡æ‰§è¡Œå™¨
    redis_lock = RedisDistributedLock(
        "clean_task_executor", lock_value=CONSUMER_NAME, timeout=60
    )

    # ä¸»å¾ªç¯ï¼šæ¯éš”ä¸€æ®µæ—¶é—´ä¸ŠæŠ¥çŠ¶æ€
    while True:
        try:
            # è·å–å½“å‰æ—¶é—´
            now = datetime.now()

            # æŸ¥è¯¢æ¶ˆæ¯é˜Ÿåˆ—ä¸­è¯¥æ¶ˆè´¹è€…ç»„çš„çŠ¶æ€ä¿¡æ¯ï¼ˆpending å’Œ lagï¼‰
            group_info = REDIS_CONN.queue_info(get_svr_queue_name(0), SVR_CONSUMER_GROUP_NAME)
            if group_info is not None:
                PENDING_TASKS = int(group_info.get("pending", 0))  # å¾…å¤„ç†ä»»åŠ¡æ•°
                LAG_TASKS = int(group_info.get("lag", 0))          # æ¶ˆè´¹è½åä»»åŠ¡æ•°

            # å¤åˆ¶å½“å‰æ­£åœ¨å¤„ç†çš„ä»»åŠ¡åˆ—è¡¨ï¼Œé¿å…ä¿®æ”¹å…±äº«å¯¹è±¡
            current = copy.deepcopy(CURRENT_TASKS)

            # æ„å»ºå¿ƒè·³ä¿¡æ¯ï¼ˆJSONï¼‰ï¼ŒåŒ…å«æ¶ˆè´¹è€…çŠ¶æ€å’Œç»Ÿè®¡ä¿¡æ¯
            heartbeat = json.dumps({
                "name": CONSUMER_NAME,
                "now": now.astimezone().isoformat(timespec="milliseconds"),
                "boot_at": BOOT_AT,
                "pending": PENDING_TASKS,
                "lag": LAG_TASKS,
                "done": DONE_TASKS,
                "failed": FAILED_TASKS,
                "current": current,
            })

            # å°†å¿ƒè·³ä¿¡æ¯å­˜å…¥ Redis zsetï¼Œscore ä¸ºå½“å‰æ—¶é—´æˆ³
            REDIS_CONN.zadd(CONSUMER_NAME, heartbeat, now.timestamp())
            logging.info(f"{CONSUMER_NAME} reported heartbeat: {heartbeat}")

            # æ¸…ç†è¿‡æœŸå¿ƒè·³ä¿¡æ¯ï¼ˆè¶…è¿‡ 30 åˆ†é’Ÿæœªæ›´æ–°çš„è®°å½•ï¼‰
            expired = REDIS_CONN.zcount(CONSUMER_NAME, 0, now.timestamp() - 60 * 30)
            if expired > 0:
                REDIS_CONN.zpopmin(CONSUMER_NAME, expired)

            # æ¸…ç†è¿‡æœŸçš„ä»»åŠ¡æ‰§è¡Œå™¨
            # å…ˆå°è¯•è·å–åˆ†å¸ƒå¼é”ï¼Œä¿è¯åªæœ‰ä¸€ä¸ªæ¶ˆè´¹è€…åœ¨æ¸…ç†
            if redis_lock.acquire():
                # è·å–æ‰€æœ‰æ´»è·ƒçš„ä»»åŠ¡æ‰§è¡Œå™¨
                task_executors = REDIS_CONN.smembers("TASKEXE")
                for consumer_name in task_executors:
                    if consumer_name == CONSUMER_NAME:
                        continue  # è·³è¿‡è‡ªå·±
                    # é€šè¿‡è¶…æ—¶é—´å†…çš„å¿ƒè·³æ¬¡æ•°ï¼Œå¦‚æœä¸º 0ï¼Œè¡¨ç¤ºè¶…æ—¶æ—¶é—´å†…æ²¡ä¸ŠæŠ¥è¿‡å¿ƒè·³
                    expired = REDIS_CONN.zcount(
                        consumer_name,
                        now.timestamp() - WORKER_HEARTBEAT_TIMEOUT,  # è¶…æ—¶æ—¶é—´ä¹‹å‰
                        now.timestamp() + 10                         # å½“å‰æ—¶é—´ç¨å¾®å®½å®¹ä¸€ç‚¹
                    )
                    if expired == 0:
                        # å¦‚æœæ²¡æœ‰å¿ƒè·³ï¼Œè®¤ä¸ºè¯¥æ‰§è¡Œå™¨å·²è¿‡æœŸï¼Œåˆ é™¤è®°å½•
                        logging.info(f"{consumer_name} expired, removed")
                        REDIS_CONN.srem("TASKEXE", consumer_name)
                        REDIS_CONN.delete(consumer_name)

        except Exception:
            # æ•è·æ‰€æœ‰å¼‚å¸¸ï¼Œé˜²æ­¢åç¨‹é€€å‡ºï¼Œå¹¶æ‰“å°å¼‚å¸¸æ—¥å¿—
            logging.exception("report_status got exception")

        finally:
            # ç¡®ä¿é‡Šæ”¾åˆ†å¸ƒå¼é”
            redis_lock.release()

        # å¼‚æ­¥ä¼‘çœ  30 ç§’ï¼Œå‘¨æœŸæ€§ä¸ŠæŠ¥
        await trio.sleep(30)
```

redis åˆ†å¸ƒå¼é”å®ç°å¦‚ä¸‹:
1. acquire æ–¹æ³•ï¼Œæ‰§è¡Œå‰å…ˆå°è¯•é‡Šæ”¾é”ï¼Œæ˜¯ä¸ºäº†åˆ·æ–° ttlï¼Œå¹¶ä¸”å¯ä»¥è´Ÿè½½å‡è¡¡ä¹ˆï¼Ÿ

```python
class RedisDistributedLock:
    def __init__(self, lock_key, lock_value=None, timeout=10, blocking_timeout=1):
        self.lock_key = lock_key
        if lock_value:
            self.lock_value = lock_value
        else:
            self.lock_value = str(uuid.uuid4())
        self.timeout = timeout
        self.lock = Lock(REDIS_CONN.REDIS, lock_key, timeout=timeout, blocking_timeout=blocking_timeout)

    def acquire(self):
        REDIS_CONN.delete_if_equal(self.lock_key, self.lock_value)
        return self.lock.acquire(token=self.lock_value)

    async def spin_acquire(self):
        REDIS_CONN.delete_if_equal(self.lock_key, self.lock_value)
        while True:
            if self.lock.acquire(token=self.lock_value):
                break
            await trio.sleep(10)

    def release(self):
        REDIS_CONN.delete_if_equal(self.lock_key, self.lock_value)

```

### 3.2 task_manager
task_manager æ˜¯ä»»åŠ¡å¤„ç†è¿‡ç¨‹:
1. collect ç”¨äºä» redis stream é˜Ÿåˆ—ä¸­è·å–ä»»åŠ¡
2. do_handle_task æ‰§è¡Œ task
3. set_progress ç”¨äºæ›´æ–° task è¿›åº¦
4. redis_msg.ack(): æˆåŠŸå¤„ç†æ¶ˆæ¯ï¼Œæ¶ˆæ¯ç¡®è®¤

```python
async def handle_task():
    global DONE_TASKS, FAILED_TASKS
    redis_msg, task = await collect()
    if not task:
        await trio.sleep(5)
        return
    try:
        logging.info(f"handle_task begin for task {json.dumps(task)}")
        CURRENT_TASKS[task["id"]] = copy.deepcopy(task)
        await do_handle_task(task)
        DONE_TASKS += 1
        CURRENT_TASKS.pop(task["id"], None)
        logging.info(f"handle_task done for task {json.dumps(task)}")
    except Exception as e:
        FAILED_TASKS += 1
        CURRENT_TASKS.pop(task["id"], None)
        try:
            err_msg = str(e)
            while isinstance(e, exceptiongroup.ExceptionGroup):
                e = e.exceptions[0]
                err_msg += ' -- ' + str(e)
            # æ›´æ–° task è¿›åº¦
            set_progress(task["id"], prog=-1, msg=f"[Exception]: {err_msg}")
        except Exception:
            pass
        logging.exception(f"handle_task got exception for task {json.dumps(task)}")
    redis_msg.ack()

```

#### collect
task æ¶ˆè´¹ä½äº RedisDB çš„ get_unacked_iterator æ–¹æ³•ï¼Œè¿”å›ä¸€ä¸ªè¿­ä»£å™¨ï¼Œæ¯æ¬¡ yeild ä¸€ä¸ª taskã€‚

redis stream æ¶ˆæ¯é˜Ÿåˆ—ï¼Œæ¯”è¾ƒéš¾ç†è§£çš„æ˜¯ msg_id çš„å€¼ã€‚
1. PEL ä¸­çš„æ¶ˆæ¯ä¼šè®°å½•æ‰€å±çš„æ¶ˆè´¹è€…
2. msg_id ä¸º 0 æ—¶ï¼Œä¼šä» PEL ä¸­å¼€å§‹è¯»å–å±äºè‡ªå·±çš„æ¶ˆæ¯ï¼Œæˆ–è€…æ˜¯ æœª ack çš„æ–°æ¶ˆæ¯
3. æ­£å¸¸é€»è¾‘æ¯æ¬¡ msg_id=0 å³å¯ï¼Œä½†æ˜¯ task exector ä¼šå¯åŠ¨å¤šä¸ª task_manager, æ‰€ä»¥éœ€è¦ä¼ å…¥ msg_id ç¡®ä¿ä¸ä¼šé‡å¤æ¶ˆè´¹å…¶ä»– task_manager æ­£åœ¨å¤„ç†çš„æ¶ˆæ¯ã€‚ 

```python
class RedisDB:
    def queue_consumer(self, queue_name, group_name, consumer_name, msg_id=b">") -> RedisMsg:
        """https://redis.io/docs/latest/commands/xreadgroup/"""
        for _ in range(3):
            try:

                try:
                    group_info = self.REDIS.xinfo_groups(queue_name)
                    if not any(gi["name"] == group_name for gi in group_info):
                        # åˆ›å»ºé˜Ÿåˆ—å’Œæ¶ˆè´¹è€…ç»„
                        self.REDIS.xgroup_create(queue_name, group_name, id="0", mkstream=True)
                except redis.exceptions.ResponseError as e:
                    if "no such key" in str(e).lower():
                        self.REDIS.xgroup_create(queue_name, group_name, id="0", mkstream=True)
                    elif "busygroup" in str(e).lower():
                        logging.warning("Group already exists, continue.")
                        pass
                    else:
                        raise

                args = {
                    "groupname": group_name,
                    "consumername": consumer_name,
                    "count": 1,
                    "block": 5,
                    "streams": {queue_name: msg_id},
                }
                # è¯»å–æ¶ˆæ¯é˜Ÿåˆ—ä¸­çš„æ¶ˆæ¯, xreadgroup è¿”å›å€¼çš„ç±»å‹
                # [
                #     (
                #         "mystream",  # Stream åç§°
                #         [
                #             ("1692633600000-0", {"task": "A"})  # æ¶ˆæ¯åˆ—è¡¨ï¼Œæ•°é‡åŒ count
                #         ]
                #     )
                # ]
                messages = self.REDIS.xreadgroup(**args)
                if not messages:
                    return None
                stream, element_list = messages[0]
                if not element_list:
                    return None
                msg_id, payload = element_list[0]
                res = RedisMsg(self.REDIS, queue_name, group_name, msg_id, payload)
                return res
            except Exception as e:
                if str(e) == 'no such key':
                    pass
                else:
                    logging.exception(
                        "RedisDB.queue_consumer "
                        + str(queue_name)
                        + " got exception: "
                        + str(e)
                    )
                    self.__open__()
        return None

    def get_unacked_iterator(self, queue_names: list[str], group_name, consumer_name):
        try:
            for queue_name in queue_names:
                try:
                    # è·å–é˜Ÿåˆ—çš„æ¶ˆè´¹è€…ç»„ä¿¡æ¯
                    group_info = self.REDIS.xinfo_groups(queue_name)
                except Exception as e:
                    if str(e) == 'no such key':
                        logging.warning(f"RedisDB.get_unacked_iterator queue {queue_name} doesn't exist")
                        continue
                # æ¶ˆè´¹è€…ç»„ä¸å­˜åœ¨
                if not any(gi["name"] == group_name for gi in group_info):
                    logging.warning(f"RedisDB.get_unacked_iterator queue {queue_name} group {group_name} doesn't exist")
                    continue
                current_min = 0
                while True:
                    payload = self.queue_consumer(queue_name, group_name, consumer_name, current_min)
                    if not payload:
                        break
                    current_min = payload.get_msg_id()
                    logging.info(f"RedisDB.get_unacked_iterator {queue_name} {consumer_name} {current_min}")
                    yield payload
        except Exception:
            logging.exception(
                "RedisDB.get_unacked_iterator got exception: "
            )
            self.__open__()

```

ç†è§£äº†æ¶ˆæ¯çš„æ¶ˆè´¹é€»è¾‘ï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹ collect çš„å…·ä½“æ‰§è¡Œæµç¨‹:
1. UNACKED_ITERATOR å±äºå…¨å±€çš„è¿­ä»£å™¨ï¼Œå¯ä»¥ä¿è¯æ¶ˆæ¯æŒ‰é¡ºåºæ¶ˆè´¹
2. redis æ¶ˆæ¯ä¸­ï¼Œåªä¿å­˜äº† task_idï¼Œè¦è·å–ä»»åŠ¡è¯¦ç»†å‚æ•°ï¼Œéœ€è¦é€šè¿‡ `TaskService.get_task(msg["id"])`
2. collect æœ€ç»ˆè¿”å›redis_msg å’Œ task

```python
async def collect():
    global CONSUMER_NAME, DONE_TASKS, FAILED_TASKS
    global UNACKED_ITERATOR

    svr_queue_names = get_svr_queue_names()
    try:
        if not UNACKED_ITERATOR:
            UNACKED_ITERATOR = REDIS_CONN.get_unacked_iterator(svr_queue_names, SVR_CONSUMER_GROUP_NAME, CONSUMER_NAME)
        try:
            redis_msg = next(UNACKED_ITERATOR)
        except StopIteration:
            for svr_queue_name in svr_queue_names:
                redis_msg = REDIS_CONN.queue_consumer(svr_queue_name, SVR_CONSUMER_GROUP_NAME, CONSUMER_NAME)
                if redis_msg:
                    break
    except Exception:
        logging.exception("collect got exception")
        return None, None

    if not redis_msg:
        return None, None
    # msg æ¶ˆæ¯è´Ÿè½½
    msg = redis_msg.get_message()
    if not msg:
        logging.error(f"collect got empty message of {redis_msg.get_msg_id()}")
        redis_msg.ack()
        return None, None

    canceled = False
    # è·å–ä»»åŠ¡
    task = TaskService.get_task(msg["id"])
    if task:
        # åˆ¤æ–­ä»»åŠ¡æ˜¯å¦å·²ç»å–æ¶ˆ
        canceled = has_canceled(task["id"])
    if not task or canceled:
        state = "is unknown" if not task else "has been cancelled"
        FAILED_TASKS += 1
        logging.warning(f"collect task {msg['id']} {state}")
        redis_msg.ack()
        return None, None
    task["task_type"] = msg.get("task_type", "")
    return redis_msg, task

# ä»»åŠ¡å–æ¶ˆåˆ¤æ–­
def has_canceled(task_id):
    try:
        if REDIS_CONN.get(f"{task_id}-cancel"):
            return True
    except Exception as e:
        logging.exception(e)
    return False
```

#### do_handle_task
do_handle_task é€»è¾‘æ¯”è¾ƒé•¿ï¼Œæˆ‘ä»¬æ”¾åœ¨ä¸‹ä¸€èŠ‚å•ç‹¬è®²è§£ã€‚

## 4. æ³¨å†Œä¿¡å·é‡
```python
async def main():
    settings.init_settings()
    print_rag_settings()
    if sys.platform != "win32":
        # å¦‚æœä¸æ˜¯ Windows ç³»ç»Ÿï¼Œåˆ™æ³¨å†Œä¸¤ä¸ªè‡ªå®šä¹‰ä¿¡å·å¤„ç†å‡½æ•°
        # SIGUSR1 -> å¯åŠ¨ tracemalloc å¹¶ç”Ÿæˆå†…å­˜å¿«ç…§
        # SIGUSR2 -> åœæ­¢ tracemalloc
        signal.signal(signal.SIGUSR1, start_tracemalloc_and_snapshot)
        signal.signal(signal.SIGUSR2, stop_tracemalloc)

    # æ³¨å†Œç³»ç»Ÿä¿¡å·å¤„ç†å‡½æ•°
    # SIGINT  -> ä¸€èˆ¬æ˜¯ Ctrl+C ç»ˆæ­¢ç¨‹åº
    # SIGTERM -> kill å‘½ä»¤å‘é€çš„ç»ˆæ­¢ä¿¡å·
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
```

task_executor åœ¨å¯åŠ¨æ—¶æ³¨å†Œäº†å‡ ä¸ªä¿¡å·é‡:
1. `signal.SIGINT`, `signal.SIGTERM`: è¿›ç¨‹é€€å‡ºçš„ä¿¡å·é‡å¤„ç†
2. `signal.SIGUSR1`, `signal.SIGUSR2`: è°ƒè¯•ç”¨ä¿¡å·é‡ï¼Œç”¨äºç”Ÿæˆå†…å­˜å¿«ç…§

### 4.1 signal_handler
signal_handler è§¦å‘ä»»åŠ¡é€€å‡º:
1. è§¦å‘ stop_event äº‹ä»¶ï¼Œé€šçŸ¥ä»»åŠ¡æ‰§è¡Œå™¨é€€å‡ºï¼Œnursery ä¼šç­‰å¾…æ‰€æœ‰åç¨‹é€€å‡º
2. ç­‰å¾… 1s æ‰§è¡Œ `sys.exit(0)`

```python
def signal_handler(sig, frame):
    logging.info("Received interrupt signal, shutting down...")
    stop_event.set()
    time.sleep(1)
    sys.exit(0)
```


#### `sys.exit()` é€€å‡ºæµç¨‹
`sys.exit(code)` **å¹¶ä¸æ˜¯ç›´æ¥ç»ˆæ­¢è¿›ç¨‹**ã€‚* å®ƒä¼šæŠ›å‡ºä¸€ä¸ªç‰¹æ®Šçš„å¼‚å¸¸ï¼š`SystemExit(code)`ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼š

```python
import sys
sys.exit(0)

# ç­‰ä»·äºï¼š
raise SystemExit(0)
```

è°ƒç”¨ `sys.exit(0)` æ—¶ï¼Œå¤§è‡´æµç¨‹å¦‚ä¸‹ï¼š

1. **æŠ›å‡ºå¼‚å¸¸**

   * Python è§£é‡Šå™¨é‡åˆ° `sys.exit(0)` â†’ æŠ›å‡º `SystemExit(0)`ã€‚

2. **å¼‚å¸¸ä¼ æ’­**

   * å¦‚æœåœ¨å½“å‰è°ƒç”¨æ ˆé‡Œæ²¡æœ‰è¢« `try/except` æ•è·ï¼Œå¼‚å¸¸ä¼šå±‚å±‚å‘ä¸Šä¼ æ’­ã€‚
   * å¦‚æœä¸€ç›´æ²¡äººæ•è·ï¼Œæœ€ç»ˆä¼ é€’åˆ° Python ä¸»å¾ªç¯ï¼ˆ`PyErr_PrintEx`ï¼‰ã€‚

3. **è§£é‡Šå™¨å¤„ç† SystemExit**

   * è§£é‡Šå™¨æ£€æµ‹åˆ°å¼‚å¸¸ç±»å‹æ˜¯ `SystemExit`ï¼š

     * å¦‚æœ exit code æ˜¯æ•´æ•° â†’ ä½œä¸ºè¿›ç¨‹çš„é€€å‡ºç ï¼ˆ`0` è¡¨ç¤ºæˆåŠŸï¼‰ã€‚
     * å¦‚æœ exit code æ˜¯ `None` â†’ é»˜è®¤é€€å‡ºç ä¸º `0`ã€‚
     * å¦‚æœ exit code æ˜¯å­—ç¬¦ä¸² â†’ æ‰“å°åˆ° stderrï¼Œé€€å‡ºç ä¸º `1`ã€‚

4. **æ‰§è¡Œæ¸…ç†**

   * Python åœ¨é€€å‡ºå‰ä¼šæ‰§è¡Œæ¸…ç†å·¥ä½œï¼š

     * è°ƒç”¨ `atexit` æ³¨å†Œçš„å‡½æ•°
     * æ¸…ç†ç¼“å†²åŒºã€å…³é—­æ–‡ä»¶ã€æ¸…ç†åƒåœ¾å›æ”¶å™¨é‡Œçš„å¯¹è±¡
     * é‡Šæ”¾æ¨¡å—ã€æ‰§è¡Œ `__del__` æ–¹æ³•ç­‰

5. **è°ƒç”¨åº•å±‚ C å‡½æ•°**

   * æœ€ç»ˆè°ƒç”¨ C å±‚çš„ `Py_Exit(status)`ï¼Œå†…éƒ¨ä¼šè½¬è°ƒåˆ° `os._exit(status)`ï¼ŒçœŸæ­£è®©æ“ä½œç³»ç»Ÿç»“æŸè¿›ç¨‹ã€‚


6. **`os._exit()` vs `sys.exit()`**

   * `os._exit()`ï¼šç«‹å³è®©è¿›ç¨‹é€€å‡ºï¼Œä¸åšä»»ä½•æ¸…ç†ï¼Œä¸æ‰§è¡Œ `finally` / `atexit` / ç¼“å†²åˆ·æ–°ã€‚
   * `sys.exit()`ï¼šå…ˆæŠ›å¼‚å¸¸ï¼Œå…è®¸æ¸…ç†ï¼Œä¼˜é›…é€€å‡ºã€‚


### 4.2 tracemalloc
start_tracemalloc_and_snapshotã€stop_tracemalloc ç”¨åˆ°äº† tracemallocã€‚

`tracemalloc` æ˜¯ Python çš„å†…å­˜åˆ†æå·¥å…·:
1. ç›‘æ§ç¨‹åºä¸­ **å†…å­˜åˆ†é…çš„æƒ…å†µ**
2. æ‰¾åˆ° **å†…å­˜æ³„æ¼** æˆ– **å¼‚å¸¸å¢é•¿çš„å¯¹è±¡**
3. å®šä½ **å†…å­˜å ç”¨çƒ­ç‚¹**ï¼ˆå“ªæ®µä»£ç æˆ–å“ªä¸ªå‡½æ•°åˆ†é…äº†æœ€å¤šå†…å­˜ï¼‰

```python
# SIGUSR1 handler: start tracemalloc and take snapshot
def start_tracemalloc_and_snapshot(signum, frame):
    if not tracemalloc.is_tracing():
        logging.info("start tracemalloc")
        # å¼€å¯ tracemalloc å†…å­˜è·Ÿè¸ª
        tracemalloc.start()
    else:
        logging.info("tracemalloc is already running")

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    snapshot_file = f"snapshot_{timestamp}.trace"
    snapshot_file = os.path.abspath(os.path.join(get_project_base_directory(), "logs", f"{os.getpid()}_snapshot_{timestamp}.trace"))
    # ç”Ÿæˆå†…å­˜å¿«ç…§
    snapshot = tracemalloc.take_snapshot()
    snapshot.dump(snapshot_file)
    # æŸ¥çœ‹å½“å‰å†…å­˜åˆ†é…ç»Ÿè®¡
    current, peak = tracemalloc.get_traced_memory()
    if sys.platform == "win32":
        import  psutil
        process = psutil.Process()
        max_rss = process.memory_info().rss / 1024
    else:
        import resource
        max_rss = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    # rss æ˜¯å¸¸é©»å†…å­˜åŒ…æ‹¬å…±äº«åº“çš„å¤§å°
    logging.info(f"taken snapshot {snapshot_file}. max RSS={max_rss / 1000:.2f} MB, current memory usage: {current / 10**6:.2f} MB, Peak memory usage: {peak / 10**6:.2f} MB")

# SIGUSR2 handler: stop tracemalloc
def stop_tracemalloc(signum, frame):
    if tracemalloc.is_tracing():
        logging.info("stop tracemalloc")
        # åœæ­¢ tracemalloc å†…å­˜è·Ÿè¸ª
        tracemalloc.stop()
    else:
        logging.info("tracemalloc not running")

```


#### tracemalloc çš„ä¸»è¦åŠŸèƒ½

1. **å¼€å¯/å…³é—­è¿½è¸ª**

   ```python
   import tracemalloc
   tracemalloc.start()  # å¼€å§‹è¿½è¸ªå†…å­˜åˆ†é…
   tracemalloc.stop()   # åœæ­¢è¿½è¸ª
   ```

2. **æ‹æ‘„å¿«ç…§ï¼ˆSnapshotï¼‰**

   * è®°å½•ç¨‹åºæŸä¸€æ—¶åˆ»çš„å†…å­˜åˆ†é…çŠ¶æ€

   ```python
   snapshot = tracemalloc.take_snapshot()
   ```

3. **æ¯”è¾ƒå¿«ç…§ï¼Œåˆ†æå†…å­˜å˜åŒ–**

   ```python
   snapshot1 = tracemalloc.take_snapshot()
   # ... è¿è¡Œä¸€æ®µä»£ç 
   snapshot2 = tracemalloc.take_snapshot()

   top_stats = snapshot2.compare_to(snapshot1, 'lineno')
   for stat in top_stats[:10]:
       print(stat)
   ```

4. **æŸ¥çœ‹å½“å‰å†…å­˜åˆ†é…ç»Ÿè®¡**

   ```python
   import tracemalloc

   tracemalloc.start()
   # ... æ‰§è¡Œä»£ç 
   current, peak = tracemalloc.get_traced_memory()
   print(f"å½“å‰å†…å­˜: {current / 1024:.1f} KB, å³°å€¼: {peak / 1024:.1f} KB")
   ```

5. **é™åˆ¶è¿½è¸ªæ·±åº¦**

   ```python
   tracemalloc.start(10)  # åªè¿½è¸ªæœ€è¿‘ 10 å±‚è°ƒç”¨æ ˆ
   ```

#### SIGUSR1ã€SIGUSR2 è§¦å‘

```bash
kill -SIGUSR1 <pid>
kill -SIGUSR2 <pid>
```

## 5. åˆå§‹åŒ–é…ç½®
### 5.1 é…ç½®è¯»å–
init_settings æ‰§è¡Œåˆå§‹åŒ–é…ç½®ã€‚é¦–å…ˆé…ç½®çš„è¯»å–ä½äº `api/utils/__init__.py`ã€‚
1. conf_realpath: ä¼šåœ¨é¡¹ç›®æ ¹ç›®å½•çš„ conf ç›®å½•ä¸‹æŸ¥æ‰¾é…ç½®æ–‡ä»¶ã€‚
2. ä¼˜å…ˆæŸ¥æ‰¾ `local.service_conf.yaml`
3. åœ¨æŸ¥æ‰¾ `service_conf.yaml`

```python
CONFIGS = read_config()
SERVICE_CONF = "service_conf.yaml"

def conf_realpath(conf_name):
    conf_path = f"conf/{conf_name}"
    return os.path.join(file_utils.get_project_base_directory(), conf_path)

def read_config(conf_name=SERVICE_CONF):
    local_config = {}
    local_path = conf_realpath(f'local.{conf_name}')

    # load local config file
    if os.path.exists(local_path):
        local_config = file_utils.load_yaml_conf(local_path)
        if not isinstance(local_config, dict):
            raise ValueError(f'Invalid config file: "{local_path}".')

    global_config_path = conf_realpath(conf_name)
    global_config = file_utils.load_yaml_conf(global_config_path)

    if not isinstance(global_config, dict):
        raise ValueError(f'Invalid config file: "{global_config_path}".')

    global_config.update(local_config)
    return global_config
```

### 5.2 é»˜è®¤é…ç½®
`conf/service_conf.yaml` æ˜¯é¡¹ç›®æä¾›çš„é»˜è®¤é…ç½®ï¼Œé…ç½®å¦‚ä¸‹ï¼ŒåŒ…æ‹¬:
1. ragflow: æœåŠ¡å¯åŠ¨é…ç½®
2. mysql: mysql æ•°æ®åº“é…ç½®
3. minio: MinIO æ˜¯ä¸€ä¸ª å¼€æºçš„å¯¹è±¡å­˜å‚¨ç³»ç»Ÿï¼Œå…¼å®¹ Amazon S3 API
4. es: elasticsearch æœåŠ¡é…ç½®
5. os: opensearch æœåŠ¡é…ç½®ï¼Œopensearch æ˜¯ es çš„å¼€æºç‰ˆæœ¬
6. infinity: å‘é‡æ•°æ®åº“ï¼ˆVector Databaseï¼‰
7. redis: redis é…ç½®
8. postgres: postgres é…ç½®ï¼Œä¸ mysql äºŒé€‰ä¸€
9. s3ï¼Œossï¼Œazure: äºšé©¬é€Šã€é˜¿é‡Œã€å¾®è½¯æä¾›çš„å¯¹è±¡å­˜å‚¨ï¼ˆObject Storageï¼‰æœåŠ¡
10. opendal:
    - opendal æ˜¯ä¸€ä¸ª å¼€æºçš„æ•°æ®è®¿é—®å±‚åº“ï¼ˆOpen Data Access Layerï¼‰
    - ç»Ÿä¸€æ¥å£ï¼šé€šè¿‡ä¸€ä¸ªç»Ÿä¸€çš„ APIï¼Œå°±èƒ½è®¿é—®å„ç§å­˜å‚¨æœåŠ¡ã€‚
    - å¤šç§å­˜å‚¨åç«¯ï¼šæ”¯æŒæœ¬åœ°æ–‡ä»¶ç³»ç»Ÿã€Amazon S3ã€Google Cloud Storageã€Azure Blobã€HDFSã€WebDAVã€HTTP(S) ç­‰ã€‚
11. user_default_llm: ä½¿ç”¨çš„å¤§æ¨¡å‹é…ç½®
12. oauth: OAuth è®¤è¯
13. authentication: API è®¿é—®å’Œ Web UI ç™»å½•çš„èº«ä»½è®¤è¯
14. permission: æƒé™æ§åˆ¶ï¼Œç®¡ç†ç”¨æˆ·å¯¹èµ„æºçš„è®¿é—®æƒé™ã€‚
14. smtp: é‚®ç®±é…ç½®


```yaml
ragflow:
  host: 0.0.0.0
  http_port: 9380
mysql:
  name: 'rag_flow'
  user: 'root'
  password: 'infini_rag_flow'
  host: 'localhost'
  port: 5455
  max_connections: 900
  stale_timeout: 300
  max_allowed_packet: 1073741824
minio:
  user: 'rag_flow'
  password: 'infini_rag_flow'
  host: 'localhost:9000'
es:
  hosts: 'http://localhost:1200'
  username: 'elastic'
  password: 'infini_rag_flow'
os:
  hosts: 'http://localhost:1201'
  username: 'admin'
  password: 'infini_rag_flow_OS_01'
infinity:
  uri: 'localhost:23817'
  db_name: 'default_db'
redis:
  db: 1
  password: 'infini_rag_flow'
  host: 'localhost:6379'
# postgres:
#   name: 'rag_flow'
#   user: 'rag_flow'
#   password: 'infini_rag_flow'
#   host: 'postgres'
#   port: 5432
#   max_connections: 100
#   stale_timeout: 30
# s3:
#   access_key: 'access_key'
#   secret_key: 'secret_key'
#   region: 'region'
# oss:
#   access_key: 'access_key'
#   secret_key: 'secret_key'
#   endpoint_url: 'http://oss-cn-hangzhou.aliyuncs.com'
#   region: 'cn-hangzhou'
#   bucket: 'bucket_name'
# azure:
#   auth_type: 'sas'
#   container_url: 'container_url'
#   sas_token: 'sas_token'
# azure:
#   auth_type: 'spn'
#   account_url: 'account_url'
#   client_id: 'client_id'
#   secret: 'secret'
#   tenant_id: 'tenant_id'
#   container_name: 'container_name'
# The OSS object storage uses the MySQL configuration above by default. If you need to switch to antoher object storage service, please uncomment and configure the following parameters.
# opendal:
#   scheme: 'mysql'  # Storage type, such as s3, oss, azure, etc.
#   config:
#     oss_table: 'opendal_storage'
# user_default_llm:
#   factory: 'BAAI'
#   api_key: 'backup'
#   base_url: 'backup_base_url'
#   default_models:
#     chat_model:
#       name: 'qwen2.5-7b-instruct'
#       factory: 'xxxx'
#       api_key: 'xxxx'
#       base_url: 'https://api.xx.com'
#     embedding_model:
#       name: 'bge-m3'
#     rerank_model: 'bge-reranker-v2'
#     asr_model:
#       model: 'whisper-large-v3' # alias of name
#     image2text_model: ''
# oauth:
#   oauth2:
#     display_name: "OAuth2"
#     client_id: "your_client_id"
#     client_secret: "your_client_secret"
#     authorization_url: "https://your-oauth-provider.com/oauth/authorize"
#     token_url: "https://your-oauth-provider.com/oauth/token"
#     userinfo_url: "https://your-oauth-provider.com/oauth/userinfo"
#     redirect_uri: "https://your-app.com/v1/user/oauth/callback/oauth2"
#   oidc:
#     display_name: "OIDC"
#     client_id: "your_client_id"
#     client_secret: "your_client_secret"
#     issuer: "https://your-oauth-provider.com/oidc"
#     scope: "openid email profile"
#     redirect_uri: "https://your-app.com/v1/user/oauth/callback/oidc"
#   github:
#     type: "github"
#     icon: "github"
#     display_name: "Github"
#     client_id: "your_client_id"
#     client_secret: "your_client_secret"
#     redirect_uri: "https://your-app.com/v1/user/oauth/callback/github"
# authentication:
#   client:
#     switch: false
#     http_app_key:
#     http_secret_key:
#   site:
#     switch: false
# permission:
#   switch: false
#   component: false
#   dataset: false
# smtp:
#   mail_server: ""
#   mail_port: 465
#   mail_use_ssl: true
#   mail_use_tls: false
#   mail_username: ""
#   mail_password: ""
#   mail_default_sender:
#     - "RAGFlow" # display name
#     - "" # sender email address
#   mail_frontend_url: "https://your-frontend.example.com"

```

### 5.3 åˆå§‹åŒ–é…ç½® 
é…ç½®åˆå§‹åŒ–ä½äº `api/settings.py`ï¼Œinit_settings å‡½æ•°ä»£ç æ¯”è¾ƒé•¿ï¼Œæˆ‘ä»¬åˆ†å—æ¥çœ‹

#### å¤§æ¨¡å‹é…ç½®
å¤§æ¨¡å‹é…ç½®å¯¹åº”çš„é…ç½®æ®µæ˜¯ user_default_llm:

```yaml
user_default_llm:
  factory: 'BAAI'
  api_key: 'backup'
  base_url: 'backup_base_url'
  default_models:
    chat_model:
      name: 'qwen2.5-7b-instruct'
      factory: 'xxxx'
      api_key: 'xxxx'
      base_url: 'https://api.xx.com'
    embedding_model:
      name: 'bge-m3'
    rerank_model: 'bge-reranker-v2'
    asr_model:
      model: 'whisper-large-v3' # alias of name
    image2text_model: ''
```

```python
BUILTIN_EMBEDDING_MODELS = ["BAAI/bge-large-zh-v1.5@BAAI", "maidalun1020/bce-embedding-base_v1@Youdao"]

def init_settings():
    global LLM, LLM_FACTORY, LLM_BASE_URL, LIGHTEN, DATABASE_TYPE, DATABASE, FACTORY_LLM_INFOS, REGISTER_ENABLED
    LIGHTEN = int(os.environ.get("LIGHTEN", "0"))
    # åŠ è½½ä¸åŒå‚å•†æä¾›çš„å¤§æ¨¡å‹å…ƒæ•°æ®ä¿¡æ¯ï¼Œä½äº conf/llm_factories.json
    try:
        with open(os.path.join(get_project_base_directory(), "conf", "llm_factories.json"), "r") as f:
            FACTORY_LLM_INFOS = json.load(f)["factory_llm_infos"]
    except Exception:
        FACTORY_LLM_INFOS = []
    # è¯»å– user_default_llm é…ç½®æ®µ
    LLM = get_base_config("user_default_llm", {}) or {}
    LLM_DEFAULT_MODELS = LLM.get("default_models", {}) or {}
    LLM_FACTORY = LLM.get("factory", "") or ""
    LLM_BASE_URL = LLM.get("base_url", "") or ""

    global CHAT_MDL, EMBEDDING_MDL, RERANK_MDL, ASR_MDL, IMAGE2TEXT_MDL
    global CHAT_CFG, EMBEDDING_CFG, RERANK_CFG, ASR_CFG, IMAGE2TEXT_CFG
    if not LIGHTEN:
        EMBEDDING_MDL = BUILTIN_EMBEDDING_MODELS[0]

    # åŠ è½½ default_models ä¸åŒæ¨¡å‹çš„é…ç½®
    chat_entry = _parse_model_entry(LLM_DEFAULT_MODELS.get("chat_model", CHAT_MDL))
    embedding_entry = _parse_model_entry(LLM_DEFAULT_MODELS.get("embedding_model", EMBEDDING_MDL))
    rerank_entry = _parse_model_entry(LLM_DEFAULT_MODELS.get("rerank_model", RERANK_MDL))
    asr_entry = _parse_model_entry(LLM_DEFAULT_MODELS.get("asr_model", ASR_MDL))
    image2text_entry = _parse_model_entry(LLM_DEFAULT_MODELS.get("image2text_model", IMAGE2TEXT_MDL))

    # ä½¿ç”¨ user_default_llm ä¸‹çš„æ ¹é…ç½®ï¼Œä½œä¸ºé»˜è®¤é…ç½®åˆå¹¶æ¨¡å‹é…ç½®
    global API_KEY, PARSERS, HOST_IP, HOST_PORT, SECRET_KEY
    API_KEY = LLM.get("api_key")
    PARSERS = LLM.get(
        "parsers", "naive:General,qa:Q&A,resume:Resume,manual:Manual,table:Table,paper:Paper,book:Book,laws:Laws,presentation:Presentation,picture:Picture,one:One,audio:Audio,email:Email,tag:Tag"
    )
    CHAT_CFG = _resolve_per_model_config(chat_entry, LLM_FACTORY, API_KEY, LLM_BASE_URL)
    EMBEDDING_CFG = _resolve_per_model_config(embedding_entry, LLM_FACTORY, API_KEY, LLM_BASE_URL)
    RERANK_CFG = _resolve_per_model_config(rerank_entry, LLM_FACTORY, API_KEY, LLM_BASE_URL)
    ASR_CFG = _resolve_per_model_config(asr_entry, LLM_FACTORY, API_KEY, LLM_BASE_URL)
    IMAGE2TEXT_CFG = _resolve_per_model_config(image2text_entry, LLM_FACTORY, API_KEY, LLM_BASE_URL)

    # ç›¸å½“äºæŒ‰ç…§ 1. default_modelsã€2. ä»£ç ä¸­é…ç½®çš„å¸¸é‡ã€3. user_default_llm æ ¹ä¸‹é»˜è®¤é…ç½®çš„ä¼˜å…ˆçº§ï¼Œè·å–æ¨¡å‹é…ç½®
    CHAT_MDL = CHAT_CFG.get("model", "") or ""
    EMBEDDING_MDL = EMBEDDING_CFG.get("model", "") or ""
    RERANK_MDL = RERANK_CFG.get("model", "") or ""
    ASR_MDL = ASR_CFG.get("model", "") or ""
    IMAGE2TEXT_MDL = IMAGE2TEXT_CFG.get("model", "") or ""
```

#### æ•°æ®åº“é…ç½®
æ•°æ®åº“é…ç½®ï¼Œä¸»è¦:
1. mysql/postgresql çš„é€‰æ‹©
2. es/opensearch çš„é€‰æ‹©
3. retriever çš„åˆå§‹åŒ–

```python
def init_settings():
    # mysql/postgresql çš„é€‰æ‹©
    DATABASE_TYPE = os.getenv("DB_TYPE", "mysql")
    DATABASE = decrypt_database_config(name=DATABASE_TYPE)
    
    global DOC_ENGINE, docStoreConn, retrievaler, kg_retrievaler
    DOC_ENGINE = os.environ.get("DOC_ENGINE", "elasticsearch")
    # DOC_ENGINE = os.environ.get('DOC_ENGINE', "opensearch")
    lower_case_doc_engine = DOC_ENGINE.lower()
    if lower_case_doc_engine == "elasticsearch":
        docStoreConn = rag.utils.es_conn.ESConnection()
    elif lower_case_doc_engine == "infinity":
        docStoreConn = rag.utils.infinity_conn.InfinityConnection()
    elif lower_case_doc_engine == "opensearch":
        docStoreConn = rag.utils.opensearch_conn.OSConnection()
    else:
        raise Exception(f"Not supported doc engine: {DOC_ENGINE}")

    retrievaler = search.Dealer(docStoreConn)
    from graphrag import search as kg_search

    kg_retrievaler = kg_search.KGSearch(docStoreConn)
```

#### å…¶ä»–é…ç½®é¡¹

```python
def init_settings():
    # æœåŠ¡å¯åŠ¨é…ç½®
    try:
        REGISTER_ENABLED = int(os.environ.get("REGISTER_ENABLED", "1"))
    except Exception:
        pass

    global API_KEY, PARSERS, HOST_IP, HOST_PORT, SECRET_KEY
    
    HOST_IP = get_base_config(RAG_FLOW_SERVICE_NAME, {}).get("host", "127.0.0.1")
    HOST_PORT = get_base_config(RAG_FLOW_SERVICE_NAME, {}).get("http_port")

    SECRET_KEY = get_or_create_secret_key()

    # å®¢æˆ·ç«¯è®¤è¯é…ç½®
    global AUTHENTICATION_CONF, CLIENT_AUTHENTICATION, HTTP_APP_KEY, GITHUB_OAUTH, FEISHU_OAUTH, OAUTH_CONFIG
    # authentication
    AUTHENTICATION_CONF = get_base_config("authentication", {})

    CLIENT_AUTHENTICATION = AUTHENTICATION_CONF.get("client", {}).get("switch", False)
    HTTP_APP_KEY = AUTHENTICATION_CONF.get("client", {}).get("http_app_key")
    GITHUB_OAUTH = get_base_config("oauth", {}).get("github")
    FEISHU_OAUTH = get_base_config("oauth", {}).get("feishu")

    OAUTH_CONFIG = get_base_config("oauth", {})

    if int(os.environ.get("SANDBOX_ENABLED", "0")):
        global SANDBOX_HOST
        SANDBOX_HOST = os.environ.get("SANDBOX_HOST", "sandbox-executor-manager")

    # é‚®ç®±æœåŠ¡é…ç½®
    global SMTP_CONF, MAIL_SERVER, MAIL_PORT, MAIL_USE_SSL, MAIL_USE_TLS
    global MAIL_USERNAME, MAIL_PASSWORD, MAIL_DEFAULT_SENDER, MAIL_FRONTEND_URL
    SMTP_CONF = get_base_config("smtp", {})

    MAIL_SERVER = SMTP_CONF.get("mail_server", "")
    MAIL_PORT = SMTP_CONF.get("mail_port", 000)
    MAIL_USE_SSL = SMTP_CONF.get("mail_use_ssl", True)
    MAIL_USE_TLS = SMTP_CONF.get("mail_use_tls", False)
    MAIL_USERNAME = SMTP_CONF.get("mail_username", "")
    MAIL_PASSWORD = SMTP_CONF.get("mail_password", "")
    mail_default_sender = SMTP_CONF.get("mail_default_sender", [])
    if mail_default_sender and len(mail_default_sender) >= 2:
        MAIL_DEFAULT_SENDER = (mail_default_sender[0], mail_default_sender[1])
    MAIL_FRONTEND_URL = SMTP_CONF.get("mail_frontend_url", "")
```