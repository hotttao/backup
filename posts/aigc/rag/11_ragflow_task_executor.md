---
weight: 1
title: "RagFlow Task Executor"
date: 2025-08-20T10:00:00+08:00
lastmod: 2025-08-20T10:00:00+08:00
draft: false
author: "å®‹æ¶›"
authorLink: "https://hotttao.github.io/"
description: "RagFlow Task Executor"
featuredImage: 

tags: ["RAG", "RagFlow"]
categories: ["langchain"]

lightgallery: true

toc:
  auto: false
---

å‰é¢æˆ‘ä»¬ä»‹ç»äº† RagFlow çš„æ¡†æ¶ï¼Œå¯åŠ¨æµç¨‹ï¼Œè¿™ä¸€èŠ‚æˆ‘ä»¬æ¥è¯¦ç»†ä»‹ç» RagFlow çš„æ ¸å¿ƒè¿›ç¨‹ Task Executorã€‚

## 1. Task Executor
task executor å¯åŠ¨çš„å…¥å£åœ¨ `rag/svr/task_executor.py`ï¼Œå¯åŠ¨è¿‡ç¨‹æœ‰å¦‚ä¸‹æ ¸å¿ƒæ­¥éª¤:
1. åˆå§‹åŒ–é…ç½®
2. æ³¨å†Œä¿¡å·é‡
3. ä½¿ç”¨ trio ç®¡ç†å¼‚æ­¥ä»»åŠ¡

åœ¨æˆ‘ä»¬è¯¦ç»†ä»‹ç»æ¯ä¸€æ­¥çš„ä½¿ç”¨ä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆæ¥ç®€å•äº†è§£ä¸€ä¸‹ trioã€‚

```python
async def main():
    settings.init_settings()
    print_rag_settings()
    if sys.platform != "win32":
        # å¦‚æœä¸æ˜¯ Windows ç³»ç»Ÿï¼Œåˆ™æ³¨å†Œä¸¤ä¸ªè‡ªå®šä¹‰ä¿¡å·å¤„ç†å‡½æ•°
        # SIGUSR1 -> å¯åŠ¨ tracemalloc å¹¶ç”Ÿæˆå†…å­˜å¿«ç…§
        # SIGUSR2 -> åœæ­¢ tracemalloc
        signal.signal(signal.SIGUSR1, start_tracemalloc_and_snapshot)
        signal.signal(signal.SIGUSR2, stop_tracemalloc)

    # ä»ç¯å¢ƒå˜é‡ TRACE_MALLOC_ENABLED ä¸­è¯»å–æ˜¯å¦å¯ç”¨å†…å­˜è·Ÿè¸ª (é»˜è®¤ 0ï¼Œå³ä¸å¼€å¯)
    TRACE_MALLOC_ENABLED = int(os.environ.get('TRACE_MALLOC_ENABLED', "0"))
    if TRACE_MALLOC_ENABLED:
        # å¦‚æœå¯ç”¨ï¼Œåˆ™ç«‹å³å¯åŠ¨ tracemalloc å¹¶ç”Ÿæˆä¸€æ¬¡å¿«ç…§
        start_tracemalloc_and_snapshot(None, None)

    # æ³¨å†Œç³»ç»Ÿä¿¡å·å¤„ç†å‡½æ•°
    # SIGINT  -> ä¸€èˆ¬æ˜¯ Ctrl+C ç»ˆæ­¢ç¨‹åº
    # SIGTERM -> kill å‘½ä»¤å‘é€çš„ç»ˆæ­¢ä¿¡å·
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)

    # ä½¿ç”¨ Trio å¼‚æ­¥åº“å¯åŠ¨å¹¶å‘ä»»åŠ¡
    async with trio.open_nursery() as nursery:
        # å¯åŠ¨ä¸€ä¸ªåå°ä»»åŠ¡ï¼šå‘¨æœŸæ€§ä¸ŠæŠ¥ç¨‹åºçŠ¶æ€
        nursery.start_soon(report_status)
        
        # ä¸»å¾ªç¯ï¼šåªè¦ stop_event æœªè¢«è§¦å‘ï¼Œå°±æŒç»­æ‰§è¡Œä»»åŠ¡
        while not stop_event.is_set():
            # ä»»åŠ¡è°ƒåº¦æ§åˆ¶ï¼šè·å–ä¸€ä¸ªä»»åŠ¡æ‰§è¡Œè®¸å¯ï¼ˆé˜²æ­¢ä»»åŠ¡è¿‡å¤šï¼‰
            await task_limiter.acquire()
            # å¯åŠ¨ä¸€ä¸ªæ–°çš„åå°ä»»åŠ¡ï¼štask_managerï¼ˆè´Ÿè´£å¤„ç†å…·ä½“ä»»åŠ¡ï¼‰
            nursery.start_soon(task_manager)

    # å¦‚æœèƒ½èµ°åˆ°è¿™é‡Œï¼Œè¯´æ˜é€»è¾‘å¼‚å¸¸ï¼ˆç†è®ºä¸Šä¸åº”è¯¥åˆ°è¾¾ï¼‰
    logging.error("BUG!!! You should not reach here!!!")

if __name__ == "__main__":
    # å¯ç”¨ faulthandlerï¼Œåœ¨ Python å´©æºƒæ—¶è‡ªåŠ¨æ‰“å° tracebackï¼Œæ–¹ä¾¿è°ƒè¯•
    faulthandler.enable()
    # åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿï¼Œä½¿ç”¨æŒ‡å®šçš„æ¶ˆè´¹è€…å
    init_root_logger(CONSUMER_NAME)
    # ä½¿ç”¨ Trio äº‹ä»¶å¾ªç¯è¿è¡Œ main åç¨‹
    trio.run(main)
```

## 2. Trio ç®€ä»‹

### 2.1 Trio æ˜¯ä»€ä¹ˆ

* **å®šä¹‰**ï¼šTrio æ˜¯ä¸€ä¸ªåŸºäº Python `async/await` è¯­æ³•çš„å¼‚æ­¥ I/O åº“ã€‚
* **è®¾è®¡ç†å¿µ**ï¼šæä¾› *ç®€å•*ã€*å¯é *ã€*ç¬¦åˆäººç±»ç›´è§‰* çš„å¹¶å‘æ¨¡å‹ã€‚
* **ç›®æ ‡**ï¼šè®©å¼€å‘è€…åœ¨å†™å¼‚æ­¥ä»£ç æ—¶ **åƒå†™åŒæ­¥ä»£ç ä¸€æ ·è‡ªç„¶**ï¼ŒåŒæ—¶ä¿æŒå¥å£®æ€§å’Œå¯ç»´æŠ¤æ€§ã€‚

Trio çš„ä½œè€…æ˜¯ Nathaniel J. Smithï¼Œä»–çš„ç†å¿µæ˜¯ï¼š

> â€œæ­£ç¡®çš„æŠ½è±¡æ¯”è¿½æ±‚æé™æ€§èƒ½æ›´é‡è¦ï¼ŒTrio è¦è®©å¹¶å‘ç¼–ç¨‹å˜å¾—ä¸å†ç—›è‹¦ã€‚â€

---

### 2.2 Trio çš„ä½œç”¨

Trio ä¸»è¦ç”¨äºï¼š

1. **é«˜å¹¶å‘ä»»åŠ¡ç®¡ç†**ï¼šæ¯”å¦‚åŒæ—¶å¤„ç†å¤§é‡ç½‘ç»œè¯·æ±‚ã€I/O æ“ä½œã€‚
2. **åå°ä»»åŠ¡è¿è¡Œ**ï¼šå¯åŠ¨/å–æ¶ˆä»»åŠ¡æ›´åŠ ç›´è§‚ã€‚
3. **å—æ§é€€å‡ºæœºåˆ¶**ï¼šä¿è¯ä»»åŠ¡å¯ä»¥ä¼˜é›…åœ°ç»“æŸï¼Œä¸ä¼šç•™â€œåƒµå°¸ä»»åŠ¡â€ã€‚
4. **è°ƒè¯•/ç›‘æ§**ï¼šTrio åœ¨é”™è¯¯ä¼ æ’­å’Œå¼‚å¸¸å¤„ç†ä¸Šè®¾è®¡å¾—æ¯” asyncio æ›´æ¸…æ™°ã€‚

---

### 2.3 Trio ç›¸æ¯”å…¶ä»–å¼‚æ­¥åº“çš„ä¼˜åŠ¿

| ç‰¹æ€§       | `asyncio`                     | `gevent`     | `curio`         | **Trio**                     |
| -------- | ----------------------------- | ------------ | --------------- | ---------------------------- |
| **å¹¶å‘æ¨¡å‹** | äº‹ä»¶å¾ªç¯ + ä»»åŠ¡è°ƒåº¦                   | åç¨‹ï¼ˆåŸºäºç»¿è‰²çº¿ç¨‹ï¼‰   | ç±»ä¼¼ asyncioï¼Œä½†æ›´ç®€æ´ | **ç»“æ„åŒ–å¹¶å‘ï¼ˆnurseryï¼‰**           |
| **é”™è¯¯å¤„ç†** | é”™è¯¯å¯èƒ½è¢«åæ‰ï¼Œéœ€è¦å°å¿ƒ                  | å¼‚å¸¸ä¼ æ’­ä¸è‡ªç„¶      | æ”¹è¿›äº†å¼‚å¸¸å¤„ç†         | **å¼‚å¸¸ä¼šè‡ªåŠ¨ä¼ æ’­ç»™çˆ¶ä»»åŠ¡ï¼Œé˜²æ­¢é—æ¼**         |
| **ä»»åŠ¡ç®¡ç†** | `create_task`/`gather` å®¹æ˜“æ³„éœ²ä»»åŠ¡ | åç¨‹è°ƒåº¦éšå¼ï¼Œä¸é€æ˜   | ä»»åŠ¡ç®¡ç†ç®€å•          | **æ˜¾å¼çš„ä»»åŠ¡ä½œç”¨åŸŸï¼ˆnurseryï¼‰ï¼Œé˜²æ­¢ä»»åŠ¡æ³„éœ²** |
| **æ˜“ç”¨æ€§**  | åŠŸèƒ½å¤šä½†å¤æ‚                        | è¯­æ³•çœ‹ä¼¼åŒæ­¥ï¼Œä½†è°ƒè¯•éš¾  | æ›´è½»é‡ï¼Œä½†ä¸æ´»è·ƒ        | **API ç®€æ´ç›´è§‚ï¼Œå¼‚å¸¸å¤„ç†æ›´å®‰å…¨**         |
| **ç”Ÿæ€**   | å†…ç½®åº“ï¼Œç”Ÿæ€æœ€å¼º                      | åœ¨ web æ¡†æ¶ä¸­ç”¨å¾—å¤š | åœæ›´è¶‹åŠ¿            | ç¤¾åŒºå°ä¼—ï¼Œä½†è¶Šæ¥è¶Šè¢«å…³æ³¨                 |

ğŸ‘‰ **æœ€å¤§äº®ç‚¹**ï¼š

* Trio æå‡ºäº† **â€œç»“æ„åŒ–å¹¶å‘ (Structured Concurrency)â€** çš„æ¦‚å¿µï¼š

  * ä»»åŠ¡å¿…é¡»è¿è¡Œåœ¨ä¸€ä¸ªæ˜ç¡®çš„ä½œç”¨åŸŸï¼ˆnurseryï¼‰é‡Œã€‚
  * å½“ä½œç”¨åŸŸé€€å‡ºæ—¶ï¼Œæ‰€æœ‰å­ä»»åŠ¡å¿…é¡»å®Œæˆæˆ–è¢«å–æ¶ˆã€‚
  * é¿å…äº† `asyncio.create_task` è¿™ç§â€œæ‚¬ç©ºä»»åŠ¡â€å¯¼è‡´çš„ bugã€‚

---

### 2.4 Trio çš„æ ¸å¿ƒç”¨æ³•

#### åŸºç¡€ï¼šè¿è¡Œä¸€ä¸ªå¼‚æ­¥å‡½æ•°

```python
import trio

async def hello():
    print("Hello, Trio!")

trio.run(hello)  # ç±»ä¼¼ asyncio.run
```

---

#### Nurseryï¼šç»“æ„åŒ–å¹¶å‘

Trio çš„æ ¸å¿ƒæ˜¯ `nursery`ï¼ˆä»»åŠ¡æ‰˜ç®¡åŒºï¼‰ï¼š

```python
import trio

async def child(name, delay):
    await trio.sleep(delay)
    print(f"{name} done")

async def main():
    async with trio.open_nursery() as nursery:
        # å¯åŠ¨ä¸¤ä¸ªå­ä»»åŠ¡ï¼Œå®ƒä»¬ç”± nursery ç®¡ç†
        nursery.start_soon(child, "task1", 2)
        nursery.start_soon(child, "task2", 1)
    # é€€å‡º with æ—¶ï¼Œä¿è¯æ‰€æœ‰å­ä»»åŠ¡å®Œæˆæˆ–è¢«å–æ¶ˆ
    print("All tasks finished")

trio.run(main)
```

âœ… **ä¼˜ç‚¹**ï¼š

* å­ä»»åŠ¡çš„ç”Ÿå‘½å‘¨æœŸå’Œ `with` å—ç»‘å®šï¼Œé˜²æ­¢ä»»åŠ¡æ³„éœ²ã€‚
* å¦‚æœä¸€ä¸ªå­ä»»åŠ¡æŠ¥é”™ï¼Œé”™è¯¯ä¼šä¼ é€’åˆ° nurseryï¼Œå¹¶å–æ¶ˆæ‰€æœ‰å…¶ä»–å­ä»»åŠ¡ã€‚

---

#### å¹¶å‘é€šä¿¡ï¼ˆé€šé“ï¼‰

Trio æä¾›å†…å»ºçš„ **é«˜æ•ˆé€šé“ (Channel)** æ¥åšä»»åŠ¡é—´é€šä¿¡ã€‚

```python
import trio

async def sender(send_channel):
    for i in range(3):
        await send_channel.send(i)
    send_channel.close()

async def receiver(receive_channel):
    async for value in receive_channel:
        print("Received:", value)

async def main():
    send_channel, receive_channel = trio.open_memory_channel(0)
    async with trio.open_nursery() as nursery:
        nursery.start_soon(sender, send_channel)
        nursery.start_soon(receiver, receive_channel)

trio.run(main)
```

---

#### å–æ¶ˆå’Œè¶…æ—¶æ§åˆ¶

Trio çš„å–æ¶ˆæœºåˆ¶éå¸¸ç›´è§‚ï¼Œç»“åˆ **CancelScope** ä½¿ç”¨ï¼š

```python
import trio

async def slow_task():
    await trio.sleep(5)
    print("Done")

async def main():
    with trio.move_on_after(2):  # è¶…è¿‡ 2 ç§’è‡ªåŠ¨å–æ¶ˆ
        await slow_task()
    print("Task was cancelled")

trio.run(main)
```

---

#### é”™è¯¯ä¼ æ’­

Trio çš„ nursery ä¼šè‡ªåŠ¨æŠŠå­ä»»åŠ¡é”™è¯¯ä¼ æ’­ç»™çˆ¶ä»»åŠ¡ï¼Œä¿è¯ä½ ä¸ä¼šé—æ¼å¼‚å¸¸ï¼š

```python
import trio

async def broken():
    raise RuntimeError("Boom!")

async def main():
    async with trio.open_nursery() as nursery:
        nursery.start_soon(broken)
        # è¿™é‡Œä¸ä¼šæ— é™æŒ‚èµ·ï¼Œè€Œæ˜¯ç«‹å³æ”¶åˆ°å¼‚å¸¸
    print("This will not run")

trio.run(main)  # ä¼šæŠ¥é”™ RuntimeError: Boom!
```

---

### 2.5 Trio çš„é€‚ç”¨åœºæ™¯

* éœ€è¦ **é«˜å¯é æ€§** çš„å¼‚æ­¥ä»»åŠ¡ç®¡ç†ï¼ˆæ¯”å¦‚æœåŠ¡ç«¯ã€åå° workerï¼‰
* å¯¹ **è°ƒè¯•ã€å¼‚å¸¸å¤„ç†ã€å®‰å…¨é€€å‡º** æœ‰è¦æ±‚çš„é¡¹ç›®
* ä¸æƒ³è¢« asyncio çš„â€œéšå¼ä»»åŠ¡â€å‘åˆ°
* å­¦ä¹ ç»“æ„åŒ–å¹¶å‘çš„æœ€ä½³é€‰æ‹©

---

ğŸ“Œ **ä¸€å¥è¯æ€»ç»“**ï¼š

> Trio çš„æ ¸å¿ƒä¼˜åŠ¿æ˜¯ **ç»“æ„åŒ–å¹¶å‘ + ç®€æ´ API + å¼ºå¼‚å¸¸å®‰å…¨**ï¼Œæ¯” asyncio æ›´ç›´è§‚ã€æ›´å®‰å…¨ï¼Œé€‚åˆéœ€è¦é•¿æœŸç»´æŠ¤ã€ç¨³å®šå¯é çš„å¼‚æ­¥åº”ç”¨ã€‚


## 3. Trio ç®¡ç†çš„ä»»åŠ¡
task scheduler ä¸­ä½¿ç”¨ trio å¯åŠ¨äº†å¦‚ä¸‹ä»»åŠ¡:
1. `report_status`: ä¸ŠæŠ¥ä»»åŠ¡æ‰§è¡ŒçŠ¶æ€
2. `task_manager`: ä» redis é˜Ÿåˆ—è·å– taskï¼Œæ‰§è¡Œ task çš„å¤„ç†

ragflow ä½¿ç”¨ redis ä½œä¸ºæ¶ˆæ¯ä¸­é—´ä»¶:
1. ä½¿ç”¨ stream é˜Ÿåˆ—æ¥ä¼ é€’ task
2. ä½¿ç”¨ zset æ”¶é›†å’Œç»Ÿè®¡ä»»åŠ¡çŠ¶æ€
3. ä½¿ç”¨ redis åˆ†å¸ƒå¼é”ï¼Œä¿è¯åªæœ‰ä¸€ä¸ªèŠ‚ç‚¹ï¼Œå®Œæˆè¿‡æœŸçŠ¶æ€çš„æ¸…ç†

### 3.1 redis é˜Ÿåˆ—
æˆ‘ä»¬å…ˆå›é¡¾ä¸€ä¸‹ redis streamã€zsetã€list çš„æ“ä½œ

#### åŸºäº List çš„é˜Ÿåˆ—æ“ä½œï¼ˆValkeyï¼‰

| Redis å‘½ä»¤                      | ä½œç”¨     | è¯´æ˜            | Valkey Redis client å¯¹åº”æ–¹æ³•     |
| ----------------------------- | ------ | ------------- | ---------------------------- |
| `LPUSH key value`             | å·¦ç«¯å…¥é˜Ÿ   | å°†å…ƒç´ æ’å…¥åˆ—è¡¨å·¦ç«¯ï¼ˆé˜Ÿå¤´ï¼‰ | `r.lpush(key, value)`        |
| `RPUSH key value`             | å³ç«¯å…¥é˜Ÿ   | å°†å…ƒç´ æ’å…¥åˆ—è¡¨å³ç«¯ï¼ˆé˜Ÿå°¾ï¼‰ | `r.rpush(key, value)`        |
| `LPOP key`                    | å·¦ç«¯å‡ºé˜Ÿ   | ç§»é™¤å¹¶è¿”å›å·¦ç«¯å…ƒç´      | `r.lpop(key)`                |
| `RPOP key`                    | å³ç«¯å‡ºé˜Ÿ   | ç§»é™¤å¹¶è¿”å›å³ç«¯å…ƒç´      | `r.rpop(key)`                |
| `BLPOP key [key ...] timeout` | é˜»å¡å·¦ç«¯å‡ºé˜Ÿ | é˜»å¡ç­‰å¾…å…ƒç´         | `r.blpop(keys, timeout)`     |
| `BRPOP key [key ...] timeout` | é˜»å¡å³ç«¯å‡ºé˜Ÿ | é˜»å¡ç­‰å¾…å…ƒç´         | `r.brpop(keys, timeout)`     |
| `LLEN key`                    | è·å–é•¿åº¦   | åˆ—è¡¨å…ƒç´ æ•°é‡        | `r.llen(key)`                |
| `LRANGE key start stop`       | è·å–èŒƒå›´   | æŸ¥çœ‹é˜Ÿåˆ—å†…å®¹        | `r.lrange(key, start, stop)` |

---

#### åŸºäº Stream çš„é˜Ÿåˆ—æ“ä½œï¼ˆValkeyï¼‰

| Redis å‘½ä»¤                                                     | ä½œç”¨        | è¯´æ˜            | Valkey Redis client å¯¹åº”æ–¹æ³•                                        |
| ------------------------------------------------------------ | --------- | ------------- | --------------------------------------------------------------- |
| `XADD key * field value`                                     | å†™å…¥æ¶ˆæ¯      | å‘ Stream è¿½åŠ æ¶ˆæ¯ | `r.xadd(key, message_dict)`                                     |
| `XREAD COUNT n STREAMS key id`                               | è¯»å–æ¶ˆæ¯      | ä»æŒ‡å®š ID ä¹‹åè¯»å–   | `r.xread(streams={key: id}, count=n)`                           |
| `XREAD BLOCK milliseconds STREAMS key id`                    | é˜»å¡è¯»å–      | é˜»å¡ç­‰å¾…æ–°æ¶ˆæ¯       | `r.xread(streams={key: id}, block=ms)`                          |
| `XGROUP CREATE key groupname id`                             | åˆ›å»ºæ¶ˆè´¹è€…ç»„    | ç”¨äºå¤šæ¶ˆè´¹è€…å…±äº«      | `r.xgroup_create(key, groupname, id)`                           |
| `XREADGROUP GROUP groupname consumer COUNT n STREAMS key id` | æ¶ˆè´¹æ¶ˆæ¯      | æ¶ˆè´¹è€…ç»„è¯»å–        | `r.xreadgroup(groupname, consumer, streams={key: id}, count=n)` |
| `XACK key groupname id [id ...]`                             | ç¡®è®¤æ¶ˆæ¯      | æ¶ˆè´¹å®Œæˆç¡®è®¤        | `r.xack(key, groupname, ids)`                                   |
| `XPENDING key groupname [start end count] [consumer]`        | æŸ¥çœ‹å¾…ç¡®è®¤     | æœªç¡®è®¤æ¶ˆæ¯çŠ¶æ€       | `r.xpending(key, groupname)`                                    |
| `XDEL key id [id ...]`                                       | åˆ é™¤æ¶ˆæ¯      | åˆ é™¤ Stream æ¶ˆæ¯  | `r.xdel(key, *ids)`                                             |
| `XTRIM key MAXLEN ~ n`                                       | ä¿®å‰ª Stream | ä¿æŒé•¿åº¦ä¸Šé™        | `r.xtrim(key, maxlen=n, approximate=True)`                      |

---

## 3. åŸºäº Sorted Set çš„å»¶è¿Ÿé˜Ÿåˆ—ï¼ˆValkeyï¼‰

| Redis å‘½ä»¤                             | ä½œç”¨      | è¯´æ˜            | Valkey Redis client å¯¹åº”æ–¹æ³•                          |
| ------------------------------------ | ------- | ------------- | ------------------------------------------------- |
| `ZADD key score member`              | æ·»åŠ ä»»åŠ¡    | score ä½œä¸ºæ‰§è¡Œæ—¶é—´æˆ³ | `r.zadd(key, {member: score})`                    |
| `ZRANGE key start stop [WITHSCORES]` | æŸ¥è¯¢ä»»åŠ¡    | è·å–åŒºé—´å†…ä»»åŠ¡       | `r.zrange(key, start, stop, withscores=True)`     |
| `ZRANGEBYSCORE key min max`          | è·å–å¯æ‰§è¡Œä»»åŠ¡ | score â‰¤ å½“å‰æ—¶é—´  | `r.zrangebyscore(key, min, max, withscores=True)` |
| `ZREM key member`                    | ç§»é™¤ä»»åŠ¡    | æ‰§è¡Œå®Œæˆååˆ é™¤       | `r.zrem(key, member)`                             |

---

#### æ€»ç»“

| é˜Ÿåˆ—ç±»å‹       | ä¼˜ç‚¹              | é€‚ç”¨åœºæ™¯       | Valkey client ç‰¹ç‚¹                     |
| ---------- | --------------- | ---------- | ------------------------------------ |
| List       | ç®€å•ã€ä½å»¶è¿Ÿã€é˜»å¡æ“ä½œ     | å•ç”Ÿäº§è€…-å•æ¶ˆè´¹è€…  | ç›´æ¥è°ƒç”¨ `lpush/rpush/lpop` ç­‰            |
| Stream     | æ¶ˆæ¯ç¡®è®¤ã€å¤šæ¶ˆè´¹è€…ç»„ã€å†å²è®°å½• | é«˜å¯é æ¶ˆæ¯é˜Ÿåˆ—    | æä¾›å®Œæ•´ `xadd/xread/xack/xgroup` æ–¹æ³•     |
| Sorted Set | å¯å»¶è¿Ÿã€æŒ‰ä¼˜å…ˆçº§æ¶ˆè´¹      | å»¶è¿Ÿä»»åŠ¡ã€ä¼˜å…ˆçº§é˜Ÿåˆ— | `zadd/zrangebyscore/zrem` æ”¯æŒæ—¶é—´/ä¼˜å…ˆçº§æ“ä½œ |



### 3.1 report_status

è¿™æ®µä»£ç çš„ä¸»è¦åŠŸèƒ½æ˜¯ï¼š

1. **å¿ƒè·³ä¸ŠæŠ¥**
   * ä½¿ç”¨ redis zset ä¿å­˜å¿ƒè·³ä¿¡æ¯
   * å‘¨æœŸæ€§æŠŠæ¶ˆè´¹è€…çŠ¶æ€ï¼ˆå¾…å¤„ç†ã€è½åã€å½“å‰ä»»åŠ¡ç­‰ï¼‰å†™å…¥ Redisï¼Œæ–¹ä¾¿ç›‘æ§ã€‚

2. **è¿‡æœŸæ•°æ®æ¸…ç†**
   * ä½¿ç”¨ RedisDistributedLock å»æŠ¥åªæœ‰ä¸€ä¸ªè¿›ç¨‹æ‰§è¡Œæ¸…ç†æ“ä½œ
   * åˆ é™¤è¶…è¿‡ 30 åˆ†é’Ÿæœªæ›´æ–°çš„å¿ƒè·³ä¿¡æ¯ã€‚
   * åˆ é™¤å·²ç»ä¸æ´»è·ƒçš„ä»»åŠ¡æ‰§è¡Œå™¨ï¼ˆåŸºäºåˆ†å¸ƒå¼é”ä¿è¯ä¸ä¼šå†²çªï¼‰ã€‚

```python
async def report_status():
    # å£°æ˜ä½¿ç”¨çš„ä¸€äº›å…¨å±€å˜é‡
    global CONSUMER_NAME, BOOT_AT, PENDING_TASKS, LAG_TASKS, DONE_TASKS, FAILED_TASKS

    # å°†å½“å‰æ¶ˆè´¹è€…åç§°åŠ å…¥ Redis çš„é›†åˆ(set) "TASKEXE"ï¼Œè¡¨ç¤ºè¿™æ˜¯æ´»è·ƒçš„ä»»åŠ¡æ‰§è¡Œå™¨ä¹‹ä¸€
    REDIS_CONN.sadd("TASKEXE", CONSUMER_NAME)

    # åˆ›å»ºä¸€ä¸ªåˆ†å¸ƒå¼é”ï¼Œç”¨äºåœ¨å¤šè¿›ç¨‹/å¤šæœºå™¨ç¯å¢ƒä¸‹æ¸…ç†è¿‡æœŸä»»åŠ¡æ‰§è¡Œå™¨
    redis_lock = RedisDistributedLock(
        "clean_task_executor", lock_value=CONSUMER_NAME, timeout=60
    )

    # ä¸»å¾ªç¯ï¼šæ¯éš”ä¸€æ®µæ—¶é—´ä¸ŠæŠ¥çŠ¶æ€
    while True:
        try:
            # è·å–å½“å‰æ—¶é—´
            now = datetime.now()

            # æŸ¥è¯¢æ¶ˆæ¯é˜Ÿåˆ—ä¸­è¯¥æ¶ˆè´¹è€…ç»„çš„çŠ¶æ€ä¿¡æ¯ï¼ˆpending å’Œ lagï¼‰
            group_info = REDIS_CONN.queue_info(get_svr_queue_name(0), SVR_CONSUMER_GROUP_NAME)
            if group_info is not None:
                PENDING_TASKS = int(group_info.get("pending", 0))  # å¾…å¤„ç†ä»»åŠ¡æ•°
                LAG_TASKS = int(group_info.get("lag", 0))          # æ¶ˆè´¹è½åä»»åŠ¡æ•°

            # å¤åˆ¶å½“å‰æ­£åœ¨å¤„ç†çš„ä»»åŠ¡åˆ—è¡¨ï¼Œé¿å…ä¿®æ”¹å…±äº«å¯¹è±¡
            current = copy.deepcopy(CURRENT_TASKS)

            # æ„å»ºå¿ƒè·³ä¿¡æ¯ï¼ˆJSONï¼‰ï¼ŒåŒ…å«æ¶ˆè´¹è€…çŠ¶æ€å’Œç»Ÿè®¡ä¿¡æ¯
            heartbeat = json.dumps({
                "name": CONSUMER_NAME,
                "now": now.astimezone().isoformat(timespec="milliseconds"),
                "boot_at": BOOT_AT,
                "pending": PENDING_TASKS,
                "lag": LAG_TASKS,
                "done": DONE_TASKS,
                "failed": FAILED_TASKS,
                "current": current,
            })

            # å°†å¿ƒè·³ä¿¡æ¯å­˜å…¥ Redis zsetï¼Œscore ä¸ºå½“å‰æ—¶é—´æˆ³
            REDIS_CONN.zadd(CONSUMER_NAME, heartbeat, now.timestamp())
            logging.info(f"{CONSUMER_NAME} reported heartbeat: {heartbeat}")

            # æ¸…ç†è¿‡æœŸå¿ƒè·³ä¿¡æ¯ï¼ˆè¶…è¿‡ 30 åˆ†é’Ÿæœªæ›´æ–°çš„è®°å½•ï¼‰
            expired = REDIS_CONN.zcount(CONSUMER_NAME, 0, now.timestamp() - 60 * 30)
            if expired > 0:
                REDIS_CONN.zpopmin(CONSUMER_NAME, expired)

            # æ¸…ç†è¿‡æœŸçš„ä»»åŠ¡æ‰§è¡Œå™¨
            # å…ˆå°è¯•è·å–åˆ†å¸ƒå¼é”ï¼Œä¿è¯åªæœ‰ä¸€ä¸ªæ¶ˆè´¹è€…åœ¨æ¸…ç†
            if redis_lock.acquire():
                # è·å–æ‰€æœ‰æ´»è·ƒçš„ä»»åŠ¡æ‰§è¡Œå™¨
                task_executors = REDIS_CONN.smembers("TASKEXE")
                for consumer_name in task_executors:
                    if consumer_name == CONSUMER_NAME:
                        continue  # è·³è¿‡è‡ªå·±
                    # é€šè¿‡è¶…æ—¶é—´å†…çš„å¿ƒè·³æ¬¡æ•°ï¼Œå¦‚æœä¸º 0ï¼Œè¡¨ç¤ºè¶…æ—¶æ—¶é—´å†…æ²¡ä¸ŠæŠ¥è¿‡å¿ƒè·³
                    expired = REDIS_CONN.zcount(
                        consumer_name,
                        now.timestamp() - WORKER_HEARTBEAT_TIMEOUT,  # è¶…æ—¶æ—¶é—´ä¹‹å‰
                        now.timestamp() + 10                         # å½“å‰æ—¶é—´ç¨å¾®å®½å®¹ä¸€ç‚¹
                    )
                    if expired == 0:
                        # å¦‚æœæ²¡æœ‰å¿ƒè·³ï¼Œè®¤ä¸ºè¯¥æ‰§è¡Œå™¨å·²è¿‡æœŸï¼Œåˆ é™¤è®°å½•
                        logging.info(f"{consumer_name} expired, removed")
                        REDIS_CONN.srem("TASKEXE", consumer_name)
                        REDIS_CONN.delete(consumer_name)

        except Exception:
            # æ•è·æ‰€æœ‰å¼‚å¸¸ï¼Œé˜²æ­¢åç¨‹é€€å‡ºï¼Œå¹¶æ‰“å°å¼‚å¸¸æ—¥å¿—
            logging.exception("report_status got exception")

        finally:
            # ç¡®ä¿é‡Šæ”¾åˆ†å¸ƒå¼é”
            redis_lock.release()

        # å¼‚æ­¥ä¼‘çœ  30 ç§’ï¼Œå‘¨æœŸæ€§ä¸ŠæŠ¥
        await trio.sleep(30)
```

redis åˆ†å¸ƒå¼é”å®ç°å¦‚ä¸‹:
1. acquire æ–¹æ³•ï¼Œæ‰§è¡Œå‰å…ˆå°è¯•é‡Šæ”¾é”ï¼Œæ˜¯ä¸ºäº†åˆ·æ–° ttlï¼Œå¹¶ä¸”å¯ä»¥è´Ÿè½½å‡è¡¡ä¹ˆï¼Ÿ

```python
class RedisDistributedLock:
    def __init__(self, lock_key, lock_value=None, timeout=10, blocking_timeout=1):
        self.lock_key = lock_key
        if lock_value:
            self.lock_value = lock_value
        else:
            self.lock_value = str(uuid.uuid4())
        self.timeout = timeout
        self.lock = Lock(REDIS_CONN.REDIS, lock_key, timeout=timeout, blocking_timeout=blocking_timeout)

    def acquire(self):
        REDIS_CONN.delete_if_equal(self.lock_key, self.lock_value)
        return self.lock.acquire(token=self.lock_value)

    async def spin_acquire(self):
        REDIS_CONN.delete_if_equal(self.lock_key, self.lock_value)
        while True:
            if self.lock.acquire(token=self.lock_value):
                break
            await trio.sleep(10)

    def release(self):
        REDIS_CONN.delete_if_equal(self.lock_key, self.lock_value)

```

### 3.2 task_manager
task_manager æ˜¯ä»»åŠ¡å¤„ç†è¿‡ç¨‹:
1. collect ç”¨äºä» redis stream é˜Ÿåˆ—ä¸­è·å–ä»»åŠ¡
2. do_handle_task æ‰§è¡Œ task
3. set_progress ç”¨äºæ›´æ–° task è¿›åº¦
4. redis_msg.ack(): æˆåŠŸå¤„ç†æ¶ˆæ¯ï¼Œæ¶ˆæ¯ç¡®è®¤

```python
async def handle_task():
    global DONE_TASKS, FAILED_TASKS
    redis_msg, task = await collect()
    if not task:
        await trio.sleep(5)
        return
    try:
        logging.info(f"handle_task begin for task {json.dumps(task)}")
        CURRENT_TASKS[task["id"]] = copy.deepcopy(task)
        await do_handle_task(task)
        DONE_TASKS += 1
        CURRENT_TASKS.pop(task["id"], None)
        logging.info(f"handle_task done for task {json.dumps(task)}")
    except Exception as e:
        FAILED_TASKS += 1
        CURRENT_TASKS.pop(task["id"], None)
        try:
            err_msg = str(e)
            while isinstance(e, exceptiongroup.ExceptionGroup):
                e = e.exceptions[0]
                err_msg += ' -- ' + str(e)
            # æ›´æ–° task è¿›åº¦
            set_progress(task["id"], prog=-1, msg=f"[Exception]: {err_msg}")
        except Exception:
            pass
        logging.exception(f"handle_task got exception for task {json.dumps(task)}")
    redis_msg.ack()

```

#### collect
task æ¶ˆè´¹ä½äº RedisDB çš„ get_unacked_iterator æ–¹æ³•ï¼Œè¿”å›ä¸€ä¸ªè¿­ä»£å™¨ï¼Œæ¯æ¬¡ yeild ä¸€ä¸ª taskã€‚

redis stream æ¶ˆæ¯é˜Ÿåˆ—ï¼Œæ¯”è¾ƒéš¾ç†è§£çš„æ˜¯ msg_id çš„å€¼ã€‚
1. PEL ä¸­çš„æ¶ˆæ¯ä¼šè®°å½•æ‰€å±çš„æ¶ˆè´¹è€…
2. msg_id ä¸º 0 æ—¶ï¼Œä¼šä» PEL ä¸­å¼€å§‹è¯»å–å±äºè‡ªå·±çš„æ¶ˆæ¯ï¼Œæˆ–è€…æ˜¯ æœª ack çš„æ–°æ¶ˆæ¯
3. æ­£å¸¸é€»è¾‘æ¯æ¬¡ msg_id=0 å³å¯ï¼Œä½†æ˜¯ task exector ä¼šå¯åŠ¨å¤šä¸ª task_manager, æ‰€ä»¥éœ€è¦ä¼ å…¥ msg_id ç¡®ä¿ä¸ä¼šé‡å¤æ¶ˆè´¹å…¶ä»– task_manager æ­£åœ¨å¤„ç†çš„æ¶ˆæ¯ã€‚ 

```python
class RedisDB:
    def queue_consumer(self, queue_name, group_name, consumer_name, msg_id=b">") -> RedisMsg:
        """https://redis.io/docs/latest/commands/xreadgroup/"""
        for _ in range(3):
            try:

                try:
                    group_info = self.REDIS.xinfo_groups(queue_name)
                    if not any(gi["name"] == group_name for gi in group_info):
                        # åˆ›å»ºé˜Ÿåˆ—å’Œæ¶ˆè´¹è€…ç»„
                        self.REDIS.xgroup_create(queue_name, group_name, id="0", mkstream=True)
                except redis.exceptions.ResponseError as e:
                    if "no such key" in str(e).lower():
                        self.REDIS.xgroup_create(queue_name, group_name, id="0", mkstream=True)
                    elif "busygroup" in str(e).lower():
                        logging.warning("Group already exists, continue.")
                        pass
                    else:
                        raise

                args = {
                    "groupname": group_name,
                    "consumername": consumer_name,
                    "count": 1,
                    "block": 5,
                    "streams": {queue_name: msg_id},
                }
                # è¯»å–æ¶ˆæ¯é˜Ÿåˆ—ä¸­çš„æ¶ˆæ¯, xreadgroup è¿”å›å€¼çš„ç±»å‹
                # [
                #     (
                #         "mystream",  # Stream åç§°
                #         [
                #             ("1692633600000-0", {"task": "A"})  # æ¶ˆæ¯åˆ—è¡¨ï¼Œæ•°é‡åŒ count
                #         ]
                #     )
                # ]
                messages = self.REDIS.xreadgroup(**args)
                if not messages:
                    return None
                stream, element_list = messages[0]
                if not element_list:
                    return None
                msg_id, payload = element_list[0]
                res = RedisMsg(self.REDIS, queue_name, group_name, msg_id, payload)
                return res
            except Exception as e:
                if str(e) == 'no such key':
                    pass
                else:
                    logging.exception(
                        "RedisDB.queue_consumer "
                        + str(queue_name)
                        + " got exception: "
                        + str(e)
                    )
                    self.__open__()
        return None

    def get_unacked_iterator(self, queue_names: list[str], group_name, consumer_name):
        try:
            for queue_name in queue_names:
                try:
                    # è·å–é˜Ÿåˆ—çš„æ¶ˆè´¹è€…ç»„ä¿¡æ¯
                    group_info = self.REDIS.xinfo_groups(queue_name)
                except Exception as e:
                    if str(e) == 'no such key':
                        logging.warning(f"RedisDB.get_unacked_iterator queue {queue_name} doesn't exist")
                        continue
                # æ¶ˆè´¹è€…ç»„ä¸å­˜åœ¨
                if not any(gi["name"] == group_name for gi in group_info):
                    logging.warning(f"RedisDB.get_unacked_iterator queue {queue_name} group {group_name} doesn't exist")
                    continue
                current_min = 0
                while True:
                    payload = self.queue_consumer(queue_name, group_name, consumer_name, current_min)
                    if not payload:
                        break
                    current_min = payload.get_msg_id()
                    logging.info(f"RedisDB.get_unacked_iterator {queue_name} {consumer_name} {current_min}")
                    yield payload
        except Exception:
            logging.exception(
                "RedisDB.get_unacked_iterator got exception: "
            )
            self.__open__()

```

ç†è§£äº†æ¶ˆæ¯çš„æ¶ˆè´¹é€»è¾‘ï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹ collect çš„å…·ä½“æ‰§è¡Œæµç¨‹:
1. UNACKED_ITERATOR å±äºå…¨å±€çš„è¿­ä»£å™¨ï¼Œå¯ä»¥ä¿è¯æ¶ˆæ¯æŒ‰é¡ºåºæ¶ˆè´¹
2. redis æ¶ˆæ¯ä¸­ï¼Œåªä¿å­˜äº† task_idï¼Œè¦è·å–ä»»åŠ¡è¯¦ç»†å‚æ•°ï¼Œéœ€è¦é€šè¿‡ `TaskService.get_task(msg["id"])`
2. collect æœ€ç»ˆè¿”å›redis_msg å’Œ task

```python
async def collect():
    global CONSUMER_NAME, DONE_TASKS, FAILED_TASKS
    global UNACKED_ITERATOR

    svr_queue_names = get_svr_queue_names()
    try:
        if not UNACKED_ITERATOR:
            UNACKED_ITERATOR = REDIS_CONN.get_unacked_iterator(svr_queue_names, SVR_CONSUMER_GROUP_NAME, CONSUMER_NAME)
        try:
            redis_msg = next(UNACKED_ITERATOR)
        except StopIteration:
            for svr_queue_name in svr_queue_names:
                redis_msg = REDIS_CONN.queue_consumer(svr_queue_name, SVR_CONSUMER_GROUP_NAME, CONSUMER_NAME)
                if redis_msg:
                    break
    except Exception:
        logging.exception("collect got exception")
        return None, None

    if not redis_msg:
        return None, None
    # msg æ¶ˆæ¯è´Ÿè½½
    msg = redis_msg.get_message()
    if not msg:
        logging.error(f"collect got empty message of {redis_msg.get_msg_id()}")
        redis_msg.ack()
        return None, None

    canceled = False
    # è·å–ä»»åŠ¡
    task = TaskService.get_task(msg["id"])
    if task:
        # åˆ¤æ–­ä»»åŠ¡æ˜¯å¦å·²ç»å–æ¶ˆ
        canceled = has_canceled(task["id"])
    if not task or canceled:
        state = "is unknown" if not task else "has been cancelled"
        FAILED_TASKS += 1
        logging.warning(f"collect task {msg['id']} {state}")
        redis_msg.ack()
        return None, None
    task["task_type"] = msg.get("task_type", "")
    return redis_msg, task

# ä»»åŠ¡å–æ¶ˆåˆ¤æ–­
def has_canceled(task_id):
    try:
        if REDIS_CONN.get(f"{task_id}-cancel"):
            return True
    except Exception as e:
        logging.exception(e)
    return False
```

#### do_handle_task
do_handle_task é€»è¾‘æ¯”è¾ƒé•¿ï¼Œæˆ‘ä»¬æ”¾åœ¨ä¸‹ä¸€èŠ‚å•ç‹¬è®²è§£ã€‚

## 4. æ³¨å†Œä¿¡å·é‡
```python
async def main():
    settings.init_settings()
    print_rag_settings()
    if sys.platform != "win32":
        # å¦‚æœä¸æ˜¯ Windows ç³»ç»Ÿï¼Œåˆ™æ³¨å†Œä¸¤ä¸ªè‡ªå®šä¹‰ä¿¡å·å¤„ç†å‡½æ•°
        # SIGUSR1 -> å¯åŠ¨ tracemalloc å¹¶ç”Ÿæˆå†…å­˜å¿«ç…§
        # SIGUSR2 -> åœæ­¢ tracemalloc
        signal.signal(signal.SIGUSR1, start_tracemalloc_and_snapshot)
        signal.signal(signal.SIGUSR2, stop_tracemalloc)

    # æ³¨å†Œç³»ç»Ÿä¿¡å·å¤„ç†å‡½æ•°
    # SIGINT  -> ä¸€èˆ¬æ˜¯ Ctrl+C ç»ˆæ­¢ç¨‹åº
    # SIGTERM -> kill å‘½ä»¤å‘é€çš„ç»ˆæ­¢ä¿¡å·
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
```

task_executor åœ¨å¯åŠ¨æ—¶æ³¨å†Œäº†å‡ ä¸ªä¿¡å·é‡:
1. `signal.SIGINT`, `signal.SIGTERM`: è¿›ç¨‹é€€å‡ºçš„ä¿¡å·é‡å¤„ç†
2. `signal.SIGUSR1`, `signal.SIGUSR2`: è°ƒè¯•ç”¨ä¿¡å·é‡ï¼Œç”¨äºç”Ÿæˆå†…å­˜å¿«ç…§

### 4.1 signal_handler
signal_handler è§¦å‘ä»»åŠ¡é€€å‡º:
1. è§¦å‘ stop_event äº‹ä»¶ï¼Œé€šçŸ¥ä»»åŠ¡æ‰§è¡Œå™¨é€€å‡ºï¼Œnursery ä¼šç­‰å¾…æ‰€æœ‰åç¨‹é€€å‡º
2. ç­‰å¾… 1s æ‰§è¡Œ `sys.exit(0)`

```python
def signal_handler(sig, frame):
    logging.info("Received interrupt signal, shutting down...")
    stop_event.set()
    time.sleep(1)
    sys.exit(0)
```


#### `sys.exit()` é€€å‡ºæµç¨‹
`sys.exit(code)` **å¹¶ä¸æ˜¯ç›´æ¥ç»ˆæ­¢è¿›ç¨‹**ã€‚* å®ƒä¼šæŠ›å‡ºä¸€ä¸ªç‰¹æ®Šçš„å¼‚å¸¸ï¼š`SystemExit(code)`ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼š

```python
import sys
sys.exit(0)

# ç­‰ä»·äºï¼š
raise SystemExit(0)
```

è°ƒç”¨ `sys.exit(0)` æ—¶ï¼Œå¤§è‡´æµç¨‹å¦‚ä¸‹ï¼š

1. **æŠ›å‡ºå¼‚å¸¸**

   * Python è§£é‡Šå™¨é‡åˆ° `sys.exit(0)` â†’ æŠ›å‡º `SystemExit(0)`ã€‚

2. **å¼‚å¸¸ä¼ æ’­**

   * å¦‚æœåœ¨å½“å‰è°ƒç”¨æ ˆé‡Œæ²¡æœ‰è¢« `try/except` æ•è·ï¼Œå¼‚å¸¸ä¼šå±‚å±‚å‘ä¸Šä¼ æ’­ã€‚
   * å¦‚æœä¸€ç›´æ²¡äººæ•è·ï¼Œæœ€ç»ˆä¼ é€’åˆ° Python ä¸»å¾ªç¯ï¼ˆ`PyErr_PrintEx`ï¼‰ã€‚

3. **è§£é‡Šå™¨å¤„ç† SystemExit**

   * è§£é‡Šå™¨æ£€æµ‹åˆ°å¼‚å¸¸ç±»å‹æ˜¯ `SystemExit`ï¼š

     * å¦‚æœ exit code æ˜¯æ•´æ•° â†’ ä½œä¸ºè¿›ç¨‹çš„é€€å‡ºç ï¼ˆ`0` è¡¨ç¤ºæˆåŠŸï¼‰ã€‚
     * å¦‚æœ exit code æ˜¯ `None` â†’ é»˜è®¤é€€å‡ºç ä¸º `0`ã€‚
     * å¦‚æœ exit code æ˜¯å­—ç¬¦ä¸² â†’ æ‰“å°åˆ° stderrï¼Œé€€å‡ºç ä¸º `1`ã€‚

4. **æ‰§è¡Œæ¸…ç†**

   * Python åœ¨é€€å‡ºå‰ä¼šæ‰§è¡Œæ¸…ç†å·¥ä½œï¼š

     * è°ƒç”¨ `atexit` æ³¨å†Œçš„å‡½æ•°
     * æ¸…ç†ç¼“å†²åŒºã€å…³é—­æ–‡ä»¶ã€æ¸…ç†åƒåœ¾å›æ”¶å™¨é‡Œçš„å¯¹è±¡
     * é‡Šæ”¾æ¨¡å—ã€æ‰§è¡Œ `__del__` æ–¹æ³•ç­‰

5. **è°ƒç”¨åº•å±‚ C å‡½æ•°**

   * æœ€ç»ˆè°ƒç”¨ C å±‚çš„ `Py_Exit(status)`ï¼Œå†…éƒ¨ä¼šè½¬è°ƒåˆ° `os._exit(status)`ï¼ŒçœŸæ­£è®©æ“ä½œç³»ç»Ÿç»“æŸè¿›ç¨‹ã€‚


6. **`os._exit()` vs `sys.exit()`**

   * `os._exit()`ï¼šç«‹å³è®©è¿›ç¨‹é€€å‡ºï¼Œä¸åšä»»ä½•æ¸…ç†ï¼Œä¸æ‰§è¡Œ `finally` / `atexit` / ç¼“å†²åˆ·æ–°ã€‚
   * `sys.exit()`ï¼šå…ˆæŠ›å¼‚å¸¸ï¼Œå…è®¸æ¸…ç†ï¼Œä¼˜é›…é€€å‡ºã€‚


### 4.2 tracemalloc
start_tracemalloc_and_snapshotã€stop_tracemalloc ç”¨åˆ°äº† tracemallocã€‚

`tracemalloc` æ˜¯ Python çš„å†…å­˜åˆ†æå·¥å…·:
1. ç›‘æ§ç¨‹åºä¸­ **å†…å­˜åˆ†é…çš„æƒ…å†µ**
2. æ‰¾åˆ° **å†…å­˜æ³„æ¼** æˆ– **å¼‚å¸¸å¢é•¿çš„å¯¹è±¡**
3. å®šä½ **å†…å­˜å ç”¨çƒ­ç‚¹**ï¼ˆå“ªæ®µä»£ç æˆ–å“ªä¸ªå‡½æ•°åˆ†é…äº†æœ€å¤šå†…å­˜ï¼‰

```python
# SIGUSR1 handler: start tracemalloc and take snapshot
def start_tracemalloc_and_snapshot(signum, frame):
    if not tracemalloc.is_tracing():
        logging.info("start tracemalloc")
        # å¼€å¯ tracemalloc å†…å­˜è·Ÿè¸ª
        tracemalloc.start()
    else:
        logging.info("tracemalloc is already running")

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    snapshot_file = f"snapshot_{timestamp}.trace"
    snapshot_file = os.path.abspath(os.path.join(get_project_base_directory(), "logs", f"{os.getpid()}_snapshot_{timestamp}.trace"))
    # ç”Ÿæˆå†…å­˜å¿«ç…§
    snapshot = tracemalloc.take_snapshot()
    snapshot.dump(snapshot_file)
    # æŸ¥çœ‹å½“å‰å†…å­˜åˆ†é…ç»Ÿè®¡
    current, peak = tracemalloc.get_traced_memory()
    if sys.platform == "win32":
        import  psutil
        process = psutil.Process()
        max_rss = process.memory_info().rss / 1024
    else:
        import resource
        max_rss = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    # rss æ˜¯å¸¸é©»å†…å­˜åŒ…æ‹¬å…±äº«åº“çš„å¤§å°
    logging.info(f"taken snapshot {snapshot_file}. max RSS={max_rss / 1000:.2f} MB, current memory usage: {current / 10**6:.2f} MB, Peak memory usage: {peak / 10**6:.2f} MB")

# SIGUSR2 handler: stop tracemalloc
def stop_tracemalloc(signum, frame):
    if tracemalloc.is_tracing():
        logging.info("stop tracemalloc")
        # åœæ­¢ tracemalloc å†…å­˜è·Ÿè¸ª
        tracemalloc.stop()
    else:
        logging.info("tracemalloc not running")

```


#### tracemalloc çš„ä¸»è¦åŠŸèƒ½

1. **å¼€å¯/å…³é—­è¿½è¸ª**

   ```python
   import tracemalloc
   tracemalloc.start()  # å¼€å§‹è¿½è¸ªå†…å­˜åˆ†é…
   tracemalloc.stop()   # åœæ­¢è¿½è¸ª
   ```

2. **æ‹æ‘„å¿«ç…§ï¼ˆSnapshotï¼‰**

   * è®°å½•ç¨‹åºæŸä¸€æ—¶åˆ»çš„å†…å­˜åˆ†é…çŠ¶æ€

   ```python
   snapshot = tracemalloc.take_snapshot()
   ```

3. **æ¯”è¾ƒå¿«ç…§ï¼Œåˆ†æå†…å­˜å˜åŒ–**

   ```python
   snapshot1 = tracemalloc.take_snapshot()
   # ... è¿è¡Œä¸€æ®µä»£ç 
   snapshot2 = tracemalloc.take_snapshot()

   top_stats = snapshot2.compare_to(snapshot1, 'lineno')
   for stat in top_stats[:10]:
       print(stat)
   ```

4. **æŸ¥çœ‹å½“å‰å†…å­˜åˆ†é…ç»Ÿè®¡**

   ```python
   import tracemalloc

   tracemalloc.start()
   # ... æ‰§è¡Œä»£ç 
   current, peak = tracemalloc.get_traced_memory()
   print(f"å½“å‰å†…å­˜: {current / 1024:.1f} KB, å³°å€¼: {peak / 1024:.1f} KB")
   ```

5. **é™åˆ¶è¿½è¸ªæ·±åº¦**

   ```python
   tracemalloc.start(10)  # åªè¿½è¸ªæœ€è¿‘ 10 å±‚è°ƒç”¨æ ˆ
   ```

#### SIGUSR1ã€SIGUSR2 è§¦å‘

```bash
kill -SIGUSR1 <pid>
kill -SIGUSR2 <pid>
```