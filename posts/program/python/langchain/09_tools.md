---
weight: 1
title: "LangChain Tools"
date: 2025-07-23T13:00:00+08:00
lastmod: 2025-07-23T13:00:00+08:00
draft: false
author: "å®‹æ¶›"
authorLink: "https://hotttao.github.io/"
description: "langchain tools"
featuredImage: 

tags: ["langchain æºç "]
categories: ["langchain"]

lightgallery: true

toc:
  auto: false
---

## 1. Function Call

æé—®: èƒ½ä¸èƒ½æƒ³å†™ç»™æˆ‘ä»‹ç»ä¸€ä¸‹ OpenAI Function Calling 

å›žç­”: å½“ç„¶å¯ä»¥ï¼Œä¸‹é¢æˆ‘ä¼šä»Ž **åˆè¡·ã€è¯­æ³•ç»“æž„ã€å·¥ä½œæœºåˆ¶ã€ä»£ç ç¤ºä¾‹ã€è¿›é˜¶ç”¨æ³•å’Œä¸Ž LangChain ç»“åˆæ–¹å¼** å…­ä¸ªæ–¹é¢ï¼Œå…¨é¢ä»‹ç» **OpenAI Function Calling** çš„æœºåˆ¶ã€‚

---

### ðŸ§  1. ä»€ä¹ˆæ˜¯ OpenAI Function Callingï¼Ÿ

**Function Calling** æ˜¯ OpenAI åœ¨ 2023 å¹´å¼•å…¥çš„ä¸€é¡¹ç‰¹æ€§ï¼Œå…è®¸ä½ æŠŠæœ¬åœ°/æœåŠ¡å™¨ç«¯çš„å‡½æ•°æè¿°å‘é€ç»™ LLMï¼Œè®©æ¨¡åž‹ã€Œå†³å®šæ˜¯å¦è°ƒç”¨è¿™äº›å‡½æ•°ã€ï¼Œå¹¶è¾“å‡ºå‡½æ•°è°ƒç”¨æ‰€éœ€çš„å‚æ•°ã€‚

> æ¨¡åž‹ä¸ç›´æŽ¥è¾“å‡ºè‡ªç„¶è¯­è¨€ï¼Œè€Œæ˜¯è¿”å›žä¸€æ®µç»“æž„åŒ–çš„æ•°æ®ï¼š**æˆ‘è¦è°ƒç”¨å“ªä¸ªå‡½æ•°ã€ç”¨ä»€ä¹ˆå‚æ•°**ã€‚

è¿™è®©è¯­è¨€æ¨¡åž‹ï¼š

* èƒ½å¤Ÿå…·å¤‡ **å·¥å…·è°ƒç”¨èƒ½åŠ›**ï¼›
* å¯ä¸Ž **æ•°æ®åº“ã€APIã€æ’ä»¶** ç­‰è¿›è¡Œå¯¹æŽ¥ï¼›
* æ›´åƒä¸€ä¸ª **æ™ºèƒ½ç¼–æŽ’å™¨ï¼ˆOrchestratorï¼‰**ï¼Œè€Œä¸ä»…ä»…æ˜¯è¯­è¨€ç”Ÿæˆå™¨ã€‚

---

### ðŸ§© 2. Function Calling çš„å…¸åž‹åœºæ™¯

| åœºæ™¯       | æè¿°                                        |
| -------- | ----------------------------------------- |
| å¤©æ°”åŠ©æ‰‹     | ç”¨æˆ·é—®ã€ŒåŒ—äº¬å¤©æ°”ã€ï¼Œæ¨¡åž‹è°ƒç”¨ `get_weather(location=åŒ—äº¬)` |
| RAG æ£€ç´¢   | ç”¨æˆ·æé—®ï¼Œæ¨¡åž‹è°ƒç”¨ `search_docs(query=...)` æ£€ç´¢èµ„æ–™   |
| Agent å·¥å…· | æ¨¡åž‹è‡ªä¸»é€‰æ‹©å·¥å…·å¦‚ã€Œç¿»è¯‘ã€ã€Œè®¡ç®—å™¨ã€ã€Œçˆ¬è™«ã€                    |
| å¤šè½®å¯¹è¯è°ƒå–æ•°æ® | æ¨¡åž‹æ ¹æ®ä¸Šä¸‹æ–‡å†³å®šæ˜¯å¦å†æ¬¡è°ƒç”¨                           |

---

### ðŸ§ª 3. åŸºæœ¬è°ƒç”¨ç»“æž„

âœ… ä½ éœ€è¦å‘æ¨¡åž‹æä¾›å‡½æ•°å®šä¹‰ï¼ˆFunction Schemaï¼‰ï¼š

```json
{
  "name": "get_weather",
  "description": "èŽ·å–æŒ‡å®šåŸŽå¸‚çš„å¤©æ°”æƒ…å†µ",
  "parameters": {
    "type": "object",
    "properties": {
      "location": {
        "type": "string",
        "description": "è¦æŸ¥è¯¢çš„åŸŽå¸‚å"
      }
    },
    "required": ["location"]
  }
}
```

---

### ðŸ” 4. å·¥ä½œæœºåˆ¶ï¼šFunction Calling çš„ 3 æ­¥æµç¨‹

```
ç”¨æˆ·è¾“å…¥ --> æ¨¡åž‹å†³å®šè°ƒç”¨å“ªä¸ªå‡½æ•° --> ä½ æ‰§è¡Œå‡½æ•°å¹¶å°†ç»“æžœåé¦ˆç»™æ¨¡åž‹
```

æ­¥éª¤ 1ï¼šç”¨æˆ·æé—®

```python
messages = [{"role": "user", "content": "åŒ—äº¬ä»Šå¤©å¤©æ°”å¦‚ä½•ï¼Ÿ"}]
```

æ­¥éª¤ 2ï¼šæ¨¡åž‹è¿”å›žã€Œæˆ‘è¦è°ƒç”¨å‡½æ•°ã€

```json
{
  "role": "assistant",
  "content": null,
  "function_call": {
    "name": "get_weather",
    "arguments": "{ \"location\": \"åŒ—äº¬\" }"
  }
}
```

æ­¥éª¤ 3ï¼šä½ æ‹¿åˆ°å‡½æ•°åå’Œå‚æ•°ï¼Œè‡ªå·±è°ƒç”¨å¯¹åº”å‡½æ•°å¹¶æŠŠç»“æžœå‘å›žåŽ»

```python
function_result = get_weather("åŒ—äº¬")

# ç„¶åŽä½œä¸º FunctionMessage å‘ç»™æ¨¡åž‹ï¼š
messages.append({
  "role": "function",
  "name": "get_weather",
  "content": "{\"temperature\": \"30Â°C\", \"desc\": \"æ™´å¤©\"}"
})
```

æŽ¥ç€æ¨¡åž‹å°±å¯ä»¥åŸºäºŽè¿™ä¸ªç»“æžœç»™å‡ºè‡ªç„¶è¯­è¨€å›žå¤ã€‚

---

### ðŸ§‘â€ðŸ’» 5. Python ä½¿ç”¨ç¤ºä¾‹ï¼ˆopenai å®˜æ–¹ SDKï¼‰

```python
import openai

openai.api_key = "sk-..."

functions = [
    {
        "name": "get_weather",
        "description": "èŽ·å–åŸŽå¸‚å¤©æ°”",
        "parameters": {
            "type": "object",
            "properties": {
                "location": {"type": "string"}
            },
            "required": ["location"]
        }
    }
]

# ç¬¬ä¸€æ­¥ï¼šæ¨¡åž‹å†³å®šæ˜¯å¦è¦è°ƒç”¨å‡½æ•°
response = openai.ChatCompletion.create(
    model="gpt-4-0613",
    messages=[{"role": "user", "content": "åŒ—äº¬å¤©æ°”æ€Žä¹ˆæ ·ï¼Ÿ"}],
    functions=functions,
    function_call="auto"
)

# åˆ¤æ–­æ˜¯å¦æ˜¯ function_call
message = response.choices[0].message
if "function_call" in message:
    name = message.function_call.name
    arguments = json.loads(message.function_call.arguments)
    result = get_weather(**arguments)

    # ç¬¬äºŒæ­¥ï¼šè¿”å›žå‡½æ•°ç»“æžœï¼Œç»§ç»­äº¤äº’
    messages = [
        {"role": "user", "content": "åŒ—äº¬å¤©æ°”æ€Žä¹ˆæ ·ï¼Ÿ"},
        message,  # assistant function_call
        {"role": "function", "name": name, "content": json.dumps(result)}
    ]

    final_response = openai.ChatCompletion.create(
        model="gpt-4-0613",
        messages=messages
    )
    print(final_response.choices[0].message["content"])
```

---

### ðŸ§  6. LangChain ä¸­çš„ Function Calling æ”¯æŒ

LangChain ä¼šè‡ªåŠ¨å°†ä½ å®šä¹‰çš„ `@tool`, `@function` æˆ– `StructuredTool` è½¬æ¢æˆ Function Schemaï¼Œæ³¨å†Œç»™æ¨¡åž‹ä½¿ç”¨ã€‚

æ¯”å¦‚ä½ å†™ä¸€ä¸ªå·¥å…·ï¼š

```python
@tool
def get_weather(location: str) -> str:
    ...
```

LangChain ä¼šè‡ªåŠ¨è½¬æ¢ä¸ºï¼š

```json
{
  "name": "get_weather",
  "parameters": {
    "type": "object",
    "properties": {
      "location": {"type": "string"}
    },
    "required": ["location"]
  }
}
```

å¹¶åœ¨ chain æˆ– agent æ‰§è¡Œä¸­ï¼Œè‡ªåŠ¨ï¼š

* æ³¨å†Œ tool schemaï¼›
* æ£€æµ‹æ¨¡åž‹æ˜¯å¦è¿”å›žäº† function\_callï¼›
* è°ƒç”¨å‡½æ•°å¹¶åé¦ˆç»“æžœï¼›
* æž„é€ å®Œæ•´å¯¹è¯é“¾æ¡ã€‚

## 2. Tool Call
æé—®: èƒ½ä¸èƒ½ä»Ž åˆè¡·ã€è¯­æ³•ç»“æž„ã€å·¥ä½œæœºåˆ¶ã€ä»£ç ç¤ºä¾‹ã€è¿›é˜¶ç”¨æ³•å’Œä¸Ž LangChain ç»“åˆæ–¹å¼ å…­ä¸ªæ–¹é¢

å›žç­”: å½“ç„¶å¯ä»¥ï¼ä¸‹é¢æˆ‘å°†ä»Žä½ æŒ‡å®šçš„å…­ä¸ªæ–¹é¢ï¼Œç³»ç»Ÿåœ°è®²è§£ **OpenAI Tool Calling**ï¼š

---

### ðŸ§­ 1. åˆè¡·ï¼ˆä¸ºä»€ä¹ˆå¼•å…¥ Tool Callingï¼‰

OpenAI æŽ¨å‡º **Tool Calling** çš„åˆè¡·æ˜¯ä¸ºäº†è®©è¯­è¨€æ¨¡åž‹æ›´åƒä¸€ä¸ªâ€œæ™ºèƒ½ä½“ï¼ˆAgentï¼‰â€ï¼Œå…·å¤‡ä»¥ä¸‹èƒ½åŠ›ï¼š

* **ç»“æž„åŒ–è°ƒç”¨å¤–éƒ¨å·¥å…·**ï¼šæ¨¡åž‹å¯ä»¥ç†è§£ç”¨æˆ·æ„å›¾ï¼Œå¹¶ä¸»åŠ¨è°ƒç”¨ä½ æä¾›çš„å‡½æ•°ã€APIã€æ•°æ®åº“æŸ¥è¯¢å™¨ç­‰ã€‚
* **æ”¯æŒå¤šä¸ªå·¥å…·ååŒ**ï¼šä¸æ˜¯åªèƒ½è°ƒç”¨ä¸€ä¸ªå‡½æ•°ï¼Œè€Œæ˜¯å¯ä»¥åœ¨ä¸€æ¬¡å›žå¤ä¸­è°ƒç”¨å¤šä¸ªå·¥å…·ã€‚
* **ç»Ÿä¸€ Agent API**ï¼šæž„å»ºå¯¹è¯åž‹ Agent æ—¶ï¼Œå‡å°‘æŽ¥å£å·®å¼‚ã€‚
* **æé«˜å¯é æ€§ä¸Žå¯æŽ§æ€§**ï¼šæ¯”è‡ªç„¶è¯­è¨€æ›´å‡†ç¡®åœ°ä¼ å‚ã€æ›´å®¹æ˜“ç›‘æŽ§ã€æ›´åˆ©äºŽè°ƒè¯•ã€‚

ç®€è€Œè¨€ä¹‹ï¼šTool Calling è®© LLM ä¸æ­¢â€œè¯´è¯â€ï¼Œè€Œæ˜¯èƒ½\*\*â€œåŠ¨æ‰‹åšäº‹â€\*\*ï¼Œå…·å¤‡â€œå·¥å…·ä½¿ç”¨â€èƒ½åŠ›ï¼Œè¿›å…¥æ™ºèƒ½ä½“ï¼ˆAgentï¼‰æ—¶ä»£çš„æ ¸å¿ƒæ”¯æ’‘ã€‚

---

### ðŸ§± 2. è¯­æ³•ç»“æž„ï¼ˆPrompt æŽ¥å£ä¸Ž JSON Schemaï¼‰

âœ… è¯·æ±‚ç»“æž„ï¼ˆç²¾ç®€ç‰ˆï¼‰

```python
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[...],
    tools=[{...}],  # å®šä¹‰å·¥å…·
    tool_choice="auto"  # è‡ªåŠ¨é€‰æ‹©æ˜¯å¦è°ƒç”¨å·¥å…·
)
```

âœ… å·¥å…·å®šä¹‰ç»“æž„

æ¯ä¸ª Tool æ˜¯ä¸€ä¸ª JSON Schema æè¿°çš„å‡½æ•°ï¼š

```jsonc
{
  "type": "function",
  "function": {
    "name": "search_products",
    "description": "Search products from the catalog",
    "parameters": {
      "type": "object",
      "properties": {
        "keyword": { "type": "string" },
        "limit": { "type": "integer", "default": 10 }
      },
      "required": ["keyword"]
    }
  }
}
```

---

### âš™ï¸ 3. å·¥ä½œæœºåˆ¶ï¼ˆè°ƒç”¨æµç¨‹ï¼‰

ðŸŒ å…¨æµç¨‹æ­¥éª¤

1. **ä½ å®šä¹‰å¥½å·¥å…·åˆ—è¡¨ `tools`**ï¼Œä¼ ç»™ `chat.completions.create`
2. **ç”¨æˆ·æé—®**ï¼ˆå¦‚ï¼šâ€œä¸Šæµ·å¤©æ°”æ€Žä¹ˆæ ·ï¼Ÿâ€ï¼‰
3. **æ¨¡åž‹åˆ†æžæ„å›¾**ï¼Œå†³å®šæ˜¯å¦è°ƒç”¨å·¥å…·ï¼ˆæ ¹æ® `tool_choice`ï¼‰
4. å¦‚æžœæ¨¡åž‹è°ƒç”¨å·¥å…·ï¼Œè¿”å›žç»“æžœç±»ä¼¼ï¼š

```jsonc
{
  "tool_calls": [
    {
      "id": "call_abc123",
      "type": "function",
      "function": {
        "name": "get_weather",
        "arguments": "{ \"location\": \"Shanghai\" }"
      }
    }
  ]
}
```

5. **ä½ æ‹¿åˆ°å‚æ•° `arguments`ï¼Œæ‰§è¡Œå®žé™…å‡½æ•°è°ƒç”¨**
6. **æŠŠæ‰§è¡Œç»“æžœä½œä¸º Tool Message å›žä¼ **ï¼Œä¾‹å¦‚ï¼š

```python
{
  "tool_call_id": "call_abc123",
  "role": "tool",
  "content": "The weather in Shanghai is 30Â°C and sunny."
}
```

7. **æ¨¡åž‹åŸºäºŽå·¥å…·è¿”å›žå€¼ç”Ÿæˆæœ€ç»ˆå›žå¤**

---

### ðŸ’» 4. ä»£ç ç¤ºä¾‹ï¼ˆå®Œæ•´ end-to-endï¼‰

ðŸŽ¯ ç¤ºä¾‹ç›®æ ‡ï¼šæ¨¡åž‹è°ƒç”¨å·¥å…·èŽ·å–å¤©æ°”

```python
from openai import OpenAI

client = OpenAI()

# 1. å·¥å…·å®šä¹‰
tools = [
    {
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "Get current weather in a city",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {"type": "string", "description": "City name"},
                },
                "required": ["location"]
            }
        }
    }
]

# 2. ç”¨æˆ·è¾“å…¥
messages = [{"role": "user", "content": "ä»Šå¤©å¤©æ´¥å¤©æ°”æ€Žä¹ˆæ ·ï¼Ÿ"}]

# 3. è¯·æ±‚æ¨¡åž‹
response = client.chat.completions.create(
    model="gpt-4o",
    messages=messages,
    tools=tools,
    tool_choice="auto"
)

tool_calls = response.choices[0].message.tool_calls

# 4. æ‰§è¡ŒçœŸå®žå‡½æ•°ï¼ˆæ¨¡æ‹Ÿï¼‰
location = eval(tool_calls[0].function.arguments)["location"]
tool_result = f"{location} ä»Šå¤©å¤©æ°”æ™´ï¼Œ30Â°Cã€‚"

# 5. æž„é€  tool æ¶ˆæ¯å†æ¬¡å‘é€
messages.append({
    "tool_call_id": tool_calls[0].id,
    "role": "tool",
    "content": tool_result
})

# 6. å†æ¬¡è¯·æ±‚ï¼Œç”Ÿæˆæœ€ç»ˆå›žå¤
final = client.chat.completions.create(
    model="gpt-4o",
    messages=messages
)

print(final.choices[0].message.content)
```

---

### ðŸš€ 5. è¿›é˜¶ç”¨æ³•

âœ… æ”¯æŒå¤šä¸ªå·¥å…·è°ƒç”¨

```jsonc
"tool_calls": [
  { "function": { "name": "get_user_info", ... } },
  { "function": { "name": "get_order_status", ... } }
]
```

å¯ä»¥å¹¶è¡Œè°ƒç”¨å¤šä¸ªå‡½æ•°ï¼Œå†ç»Ÿä¸€ç”Ÿæˆå›žå¤ã€‚

---

âœ… æŽ§åˆ¶è°ƒç”¨æ–¹å¼

* `"tool_choice": "auto"`ï¼šæ¨¡åž‹è‡ªç”±å†³å®šæ˜¯å¦è°ƒç”¨å“ªä¸ªå·¥å…·
* `"tool_choice": {"type": "function", "function": {"name": "xxx"}}`ï¼šå¼ºåˆ¶è°ƒç”¨æŸä¸ªå‡½æ•°
* `"tool_choice": "none"`ï¼šä¸è®©æ¨¡åž‹è°ƒç”¨å·¥å…·


å¦‚æžœæ¨¡åž‹è§‰å¾—æ— éœ€è°ƒç”¨å·¥å…·ï¼Œå¯ä»¥ç›´æŽ¥è¿”å›žå›žç­”ï¼Œè¿™ç§çµæ´»æ€§å¾ˆé«˜ã€‚


### ðŸ”— 6. ä¸Ž LangChain çš„ç»“åˆæ–¹å¼

LangChain æž„å»ºäº†å°è£…å¥½çš„ `OpenAIToolAgent` ç±»åž‹ï¼ŒåŸºäºŽ Tool Calling é©±åŠ¨ Agent è°ƒç”¨ã€‚

âœ… æ ¸å¿ƒç±»

* `Tool`ï¼šå°è£…äº†ä¸€ä¸ªå‡½æ•°ï¼Œæä¾› nameã€descriptionã€arg\_schema
* `initialize_agent(..., agent_type="openai-tools")`ï¼šä½¿ç”¨ Tool Calling æž„é€  Agent

âœ… ç¤ºä¾‹ä»£ç 

```python
from langchain.agents import initialize_agent, Tool
from langchain.chat_models import ChatOpenAI

def get_weather(location: str) -> str:
    return f"{location} å¤©æ°”æ™´ï¼Œ30Â°C"

llm = ChatOpenAI(model="gpt-4o")

tools = [Tool.from_function(fn=get_weather)]

agent = initialize_agent(
    tools=tools,
    llm=llm,
    agent_type="openai-tools"
)

# ä½¿ç”¨ Agent
agent.invoke("è¯·é—®æˆéƒ½å¤©æ°”å¦‚ä½•ï¼Ÿ")
```

LangChain ä¼šè‡ªåŠ¨è§£æžæ¨¡åž‹çš„ `tool_calls`ï¼Œæ‰§è¡Œå¯¹åº”å‡½æ•°ï¼Œå¹¶ç»„è£…æ¶ˆæ¯ä¼ å›žæ¨¡åž‹ã€‚

---

### ðŸ§© æ€»ç»“

| éƒ¨åˆ†             | å†…å®¹                                                                 |
| -------------- | ------------------------------------------------------------------ |
| åˆè¡·             | è®©æ¨¡åž‹èƒ½ç»“æž„åŒ–è°ƒç”¨å·¥å…·ï¼Œèµ°å‘æ™ºèƒ½ä½“                                                  |
| è¯­æ³•ç»“æž„           | ä½¿ç”¨ `tools` æ³¨å†Œå‡½æ•°ï¼Œç”¨ JSON Schema æè¿°å‚æ•°                                 |
| å·¥ä½œæœºåˆ¶           | æ¨¡åž‹ -> Tool Calls -> ä½ æ‰§è¡Œ -> Tool Message -> æ¨¡åž‹å›žå¤                    |
| ä»£ç ç¤ºä¾‹           | å…¨æµç¨‹ end-to-end ç¤ºä¾‹                                                  |
| è¿›é˜¶ç”¨æ³•           | å¤šå·¥å…·è°ƒç”¨ã€è‡ªå®šä¹‰è°ƒç”¨ç­–ç•¥ã€åµŒå¥—å‚æ•°                                                 |
| ä¸Ž LangChain ç»“åˆ | ç”¨ `Tool` + `initialize_agent(..., agent_type="openai-tools")` å°è£…ä½¿ç”¨ |

## 3. AIMessage
Function Call å’Œ Tool Call çš„è°ƒç”¨ä¿¡æ¯éƒ½ä¿å­˜åœ¨ model è¿”å›žçš„ AIMessage ä¸­:

```python
class AIMessage(BaseMessage):
    """Message from an AI.

    AIMessage is returned from a chat model as a response to a prompt.

    This message represents the output of the model and consists of both
    the raw output as returned by the model together standardized fields
    (e.g., tool calls, usage metadata) added by the LangChain framework.
    """

    example: bool = False
    """Use to denote that a message is part of an example conversation.

    At the moment, this is ignored by most models. Usage is discouraged.
    """

    tool_calls: list[ToolCall] = []
    """If provided, tool calls associated with the message."""
    invalid_tool_calls: list[InvalidToolCall] = []
    """If provided, tool calls with parsing errors associated with the message."""
    usage_metadata: Optional[UsageMetadata] = None
    """If provided, usage metadata for a message, such as token counts.

    This is a standard representation of token usage that is consistent across models.
    """

    type: Literal["ai"] = "ai"
    """The type of the message (used for deserialization). Defaults to "ai"."""

```

| å­—æ®µå                  | ç±»åž‹                                | å«ä¹‰                                                               |
| -------------------- | --------------------------------- | ---------------------------------------------------------------- |
| `example`            | `bool`ï¼ˆé»˜è®¤å€¼ï¼š`False`ï¼‰               | æ ‡è¯†è¯¥æ¶ˆæ¯æ˜¯å¦æ˜¯â€œç¤ºä¾‹å¯¹è¯â€ä¸­çš„ä¸€éƒ¨åˆ†ã€‚è™½ç„¶ç›®å‰å¤§å¤šæ•°æ¨¡åž‹ä¼šå¿½ç•¥æ­¤å­—æ®µï¼Œä½†ç”¨äºŽè®­ç»ƒæ•°æ®ç¤ºä¾‹åœºæ™¯ï¼ŒLangChain ä¸æŽ¨èä½¿ç”¨ã€‚ |
| `tool_calls`         | `list[ToolCall]`ï¼ˆé»˜è®¤å€¼ï¼š`[]`ï¼‰        | è¡¨ç¤ºä¸Žè¯¥æ¶ˆæ¯å…³è”çš„æœ‰æ•ˆå·¥å…·è°ƒç”¨ä¿¡æ¯ï¼Œå¦‚å‡½æ•°è°ƒç”¨ã€APIè¯·æ±‚ç­‰ï¼Œç”¨äºŽæ”¯æŒ Tool/Function Callingã€‚       |
| `invalid_tool_calls` | `list[InvalidToolCall]`ï¼ˆé»˜è®¤å€¼ï¼š`[]`ï¼‰ | åŒ…å«è§£æžå¤±è´¥çš„å·¥å…·è°ƒç”¨ï¼ˆä¾‹å¦‚æ ¼å¼é”™è¯¯çš„ tool call ç»“æž„ï¼‰ï¼Œä¾¿äºŽè°ƒè¯•æˆ–å›žé€€é€»è¾‘å¤„ç†ã€‚                   |
| `usage_metadata`     | `Optional[UsageMetadata]`         | å¯é€‰å­—æ®µï¼Œè¡¨ç¤ºè¯¥æ¶ˆæ¯çš„èµ„æºä½¿ç”¨ä¿¡æ¯ï¼Œå¦‚ token æ•°ç­‰ã€‚è¿™ä¸ªç»“æž„åœ¨ä¸åŒæ¨¡åž‹é—´æ˜¯ç»Ÿä¸€çš„ï¼Œä¾¿äºŽç›‘æŽ§å’Œè®¡è´¹ã€‚             |
| `type`               | `Literal["ai"]`ï¼ˆé»˜è®¤å€¼ï¼š`"ai"`ï¼‰       | æ¶ˆæ¯ç±»åž‹æ ‡è¯†å­—æ®µï¼Œå›ºå®šä¸º `"ai"`ï¼Œç”¨äºŽåºåˆ—åŒ–/ååºåˆ—åŒ–å’Œæ¶ˆæ¯åˆ†ç±»åˆ¤æ–­ã€‚LangChain æ¡†æž¶å†…éƒ¨ä½¿ç”¨ã€‚          |


AIMessage ç»§æ‰¿è‡ª BaseMessageï¼ŒBaseMessage è¿˜æœ‰å¤šä¸ªå­—æ®µï¼Œå…·ä½“è¯·æŸ¥çœ‹å‰é¢çš„ BaseMessage å®šä¹‰ã€‚


### 3.1 ToolCall

```python
class ToolCall(TypedDict):
    """Represents a request to call a tool.

    Example:

        .. code-block:: python

            {
                "name": "foo",
                "args": {"a": 1},
                "id": "123"
            }

        This represents a request to call the tool named "foo" with arguments {"a": 1}
        and an identifier of "123".
    """

    name: str
    """The name of the tool to be called."""
    args: dict[str, Any]
    """The arguments to the tool call."""
    id: Optional[str]
    """An identifier associated with the tool call.

    An identifier is needed to associate a tool call request with a tool
    call result in events when multiple concurrent tool calls are made.
    """
    type: NotRequired[Literal["tool_call"]]


class InvalidToolCall(TypedDict):
    """Allowance for errors made by LLM.

    Here we add an `error` key to surface errors made during generation
    (e.g., invalid JSON arguments.)
    """

    name: Optional[str]
    """The name of the tool to be called."""
    args: Optional[str]
    """The arguments to the tool call."""
    id: Optional[str]
    """An identifier associated with the tool call."""
    error: Optional[str]
    """An error message associated with the tool call."""
    type: NotRequired[Literal["invalid_tool_call"]]
```

| å­—æ®µå    | ç±»åž‹                                  | æ˜¯å¦å¿…å¡« | å«ä¹‰                                          |
| ------ | ----------------------------------- | ---- | ------------------------------------------- |
| `name` | `str`                               | æ˜¯    | è¦è°ƒç”¨çš„å·¥å…·åç§°ï¼Œä¾‹å¦‚ `"get_weather"`ã€‚                |
| `args` | `dict[str, Any]`                    | æ˜¯    | å·¥å…·è°ƒç”¨æ‰€éœ€çš„å‚æ•°ï¼Œé”®å€¼å¯¹å½¢å¼ä¼ é€’ï¼Œä¾‹å¦‚ `{"city": "Beijing"}`ã€‚ |
| `id`   | `Optional[str]`                     | å¦    | å·¥å…·è°ƒç”¨çš„å”¯ä¸€æ ‡è¯†ç¬¦ï¼Œç”¨äºŽå°†è¯·æ±‚ä¸Žè¿”å›žç»“æžœå…³è”ï¼ˆç‰¹åˆ«æ˜¯åœ¨å¹¶å‘è°ƒç”¨å¤šä¸ªå·¥å…·æ—¶ï¼‰ã€‚     |
| `type` | `NotRequired[Literal["tool_call"]]` | å¦    | å¯é€‰å­—æ®µï¼Œç±»åž‹å›ºå®šä¸º `"tool_call"`ï¼Œç”¨äºŽæŒ‡æ˜Žè¯¥ç»“æž„æ˜¯ä¸€ä¸ªå·¥å…·è°ƒç”¨ã€‚    |



## 3. Tool Call Message And OutputParser
### 3.1 Message

ToolMessage æ˜¯ LangChain ä¸­ç”¨äºŽè¡¨ç¤ºå·¥å…·è°ƒç”¨ç»“æžœçš„ä¸“ç”¨æ¶ˆæ¯ç±»åž‹ï¼Œä¸»è¦ç”¨äºŽ OpenAI Tool Calling å·¥ä½œæµä¸­ï¼Œè¡¨ç¤ºæŸä¸ªå·¥å…·è°ƒç”¨å®ŒæˆåŽçš„ç»“æžœï¼Œå¹¶ä½œä¸ºä¸Šä¸‹æ–‡çš„ä¸€éƒ¨åˆ†ä¼ é€’å›žæ¨¡åž‹ã€‚


```python
class ToolMessage(BaseMessage, ToolOutputMixin):
    """Message for passing the result of executing a tool back to a model.

    ToolMessages contain the result of a tool invocation. Typically, the result
    is encoded inside the `content` field.
    """

    tool_call_id: str
    """Tool call that this message is responding to."""

    type: Literal["tool"] = "tool"
    """The type of the message (used for serialization). Defaults to "tool"."""

    artifact: Any = None
    """Artifact of the Tool execution which is not meant to be sent to the model.

    Should only be specified if it is different from the message content, e.g. if only
    a subset of the full tool output is being passed as message content but the full
    output is needed in other parts of the code.

    .. versionadded:: 0.2.17
    """

    status: Literal["success", "error"] = "success"
    """Status of the tool invocation.

    .. versionadded:: 0.2.24
    """

    additional_kwargs: dict = Field(default_factory=dict, repr=False)
    """Currently inherited from BaseMessage, but not used."""
    response_metadata: dict = Field(default_factory=dict, repr=False)
    """Currently inherited from BaseMessage, but not used."""
```

| å­—æ®µå                 | ç±»åž‹                            | å«ä¹‰è¯´æ˜Ž                                                                        |
| ------------------- | ----------------------------- | --------------------------------------------------------------------------- |
| `tool_call_id`      | `str`                         | è¡¨ç¤ºè¯¥æ¶ˆæ¯å¯¹åº”çš„ **å·¥å…·è°ƒç”¨ ID**ï¼Œç”¨äºŽå°†è¿”å›žç»“æžœä¸Žä¹‹å‰çš„ `tool_call` å»ºç«‹å…³è”ï¼ˆå¦‚ï¼šAIMessage ä¸­çš„ ToolCallï¼‰ã€‚ |
| `type`              | `Literal["tool"]`             | è¡¨ç¤ºæ¶ˆæ¯ç±»åž‹ä¸º `"tool"`ï¼Œç”¨äºŽåºåˆ—åŒ–æ ‡è¯†ã€‚å›ºå®šå€¼ã€‚                                               |
| `artifact`          | `Any`ï¼ˆå¯é€‰ï¼‰                     | å·¥å…·æ‰§è¡Œäº§ç”Ÿçš„å®Œæ•´è¾“å‡ºï¼Œ**ä¸ç”¨äºŽå‘é€ç»™æ¨¡åž‹**ï¼Œä»…ä¾›åŽç»­ä»£ç é€»è¾‘ä½¿ç”¨ã€‚ä¾‹å¦‚ï¼šcontent æ˜¯æ‘˜è¦ï¼Œartifact æ˜¯å…¨æ–‡ã€‚            |
| `status`            | `Literal["success", "error"]` | å·¥å…·è°ƒç”¨çš„æ‰§è¡ŒçŠ¶æ€ã€‚ç”¨äºŽæ ‡è®°æ˜¯å¦è°ƒç”¨æˆåŠŸï¼Œé»˜è®¤ä¸º `"success"`ã€‚                                       |
| `additional_kwargs` | `dict`ï¼ˆé»˜è®¤ç©ºå­—å…¸ï¼‰                 | æ¥è‡ª `BaseMessage`ï¼Œæš‚æ—¶æœªä½¿ç”¨ï¼Œä¿ç•™å­—æ®µã€‚                                                |
| `response_metadata` | `dict`ï¼ˆé»˜è®¤ç©ºå­—å…¸ï¼‰                 | åŒä¸Šï¼Œå½“å‰æœªè¢«å®žé™…ç”¨ä½œæ¶ˆæ¯å¤„ç†ï¼Œä»…ç”¨äºŽæ‰©å±•ã€‚                                                      |

### 3.2 OutputParser
![OutPut Parser ç±»å›¾](/images/langchain/output_parser.svg)

Tool ç›¸å…³çš„ Parserä¸»è¦æœ‰:
2. `JsonOutputToolsParser`
3. `PydanticToolsParser`: åœ¨ JsonOutputToolsParser çš„åŸºç¡€ä¸Šæ·»åŠ äº†å‚æ•°æ ¡éªŒ

#### JsonOutputToolsParser
```python
class JsonOutputToolsParser(BaseCumulativeTransformOutputParser[Any]):
    """Parse tools from OpenAI response."""

    strict: bool = False
    """Whether to allow non-JSON-compliant strings.

    See: https://docs.python.org/3/library/json.html#encoders-and-decoders

    Useful when the parsed output may include unicode characters or new lines.
    """
    return_id: bool = False
    """Whether to return the tool call id."""
    first_tool_only: bool = False
    """Whether to return only the first tool call.

    If False, the result will be a list of tool calls, or an empty list
    if no tool calls are found.

    If true, and multiple tool calls are found, only the first one will be returned,
    and the other tool calls will be ignored.
    If no tool calls are found, None will be returned.
    """

    def parse_result(self, result: list[Generation], *, partial: bool = False) -> Any:
        """Parse the result of an LLM call to a list of tool calls.

        Args:
            result: The result of the LLM call.
            partial: Whether to parse partial JSON.
                If True, the output will be a JSON object containing
                all the keys that have been returned so far.
                If False, the output will be the full JSON object.
                Default is False.

        Returns:
            The parsed tool calls.

        Raises:
            OutputParserException: If the output is not valid JSON.
        """
        generation = result[0]
        if not isinstance(generation, ChatGeneration):
            msg = "This output parser can only be used with a chat generation."
            raise OutputParserException(msg)
        message = generation.message
        if isinstance(message, AIMessage) and message.tool_calls:
            tool_calls = [dict(tc) for tc in message.tool_calls]
            for tool_call in tool_calls:
                if not self.return_id:
                    _ = tool_call.pop("id")
        else:
            try:
                raw_tool_calls = copy.deepcopy(message.additional_kwargs["tool_calls"])
            except KeyError:
                return []
            tool_calls = parse_tool_calls(
                raw_tool_calls,
                partial=partial,
                strict=self.strict,
                return_id=self.return_id,
            )
        # for backwards compatibility
        for tc in tool_calls:
            tc["type"] = tc.pop("name")

        if self.first_tool_only:
            return tool_calls[0] if tool_calls else None
        return tool_calls


def parse_tool_call(
    raw_tool_call: dict[str, Any],
    *,
    partial: bool = False,
    strict: bool = False,
    return_id: bool = True,
) -> Optional[dict[str, Any]]:
    """Parse a single tool call.

    Args:
        raw_tool_call: The raw tool call to parse.
        partial: Whether to parse partial JSON. Default is False.
        strict: Whether to allow non-JSON-compliant strings.
            Default is False.
        return_id: Whether to return the tool call id. Default is True.

    Returns:
        The parsed tool call.

    Raises:
        OutputParserException: If the tool call is not valid JSON.
    """
    if "function" not in raw_tool_call:
        return None
    if partial:
        try:
            function_args = parse_partial_json(
                raw_tool_call["function"]["arguments"], strict=strict
            )
        except (JSONDecodeError, TypeError):  # None args raise TypeError
            return None
    else:
        try:
            function_args = json.loads(
                raw_tool_call["function"]["arguments"], strict=strict
            )
        except JSONDecodeError as e:
            msg = (
                f"Function {raw_tool_call['function']['name']} arguments:\n\n"
                f"{raw_tool_call['function']['arguments']}\n\nare not valid JSON. "
                f"Received JSONDecodeError {e}"
            )
            raise OutputParserException(msg) from e
    parsed = {
        "name": raw_tool_call["function"]["name"] or "",
        "args": function_args or {},
    }
    if return_id:
        parsed["id"] = raw_tool_call.get("id")
        parsed = create_tool_call(**parsed)  # type: ignore[assignment,arg-type]
    return parsed

```

#### PydanticToolsParser

```python
TypeBaseModel = type[BaseModel]

class PydanticToolsParser(JsonOutputToolsParser):
    """Parse tools from OpenAI response."""

    tools: Annotated[list[TypeBaseModel], SkipValidation()]
    """The tools to parse."""

    # TODO: Support more granular streaming of objects. Currently only streams once all
    # Pydantic object fields are present.
    def parse_result(self, result: list[Generation], *, partial: bool = False) -> Any:
        """Parse the result of an LLM call to a list of Pydantic objects.

        Args:
            result: The result of the LLM call.
            partial: Whether to parse partial JSON.
                If True, the output will be a JSON object containing
                all the keys that have been returned so far.
                If False, the output will be the full JSON object.
                Default is False.

        Returns:
            The parsed Pydantic objects.

        Raises:
            OutputParserException: If the output is not valid JSON.
        """
        json_results = super().parse_result(result, partial=partial)
        if not json_results:
            return None if self.first_tool_only else []

        json_results = [json_results] if self.first_tool_only else json_results
        name_dict = {tool.__name__: tool for tool in self.tools}
        pydantic_objects = []
        for res in json_results:
            if not isinstance(res["args"], dict):
                if partial:
                    continue
                msg = (
                    f"Tool arguments must be specified as a dict, received: "
                    f"{res['args']}"
                )
                raise ValueError(msg)
            try:
                # res["type"] è®°å½•çš„æ˜¯è°ƒç”¨çš„å“ªä¸ªå‡½æ•°
                pydantic_objects.append(name_dict[res["type"]](**res["args"]))
            except (ValidationError, ValueError):
                if partial:
                    continue
                has_max_tokens_stop_reason = any(
                    generation.message.response_metadata.get("stop_reason")
                    == "max_tokens"
                    for generation in result
                    if isinstance(generation, ChatGeneration)
                )
                if has_max_tokens_stop_reason:
                    logger.exception(_MAX_TOKENS_ERROR)
                raise
        if self.first_tool_only:
            return pydantic_objects[0] if pydantic_objects else None
        return pydantic_objects
```

ä¸‹é¢æ˜¯ PydanticOutputFunctionsParser çš„ä½¿ç”¨ç¤ºä¾‹ã€‚éœ€è¦æ³¨æ„çš„ PydanticToolsParser ä¸­è¾“å…¥çš„ Model å¿…é¡»æ˜¯ä¸Ž Tool å¯¹åº”çš„ args_schemaï¼Œè¿™æ · `name_dict[res["type"]](**res["args"])` çš„ç´¢å¼•å…³ç³»æ‰èƒ½æ»¡è¶³ã€‚

```python
from audioop import mul
from langchain_core.output_parsers.openai_tools import PydanticToolsParser
from langchain_core.messages import AIMessage, ToolCall
from langchain_core.outputs import ChatGeneration
from langchain_core.tools import tool
from pydantic import BaseModel


# 1. å®šä¹‰å·¥å…·è¾“å…¥æ¨¡åž‹
class MultiplyInput(BaseModel):
    x: int
    y: int


class AddInput(BaseModel):
    a: int
    b: int


# 2. ä½¿ç”¨ @tool è£…é¥°å‡½æ•°
@tool
def multiply_tool(input: MultiplyInput) -> str:
    """è®¡ç®—ä¹˜æ³•"""
    return str(input.x * input.y)


@tool
def add_tool(input: AddInput) -> str:
    """è®¡ç®—åŠ æ³•"""
    return str(input.a + input.b)


# âœ… 3. ä»Ž StructuredTool æ‹¿åˆ°ç»‘å®šåŽçš„ Pydantic æ¨¡åž‹ç±»
multiply_model = multiply_tool.args_schema
add_model = add_tool.args_schema
print(multiply_model.__name__, multiply_model.model_json_schema())

# 4. æ¨¡æ‹Ÿ LLM è¿”å›ž tool è°ƒç”¨ç»“æžœ
fake_tool_call = ToolCall(
    # æ³¨æ„è¿™é‡Œçš„ args 
    name="multiply_tool", args={"input": {"x": 6, "y": 7}}, id="tool_call_1"
)
ai_message = AIMessage(content="", tool_calls=[fake_tool_call])
generation = ChatGeneration(message=ai_message)

# âœ… 5. ä½¿ç”¨ PydanticToolsParserï¼ˆä¼ å…¥çš„æ˜¯ Pydantic æ¨¡åž‹ç±»ï¼‰
parser = PydanticToolsParser(tools=[multiply_model, add_model])
parsed = parser.parse_result([generation])

# 6. è¾“å‡ºç»“æž„åŒ–ç»“æžœ
print("âœ… è§£æžåŽçš„ Pydantic å¯¹è±¡:")
print(parsed)

```


## 4. Function Call Message And OutputParser
### 4.1 Message
FunctionMessage ä¸“é—¨ç”¨äºŽå°è£… OpenAI Function Calling æˆ– Tool Calling ä¸­å‡½æ•°è¿”å›žç»“æžœçš„æ¶ˆæ¯ã€‚

```python
class FunctionMessage(BaseMessage):
    """Message for passing the result of executing a tool back to a model.

    FunctionMessage are an older version of the ToolMessage schema, and
    do not contain the tool_call_id field.

    The tool_call_id field is used to associate the tool call request with the
    tool call response. This is useful in situations where a chat model is able
    to request multiple tool calls in parallel.
    """

    name: str
    """The name of the function that was executed."""

    type: Literal["function"] = "function"
    """The type of the message (used for serialization). Defaults to "function"."""
```


```python
from langchain_core.messages import AIMessage, FunctionMessage, HumanMessage

history = [
    HumanMessage(content="ç»™æˆ‘æŽ¨èä¸€ä¸ªåŒ—äº¬çš„æ™¯ç‚¹"),
    AIMessage(content=None, tool_calls=[{
        "name": "get_beijing_attraction",
        "args": {"type": "æ–‡åŒ–ç±»"}
    }]),
    FunctionMessage(name="get_beijing_attraction", content="é¢å’Œå›­"),
]

```

### 4.2 OutputParser 

![OutPut Parser ç±»å›¾](/images/langchain/output_parser.svg)

Function Call æœ‰å¾ˆå¤šçš„ Parser ä½†æ˜¯æ ¸å¿ƒæ˜¯:
1. JsonOutputFunctionsParser
2. PydanticOutputFunctionsParser

è§£æžé€»è¾‘æ˜¯è¿™æ ·çš„:
1. ä»Ž model è¿”å›žçš„ message ä¸­è¿”å›ž function_callï¼š `function_call = message.additional_kwargs["function_call"]`
2. å°† function_call è§£æžä¸º Python å­—å…¸ï¼š `function_call = json.loads(function_call)`
3. ä»Ž function_call ä¸­èŽ·å– name å’Œ arguments
4. æœ‰æ²¡æœ‰ Pydantic æ¨¡åž‹ï¼Ÿå¦‚æžœæœ‰ï¼Œå°±ç”¨ Pydantic æ¨¡åž‹è§£æž arguments

#### JsonOutputFunctionsParser
```python
class JsonOutputFunctionsParser(BaseCumulativeTransformOutputParser[Any]):
    """Parse an output as the Json object."""

    strict: bool = False
    """Whether to allow non-JSON-compliant strings.

    See: https://docs.python.org/3/library/json.html#encoders-and-decoders

    Useful when the parsed output may include unicode characters or new lines.
    """

    args_only: bool = True
    """Whether to only return the arguments to the function call."""

    @property
    def _type(self) -> str:
        return "json_functions"

    @override
    def _diff(self, prev: Optional[Any], next: Any) -> Any:
        return jsonpatch.make_patch(prev, next).patch

    def parse_result(self, result: list[Generation], *, partial: bool = False) -> Any:
        """Parse the result of an LLM call to a JSON object.

        Args:
            result: The result of the LLM call.
            partial: Whether to parse partial JSON objects. Default is False.

        Returns:
            The parsed JSON object.

        Raises:
            OutputParserException: If the output is not valid JSON.
        """
        if len(result) != 1:
            msg = f"Expected exactly one result, but got {len(result)}"
            raise OutputParserException(msg)
        generation = result[0]
        if not isinstance(generation, ChatGeneration):
            msg = "This output parser can only be used with a chat generation."
            raise OutputParserException(msg)
        message = generation.message
        try:
            function_call = message.additional_kwargs["function_call"]
        except KeyError as exc:
            if partial:
                return None
            msg = f"Could not parse function call: {exc}"
            raise OutputParserException(msg) from exc
        try:
            if partial:
                try:
                    if self.args_only:
                        return parse_partial_json(
                            function_call["arguments"], strict=self.strict
                        )
                    return {
                        **function_call,
                        "arguments": parse_partial_json(
                            function_call["arguments"], strict=self.strict
                        ),
                    }
                except json.JSONDecodeError:
                    return None
            elif self.args_only:
                try:
                    return json.loads(function_call["arguments"], strict=self.strict)
                except (json.JSONDecodeError, TypeError) as exc:
                    msg = f"Could not parse function call data: {exc}"
                    raise OutputParserException(msg) from exc
            else:
                try:
                    return {
                        **function_call,
                        "arguments": json.loads(
                            function_call["arguments"], strict=self.strict
                        ),
                    }
                except (json.JSONDecodeError, TypeError) as exc:
                    msg = f"Could not parse function call data: {exc}"
                    raise OutputParserException(msg) from exc
        except KeyError:
            return None
```

#### PydanticOutputFunctionsParser
PydanticOutputFunctionsParser æŽ¥å—çš„ BaseModel æ˜¯ä¸ºäº†æ ¡éªŒ Function Call çš„ arguments å­—æ®µã€‚

```python
class PydanticOutputFunctionsParser(OutputFunctionsParser):
    """Parse an output as a pydantic object.

    This parser is used to parse the output of a ChatModel that uses
    OpenAI function format to invoke functions.

    The parser extracts the function call invocation and matches
    them to the pydantic schema provided.

    An exception will be raised if the function call does not match
    the provided schema.

    Example:
        ... code-block:: python

            message = AIMessage(
                content="This is a test message",
                additional_kwargs={
                    "function_call": {
                        "name": "cookie",
                        "arguments": json.dumps({"name": "value", "age": 10}),
                    }
                },
            )
            chat_generation = ChatGeneration(message=message)

            class Cookie(BaseModel):
                name: str
                age: int

            class Dog(BaseModel):
                species: str

            # Full output
            parser = PydanticOutputFunctionsParser(
                pydantic_schema={"cookie": Cookie, "dog": Dog}
            )
            result = parser.parse_result([chat_generation])
    """

    pydantic_schema: Union[type[BaseModel], dict[str, type[BaseModel]]]
    """The pydantic schema to parse the output with.

    If multiple schemas are provided, then the function name will be used to
    determine which schema to use.
    """

    @model_validator(mode="before")
    @classmethod
    def validate_schema(cls, values: dict) -> Any:
        """Validate the pydantic schema.

        Args:
            values: The values to validate.

        Returns:
            The validated values.

        Raises:
            ValueError: If the schema is not a pydantic schema.
        """
        schema = values["pydantic_schema"]
        if "args_only" not in values:
            values["args_only"] = (
                isinstance(schema, type)
                and not isinstance(schema, GenericAlias)
                and issubclass(schema, BaseModel)
            )
        elif values["args_only"] and isinstance(schema, dict):
            msg = (
                "If multiple pydantic schemas are provided then args_only should be"
                " False."
            )
            raise ValueError(msg)
        return values

    @override
    def parse_result(self, result: list[Generation], *, partial: bool = False) -> Any:
        """Parse the result of an LLM call to a JSON object.

        Args:
            result: The result of the LLM call.
            partial: Whether to parse partial JSON objects. Default is False.

        Returns:
            The parsed JSON object.
        """
        result_ = super().parse_result(result)
        if self.args_only:
            if hasattr(self.pydantic_schema, "model_validate_json"):
                pydantic_args = self.pydantic_schema.model_validate_json(result_)
            else:
                pydantic_args = self.pydantic_schema.parse_raw(result_)  # type: ignore[attr-defined]
        else:
            fn_name = result_["name"]
            args = result_["arguments"]
            if isinstance(self.pydantic_schema, dict):
                pydantic_schema = self.pydantic_schema[fn_name]
            else:
                pydantic_schema = self.pydantic_schema
            if issubclass(pydantic_schema, BaseModel):
                pydantic_args = pydantic_schema.model_validate_json(args)
            elif issubclass(pydantic_schema, BaseModelV1):
                pydantic_args = pydantic_schema.parse_raw(args)
            else:
                msg = f"Unsupported pydantic schema: {pydantic_schema}"
                raise ValueError(msg)
        return pydantic_args

```