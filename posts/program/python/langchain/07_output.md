---
weight: 1
title: "LangChain Output Parser"
date: 2025-07-22T22:00:00+08:00
lastmod: 2025-07-22T22:00:00+08:00
draft: false
author: "å®‹æ¶›"
authorLink: "https://hotttao.github.io/"
description: "langchain output_parsers"
featuredImage: 

tags: ["langchain æºç "]
categories: ["langchain"]

lightgallery: true

toc:
  auto: false
---
## 1. Outputs å’Œ Output Parser
çœ‹ä»£ç ä¼šå‘ç° langchain-core æœ‰ä¸¤ä¸ª outputs å’Œ output_parsers ä¸¤ä¸ªåŒ…ã€‚

æé—®: langchain æºç åŒ…å†… outputs å’Œ output_parsers ä¸¤ä¸ªpackageï¼Œè¯·ä»è¯­ä¹‰ä¸Šè§£é‡Šä¸‹è¿™ä¸¤ä¸ªåŒ…çš„ä½œç”¨
å›ç­”: è¿™ä¸¤ä¸ªåŒ…åç§°ç›¸è¿‘ä½†è¯­ä¹‰å’Œä½œç”¨ä¸åŒï¼Œåœ¨ LangChain ä¸­åˆ†åˆ«æ‰¿æ‹…ç€ **â€œç»“æ„åŒ–è¾“å‡ºâ€** çš„ä¸åŒé˜¶æ®µã€‚æˆ‘ä»¬æ¥é€ä¸€ä»**è¯­ä¹‰ã€åŠŸèƒ½å®šä½ã€å…³ç³»ä¸åŒºåˆ«**å‡ ä¸ªæ–¹é¢è¿›è¡Œè§£é‡Šï¼š


| åŒ…å               | è¯­ä¹‰              | ä½œç”¨                                                              |
| ---------------- | --------------- | --------------------------------------------------------------- |
| `output_parsers` | â€œ**è¾“å‡ºè§£æå™¨**â€     | è´Ÿè´£**æŠŠè¯­è¨€æ¨¡å‹çš„åŸå§‹è¾“å‡ºï¼ˆå­—ç¬¦ä¸²ï¼‰è§£ææˆç»“æ„åŒ–å¯¹è±¡**                                   |
| `outputs`        | â€œ**è¾“å‡ºè§„èŒƒ & ç±»å‹**â€ | å®šä¹‰ç»“æ„åŒ–è¾“å‡ºçš„**ç±»å‹ä½“ç³»ã€è¿è¡ŒåŒ…è£…å™¨ã€è¾“å‡ºå€¼å¯¹è±¡**ç­‰ï¼ˆå¦‚ `ChatGeneration`ã€`PromptValue`ï¼‰ |

---

### `output_parsers` çš„è¯­ä¹‰å’Œä½œç”¨

> `output_parsers` æ˜¯æŒ‡ä¸€ç»„ç”¨äº**å°† LLM åŸå§‹è¿”å›çš„æ–‡æœ¬æˆ–æ¶ˆæ¯æ ¼å¼è§£æä¸ºç»“æ„åŒ–æ•°æ®çš„ç»„ä»¶**ã€‚

è¿™äº›ç»„ä»¶çš„å·¥ä½œç±»ä¼¼äºï¼š

* æå– JSONã€ä»£ç å—ã€Markdown è¡¨æ ¼
* æ­£åˆ™åŒ¹é…ã€åˆ—è¡¨åˆ†å‰²ã€é”®å€¼æå–
* æ¨¡å‹è¾“å‡ºåˆ° Pydantic å®ä¾‹çš„æ˜ å°„

ğŸ”§ å¸¸è§ç±»

* `StrOutputParser`: è¾“å‡ºåŸæ ·è¿”å›å­—ç¬¦ä¸²
* `JsonOutputParser`: è¾“å‡ºè§£æä¸º JSON å­—å…¸
* `PydanticOutputParser`: è¾“å‡ºå˜ä¸ºä¸€ä¸ªæŒ‡å®š Pydantic ç±»çš„å®ä¾‹
* `OutputFixingParser`: å°è¯•ä¿®å¤é”™è¯¯è¾“å‡ºï¼ˆå¦‚ JSON ä¸åˆæ³•ï¼‰
* `RetryOutputParser`: è¾“å‡ºé”™è¯¯æ—¶è¿›è¡Œé‡è¯•

ğŸ§ª ç¤ºä¾‹ç”¨é€”

```python
from langchain.output_parsers import JsonOutputParser

parser = JsonOutputParser()
result = parser.parse('{"name": "Tom", "age": 18}')
# result => {'name': 'Tom', 'age': 18}
```

ä¸»è¦ç”¨äºï¼š

* `Runnable` é“¾æœ«å°¾è§£æå™¨ï¼ˆ`.with_config({"output_parser": ...})`ï¼‰
* Agent çš„å·¥å…·è°ƒç”¨ç»“æœè§£æ
* Prompt + LLM åçš„ç»“æ„åŒ–å¤„ç†

---

### ğŸ“¦ äºŒã€`outputs` çš„è¯­ä¹‰å’Œä½œç”¨

> `outputs` æ˜¯æŒ‡ LangChain ä¸­ç”¨äº**æè¿°è¯­è¨€æ¨¡å‹å“åº”ç»“æœçš„ç±»å‹ä½“ç³»ä¸å°è£…å¯¹è±¡**ï¼Œæ¯”å¦‚ä¸€ä¸ªæ¨¡å‹è¾“å‡ºäº†å¤šæ¡æ¶ˆæ¯ã€æŸæ¡æ¶ˆæ¯çš„æ¥æºæ˜¯è°ç­‰ã€‚

ğŸ§± åŠŸèƒ½å®šä½

* å°è£…åŸå§‹æ¨¡å‹è¾“å‡ºçš„ç»“æ„ï¼ˆæ¯”å¦‚ï¼šå¤šæ¡å›ç­”ã€è§’è‰²ã€æ¶ˆæ¯å†…å®¹ï¼‰
* æ”¯æ’‘é“¾æ¡å¼è°ƒç”¨çš„æ•°æ®ç±»å‹ä¼ é€’
* ä½œä¸º `Runnable` å’Œ `LCEL` æ„å»ºç®¡é“çš„è¾“å…¥/è¾“å‡ºç»“æ„

ğŸ“Œ å¸¸è§å¯¹è±¡

* `LLMResult`: å°è£…ä¸€ä¸ª LLM å¤šæ¬¡è°ƒç”¨çš„ç»“æœï¼ˆåŒ…å«å¤šä¸ª generationsï¼‰
* `Generation`: æ¨¡å‹è¾“å‡ºçš„ä¸€æ¡æ–‡æœ¬
* `ChatGeneration`: åŒ…å«æ¶ˆæ¯è§’è‰²ä¿¡æ¯çš„è¾“å‡ºï¼ˆç”¨äº ChatModelï¼‰
* `PromptValue`: ä¸€ä¸ª prompt è¾“å…¥å€¼ï¼ˆç”¨äºå¤šç§æ ¼å¼ï¼‰
* `ChatMessage`: å¸¦è§’è‰²çš„æ¶ˆæ¯

ğŸ§ª ç¤ºä¾‹ç”¨é€”

```python
from langchain_core.outputs import ChatGeneration

gen = ChatGeneration(message=AIMessage(content="ä½ å¥½"))
print(gen.message.content)
# => "ä½ å¥½"
```

âœ… ç”¨äºå“ªé‡Œï¼Ÿ

* LLM è°ƒç”¨çš„è¿”å›å€¼ç»“æ„
* `Runnable.invoke()` çš„ä¸­é—´å€¼
* `Runnable.map()`ã€`Runnable.each()` å¤„ç†å¤šä¸ª generations
* Agent memoryã€tracer æ—¥å¿—è®°å½•ç­‰

---

### ğŸ”„ ä¸‰ã€ä¸¤è€…ä¹‹é—´çš„å…³ç³»ä¸åŒºåˆ«

| ç»´åº¦   | `output_parsers`                      | `outputs`                                |
| ---- | ------------------------------------- | ---------------------------------------- |
| è¯­ä¹‰   | â€œæŠŠå­—ç¬¦ä¸²å˜æˆç»“æ„åŒ–å¯¹è±¡â€                         | â€œå®šä¹‰ç»“æ„åŒ–å¯¹è±¡çš„ç»“æ„å’Œç±»å‹â€                          |
| æœ¬è´¨   | è§£æå™¨å·¥å…·é›†                                | ç±»å‹å®šä¹‰æ¨¡å—                                   |
| ä¸Šæ¸¸è¾“å…¥ | LLM è¾“å‡ºå­—ç¬¦ä¸²æˆ–æ¶ˆæ¯                          | PromptTemplateã€LLMã€Parser ç­‰çš„è¿”å›å€¼          |
| ä¸‹æ¸¸ç”¨é€” | RAG è§£æã€Agent å·¥å…·è°ƒç”¨ã€Pydantic è½¬æ¢         | é“¾ä¸­é—´å€¼ä¼ é€’ã€ç»“æœå°è£…ã€æ—¥å¿—è®°å½•                         |
| ç¤ºä¾‹ç±»  | `JsonOutputParser`, `StrOutputParser` | `Generation`, `PromptValue`, `LLMResult` |

## 2. OutPut çš„ UML ç±»å›¾
![OutPut ç±»å›¾](/images/langchain/output.svg)

å¦‚å›¾ï¼ŒOutPut çš„ä»£è¡¨æ¯”è¾ƒç®€å•ï¼Œæœ‰å¦‚ä¸‹ç±»:
1. Generation
2. GenerationChunk
3. ChatGeneration
4. ChatGenerationChunk
5. ChatResult
6. LLMResult
7. RunInfo

### 2.1 æé—®
è®©æˆ‘ä»¬é—®é—®å¤§æ¨¡å‹è¿™ä¸‰ä¸ªåŸºç±»ä»£è¡¨çš„è¯­ä¹‰ã€‚

```bash
æˆ‘æ­£åœ¨é˜…è¯»  langchain output å­åŒ…çš„æºä»£ç ï¼Œæ³¨æ„åˆ°åŒ…å†…ï¼Œæœ‰å¦‚ä¸‹ç±»:
1. Generation
2. GenerationChunk
3. ChatGeneration
4. ChatGenerationChunk
5. ChatResult
6. LLMResult
7. RunInfo
è¯·ç»™æˆ‘ä»è¯­ä¹‰ä¸Šè§£é‡Šä¸€ä¸‹è¿™äº›ç±»çš„çš„ä½œç”¨

```


| ç±»å                      | ç±»å‹å½’ç±»      | è¯­ä¹‰è§£é‡Š                                     | ä¸»è¦å­—æ®µ                                                | å…¸å‹ç”¨é€”åœºæ™¯                                           |
| ----------------------- | --------- | ---------------------------------------- | --------------------------------------------------- | ------------------------------------------------ |
| **Generation**          | Output å•å…ƒ | è¡¨ç¤ºä¸€æ¬¡åŸºç¡€ LLM ç”Ÿæˆç»“æœï¼ˆé€‚ç”¨äºéå¯¹è¯æ¨¡å‹ï¼‰                | `text: str`, `generation_info: Optional[dict]`      | ç”¨äº `LLM.invoke()` æˆ– `LLM.generate()` è¾“å‡º          |
| **GenerationChunk**     | æµå¼è¾“å‡ºç‰‡æ®µ    | è¡¨ç¤ºæµå¼è¾“å‡ºè¿‡ç¨‹ä¸­çš„ä¸€ä¸ªæ–‡æœ¬ç‰‡æ®µ                         | åŒä¸Šï¼ˆä¸ä¸€å®šå®Œæ•´ï¼‰                                           | `stream=True` æ—¶ï¼Œé€æ­¥æ„å»ºå®Œæ•´è¾“å‡º                         |
| **ChatGeneration**      | Output å•å…ƒ | è¡¨ç¤º Chat æ¨¡å‹ï¼ˆå¦‚ GPTï¼‰çš„ä¸€æ¡å®Œæ•´æ¶ˆæ¯è¾“å‡º               | `message: BaseMessage`, `generation_info`           | ç”¨äº `ChatModel.invoke()` æˆ– `ChatModel.generate()` |
| **ChatGenerationChunk** | æµå¼è¾“å‡ºç‰‡æ®µ    | è¡¨ç¤º Chat æ¨¡å‹æµå¼è¾“å‡ºä¸­çš„ä¸€æ®µæ¶ˆæ¯ç‰‡æ®µ                   | `message: BaseMessageChunk`                         | ç”¨äº `ChatModel.stream()` åœºæ™¯ä¸‹çš„å¢é‡æ¶ˆæ¯è¾“å‡º               |
| **ChatResult**          | èšåˆç»“æœ      | è¡¨ç¤º Chat æ¨¡å‹ä¸€æ¬¡å®Œæ•´è°ƒç”¨çš„èšåˆç»“æœï¼ˆå¤šæ¡ ChatGenerationï¼‰ | `generations: list[ChatGeneration]`, `llm_output`   | ç”¨äºå°è£…å¤šæ¡ message è¾“å‡ºåŠå…¶é™„åŠ å…ƒä¿¡æ¯                         |
| **LLMResult**           | èšåˆç»“æœ      | è¡¨ç¤ºé Chat æ¨¡å‹ä¸€æ¬¡å®Œæ•´è°ƒç”¨çš„èšåˆç»“æœï¼ˆå¤šæ¡ Generationï¼‰    | `generations: list[list[Generation]]`, `llm_output` | å¤šè½®è¾“å‡ºå°è£…ï¼Œç”¨äº `LLM.generate()`                       |
| **RunInfo**             | å…ƒä¿¡æ¯ç»“æ„ä½“    | è®°å½•ä¸€æ¬¡æ¨¡å‹è°ƒç”¨çš„è¿è¡Œ IDï¼ˆå¯ç”¨äºè¿½è¸ªï¼‰                    | `run_id: UUID`                                      | æ”¯æŒ tracingã€loggingã€chain è¿½è¸ªç­‰åŠŸèƒ½                   |

è¿™ä¸ªå¯¹æ¯”å°±æ¯”è¾ƒæ¸…æ¥šäº†ï¼Œä¸åŒçš„ Ouput å¯¹åº”ä¸åŒçš„ model ä¸åŒçš„è¾“å‡ºæ–¹å¼ä¸‹çš„è¾“å‡ºã€‚è¿™é‡Œæˆ‘ä»¬ä¸¾å…¶ä¸­å‡ ä¸ªå­—æ®µè¾ƒå¤šçš„ç±»è¯´æ˜ä¸€ä¸‹æ¯ä¸ªå­—æ®µçš„å«ä¹‰:

```python
class Generation(Serializable):

    text: str
    """Generated text output."""

    generation_info: Optional[dict[str, Any]] = None
    """Raw response from the provider.

    May include things like the reason for finishing or token log probabilities.
    """
    type: Literal["Generation"] = "Generation"
    """Type is used exclusively for serialization purposes.
    Set to "Generation" for this class."""

class ChatGeneration(Generation):

    text: str = ""
    """The text contents of the output message.

    .. warning::
        SHOULD NOT BE SET DIRECTLY!
    """
    message: BaseMessage
    """The message output by the chat model."""
    # Override type to be ChatGeneration, ignore mypy error as this is intentional
    type: Literal["ChatGeneration"] = "ChatGeneration"  # type: ignore[assignment]
    """Type is used exclusively for serialization purposes."""


class LLMResult(BaseModel):
    """A container for results of an LLM call.

    Both chat models and LLMs generate an LLMResult object. This object contains the
    generated outputs and any additional information that the model provider wants to
    return.
    """

    generations: list[
        list[Union[Generation, ChatGeneration, GenerationChunk, ChatGenerationChunk]]
    ]
    
    llm_output: Optional[dict] = None
    """For arbitrary LLM provider specific output.

    This dictionary is a free-form dictionary that can contain any information that the
    provider wants to return. It is not standardized and is provider-specific.

    Users should generally avoid relying on this field and instead rely on accessing
    relevant information from standardized fields present in AIMessage.
    """
    run: Optional[list[RunInfo]] = None
    """List of metadata info for model call for each input."""

    type: Literal["LLMResult"] = "LLMResult"
    """Type is used exclusively for serialization purposes."""

```

ä¸‹é¢æ˜¯å¯¹ä½ ç»™å‡ºçš„ä¸‰ä¸ªç±» â€”â€” `Generation`ã€`ChatGeneration`ã€`LLMResult` ä¸­**æ¯ä¸ªå­—æ®µçš„å«ä¹‰**çš„è¡¨æ ¼åŒ–è§£é‡Šï¼š

---

#### âœ… `Generation` å­—æ®µè¯´æ˜

| å­—æ®µå               | ç±»å‹                         | å«ä¹‰è¯´æ˜                                   |
| ----------------- | -------------------------- | -------------------------------------- |
| `text`            | `str`                      | æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬è¾“å‡ºã€‚é€šå¸¸æ˜¯ `LLM` æ¨¡å‹ç”Ÿæˆçš„æ­£æ–‡å†…å®¹ã€‚         |
| `generation_info` | `Optional[dict[str, Any]]` | æ¨¡å‹æä¾›å•†è¿”å›çš„åŸå§‹é™„åŠ ä¿¡æ¯ï¼Œå¦‚åœæ­¢åŸå› ã€token logprobs ç­‰ã€‚ |
| `type`            | `Literal["Generation"]`    | ç”¨äºåºåˆ—åŒ–ç›®çš„çš„æ ‡è¯†å­—æ®µï¼ŒæŒ‡æ˜è¿™æ˜¯ `Generation` ç±»å‹çš„å®ä¾‹ã€‚  |

---

#### âœ… `ChatGeneration` å­—æ®µè¯´æ˜ï¼ˆç»§æ‰¿è‡ª `Generation`ï¼‰

| å­—æ®µå               | ç±»å‹                          | å«ä¹‰è¯´æ˜                                                 |
| ----------------- | --------------------------- | ---------------------------------------------------- |
| `text`            | `str`                       | æä¾›å¿«æ·è®¿é—®æ¶ˆæ¯å†…å®¹çš„æ–¹å¼ï¼ˆé€šå¸¸ä¸º `message.content` çš„åˆ«åï¼‰ã€‚**ä¸åº”ç›´æ¥è®¾ç½®**ã€‚ |
| `message`         | `BaseMessage`               | Chat æ¨¡å‹ç”Ÿæˆçš„æ¶ˆæ¯å¯¹è±¡ï¼ˆå¦‚ `AIMessage`ï¼‰ï¼Œæ˜¯æ¨èçš„è®¿é—®å…¥å£ã€‚              |
| `generation_info` | `Optional[dict[str, Any]]`  | åŒ `Generation`ï¼Œé™„åŠ ä¿¡æ¯å¦‚ stop\_reasonã€logprobsï¼ˆå¯èƒ½ä¸ºç©ºï¼‰ã€‚    |
| `type`            | `Literal["ChatGeneration"]` | ç”¨äºåºåˆ—åŒ–ï¼Œè¡¨æ˜è¿™æ˜¯ Chat æ¨¡å‹ç”Ÿæˆçš„ç»“æœã€‚                             |

---

#### âœ… `LLMResult` å­—æ®µè¯´æ˜

| å­—æ®µå           | ç±»å‹                                                                                    | å«ä¹‰è¯´æ˜                                                           |
| ------------- | ------------------------------------------------------------------------------------- | -------------------------------------------------------------- |
| `generations` | `list[list[Union[Generation, ChatGeneration, GenerationChunk, ChatGenerationChunk]]]` | å¤–å±‚ list å¯¹åº”å¤šä¸ªè¾“å…¥ï¼›å†…å±‚ list ä¸ºè¯¥è¾“å…¥çš„å¤šä¸ªå€™é€‰è¾“å‡ºã€‚æ¯ä¸ªè¾“å‡ºæ˜¯ä¸€æ¬¡ Generation æˆ– Chunkã€‚ |
| `llm_output`  | `Optional[dict]`                                                                      | æ¨¡å‹ä¾›åº”å•†æä¾›çš„åŸå§‹è¾“å‡ºä¿¡æ¯ï¼Œå¦‚ token ç”¨é‡ã€æ¨¡å‹ç‰ˆæœ¬ç­‰ã€‚ç»“æ„ä¸ç»Ÿä¸€ï¼Œå»ºè®®ä¸è¦ç›´æ¥ä¾èµ–ã€‚                |
| `run`         | `Optional[list[RunInfo]]`                                                             | æ¯ä¸ªè¾“å…¥è°ƒç”¨çš„è¿è¡Œå…ƒä¿¡æ¯ï¼ˆå¦‚ run\_idï¼‰ï¼Œç”¨äº tracingã€è°ƒè¯•ç­‰ã€‚                        |
| `type`        | `Literal["LLMResult"]`                                                                | ç”¨äºåºåˆ—åŒ–æ ‡è¯†è¯¥å¯¹è±¡ä¸º `LLMResult` ç±»å‹ã€‚                                    |

---

#### âœ… å›¾ç¤ºç»“æ„å…³ç³»è¡¥å……

```text
LLMResult
â”œâ”€â”€ generations (List[List[Generation/ChatGeneration]])
â”‚   â”œâ”€â”€ Generation
â”‚   â””â”€â”€ ChatGeneration
â”‚       â””â”€â”€ message (AIMessage or other BaseMessage)
â”œâ”€â”€ llm_output
â””â”€â”€ run (List[RunInfo])
```

