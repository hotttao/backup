---
weight: 1
title: "LangChain Language Model"
date: 2025-07-23T14:00:00+08:00
lastmod: 2025-07-23T14:00:00+08:00
draft: false
author: "å®‹æ¶›"
authorLink: "https://hotttao.github.io/"
description: "langchain language model"
featuredImage: 

tags: ["langchain æºç "]
categories: ["langchain"]

lightgallery: true

toc:
  auto: false
---


è¿™ä¸€èŠ‚æˆ‘ä»¬æ¥çœ‹ Langchain çš„ Language Modelã€‚

## 1. Language Model çš„ UML ç±»å›¾
![Language Model ç±»å›¾](/images/langchain/language_model.svg)

Language Model æœ‰å¦‚ä¸‹åŸºç±»:
1. BaseLanguageModel
2. BaseChatModel
3. BaseLLM


Language Model åŸºç±»ä¸å¤šï¼Œè®©æˆ‘ä»¬ç›´æ¥é—®å¤§æ¨¡å‹è¿™äº›ç±»çš„è¯­ä¹‰

```bash
æˆ‘æ­£åœ¨é˜…è¯»  Language Model å­åŒ…çš„æºä»£ç ï¼Œæ³¨æ„åˆ°åŒ…å†…ï¼Œæœ‰å¦‚ä¸‹ç±»:
1. BaseLanguageModel
2. BaseChatModel
3. BaseLLM

è¯·ç»™æˆ‘ä»è¯­ä¹‰ä¸Šè§£é‡Šä¸€ä¸‹è¿™äº›ç±»çš„çš„ä½œç”¨

```

åœ¨ LangChain ä¸­ï¼Œ`language_models` å­åŒ…æ˜¯æ ¸å¿ƒæ¨¡å—ä¹‹ä¸€ï¼Œè´Ÿè´£æŠ½è±¡å’Œå®ç°å¯¹åº•å±‚è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ OpenAIã€Anthropicã€Cohere ç­‰ï¼‰çš„è°ƒç”¨é€»è¾‘ã€‚ä½ æåˆ°çš„ä¸¤ä¸ªç±»ï¼š

1. `BaseLanguageModel`
2. `BaseChatModel`

å®ƒä»¬æ˜¯è¿™ä¸€æ¨¡å—ä¸­çš„ä¸¤ä¸ªé‡è¦æŠ½è±¡åŸºç±»ã€‚æˆ‘ä»¬å¯ä»¥ä»**è¯­ä¹‰ï¼ˆåŠŸèƒ½å®šä½ï¼‰å’Œç»§æ‰¿å…³ç³»**ä¸¤ä¸ªè§’åº¦æ¥è§£é‡Šå®ƒä»¬çš„ä½œç”¨ï¼š

---

### `BaseLanguageModel` â€” é€šç”¨è¯­è¨€æ¨¡å‹åŸºç±»

ğŸŒ è¯­ä¹‰è¯´æ˜ï¼š

`BaseLanguageModel` æ˜¯æ‰€æœ‰è¯­è¨€æ¨¡å‹çš„**æœ€åŸºç¡€æŠ½è±¡ç±»**ï¼Œå®ƒç»Ÿä¸€å°è£…äº†â€œè°ƒç”¨ä¸€ä¸ªè¯­è¨€æ¨¡å‹å¹¶è¿”å›ç”Ÿæˆç»“æœâ€çš„åŸºç¡€è¡Œä¸ºï¼Œ**ä¸é™åˆ¶æ¨¡å‹çš„äº¤äº’æ–¹å¼**ï¼ˆå³æ”¯æŒ completionã€chatã€function è°ƒç”¨ç­‰å¤šç§å½¢å¼ï¼‰ã€‚

> âœ… å®ƒæ›´ååº•å±‚ã€æ›´æŠ½è±¡ã€æ›´é€šç”¨ã€‚

ğŸ§© å…³é”®èŒè´£ï¼š

* æä¾›é€šç”¨çš„æ¥å£å®šä¹‰ï¼Œå¦‚ï¼š

  * `generate()`: å¯¹å¤šä¸ªæç¤ºè¿›è¡Œæ‰¹é‡ç”Ÿæˆã€‚
  * `invoke()`: å¯¹ä¸€ä¸ªè¾“å…¥ç”Ÿæˆè¾“å‡ºã€‚
  * è¾“å‡º `LLMResult` / `ChatResult` ç­‰ç»“æ„åŒ–ç»“æœã€‚
* å®šä¹‰ tracingï¼ˆè¿è¡Œè¿½è¸ªï¼‰ã€ç¼“å­˜ã€å›è°ƒå¤„ç†ç­‰åº•å±‚æœºåˆ¶çš„æ¥å£ã€‚
* å­ç±»ä¸€èˆ¬ä¼šé‡å†™ `_generate()` æ¥å®ç°è‡ªå·±çš„æ ¸å¿ƒé€»è¾‘ã€‚

ğŸ§¬ å…¸å‹å­ç±»æœ‰ï¼š

* `LLM`: åŸºäºæ–‡æœ¬è¡¥å…¨ï¼ˆtext completionï¼‰æ¨¡å‹ã€‚
* `ChatModel`: åŸºäºå¤šè½®å¯¹è¯ï¼ˆchat-basedï¼‰æ¨¡å‹ã€‚

---

### `BaseChatModel` â€” é¢å‘â€œå¯¹è¯æ¨¡å‹â€çš„æŠ½è±¡åŸºç±»

ğŸ’¬ è¯­ä¹‰è¯´æ˜ï¼š

`BaseChatModel` æ˜¯ `BaseLanguageModel` çš„ä¸€ä¸ªå­ç±»ï¼Œç”¨äºå°è£… **chat-based æ¨¡å‹çš„æ ‡å‡†è¡Œä¸º**ï¼Œå¦‚ OpenAI çš„ `gpt-3.5-turbo`ã€Anthropic Claude ç­‰ã€‚å®ƒä»¬éƒ½ä»¥å¤šè½®å¯¹è¯æ¶ˆæ¯ï¼ˆå¦‚ userã€systemã€assistantï¼‰ä¸ºè¾“å…¥ã€‚

> âœ… è¿™æ˜¯ä¸€ä¸ª**å¯¹è¯é£æ ¼æ¨¡å‹ï¼ˆChat Modelï¼‰ä¸“ç”¨çš„æŠ½è±¡åŸºç±»**ã€‚

ğŸ§© å…³é”®èŒè´£ï¼š

* å°† `List[BaseMessage]` ç±»å‹ä½œä¸ºè¾“å…¥ï¼ˆæ¯”å¦‚ SystemMessageã€HumanMessageã€AIMessageï¼‰ã€‚
* å®ç° `generate()` æ–¹æ³•æ¥å¤„ç†å¤šä¸ª chat promptã€‚
* æä¾› `predict_messages()` æ¥å£ï¼Œè¿”å›ç»“æ„åŒ–çš„ AIMessageã€‚
* å®šä¹‰ `_generate()` å’Œ `_combine_messages()` ç­‰é’©å­ä¾›å­ç±»å®ç°ã€‚

ğŸ¯ ä½¿ç”¨åœºæ™¯ï¼š

å½“ä½ å¸Œæœ›å°è£…ä¸€ä¸ªèŠå¤©é£æ ¼æ¥å£ï¼ˆå¦‚ OpenAI çš„ ChatCompletion APIï¼‰ï¼Œä½ åº”è¯¥ç»§æ‰¿è¿™ä¸ªç±»ï¼Œè€Œä¸æ˜¯ `BaseLanguageModel`ã€‚


| ç±»å                  | è¯­ä¹‰å®šä½              | è¾“å…¥ç±»å‹                | ä½¿ç”¨åœºæ™¯                | å­ç±»ç¤ºä¾‹                          |
| ------------------- | ----------------- | ------------------- | ------------------- | ----------------------------- |
| `BaseLanguageModel` | æ‰€æœ‰è¯­è¨€æ¨¡å‹çš„ç»Ÿä¸€æŠ½è±¡       | Promptï¼ˆå­—ç¬¦ä¸²æˆ–æ¶ˆæ¯ï¼‰      | åº•å±‚ç»Ÿä¸€æŠ½è±¡å±‚ï¼Œä¸»è¦å®šä¹‰æ¥å£ä¸å…±æ€§è¡Œä¸º | `LLM`, `BaseChatModel`        |
| `BaseChatModel`     | é¢å‘ Chat ç±»å‹è¯­è¨€æ¨¡å‹çš„æŠ½è±¡ | `List[BaseMessage]` | æ„å»ºæ”¯æŒå¤šè§’è‰²å¯¹è¯çš„æ¨¡å‹ï¼ˆå¦‚ GPTï¼‰ | `ChatOpenAI`, `ChatAnthropic` |



```
BaseLanguageModel
â”œâ”€â”€ BaseLLM               # é¢å‘è¡¥å…¨æ–‡æœ¬ï¼ˆcompletionï¼‰ç±»æ¨¡å‹
    â””â”€â”€ LLM               # å®é™…çš„æŸä¸ªå®ç°
â””â”€â”€ BaseChatModel    # é¢å‘å¯¹è¯é£æ ¼æ¨¡å‹
    â””â”€â”€ ChatOpenAI   # å®é™…çš„æŸä¸ªå®ç°
```

LanguageModel çš„å®ç°æ¯”è¾ƒå¤æ‚ï¼Œæˆ‘å…ˆæ¥å¯¹æ¯”è¿™å‡ ä¸ªç±»çš„æŠ½è±¡å±‚æ¬¡ï¼Œå†æ¥çœ‹å…·ä½“çš„ä»£ç ã€‚

## 2. å±æ€§å’ŒæŠ½è±¡æ–¹æ³•
### 2.1 BaseLanguageModel

```python
class BaseLanguageModel(
    RunnableSerializable[LanguageModelInput, LanguageModelOutputVar], ABC
):
    """Abstract base class for interfacing with language models.

    All language model wrappers inherited from BaseLanguageModel.
    """

    cache: Union[BaseCache, bool, None] = Field(default=None, exclude=True)
    """Whether to cache the response.

    * If true, will use the global cache.
    * If false, will not use a cache
    * If None, will use the global cache if it's set, otherwise no cache.
    * If instance of BaseCache, will use the provided cache.

    Caching is not currently supported for streaming methods of models.
    """
    verbose: bool = Field(default_factory=_get_verbosity, exclude=True, repr=False)
    """Whether to print out response text."""
    callbacks: Callbacks = Field(default=None, exclude=True)
    """Callbacks to add to the run trace."""
    tags: Optional[list[str]] = Field(default=None, exclude=True)
    """Tags to add to the run trace."""
    metadata: Optional[dict[str, Any]] = Field(default=None, exclude=True)
    """Metadata to add to the run trace."""
    custom_get_token_ids: Optional[Callable[[str], list[int]]] = Field(
        default=None, exclude=True
    )
    """Optional encoder to use for counting tokens."""

    model_config = ConfigDict(
        arbitrary_types_allowed=True,
    )
```

| å±æ€§å                    | ç±»å‹                                     | é»˜è®¤å€¼                            | æ˜¯å¦å‚ä¸åºåˆ—åŒ–                         | è¯­ä¹‰è§£é‡Š                                                                                                                                    |
| ---------------------- | -------------------------------------- | ------------------------------ | ------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------- |
| `cache`                | `Union[BaseCache, bool, None]`         | `None`                         | âŒ `exclude=True`                | æŒ‡å®šæ˜¯å¦å¯ç”¨å“åº”ç¼“å­˜ï¼š<br>- `True`: ä½¿ç”¨å…¨å±€ç¼“å­˜<br>- `False`: ä¸ä½¿ç”¨ç¼“å­˜<br>- `BaseCache` å®ä¾‹ï¼šä½¿ç”¨è‡ªå®šä¹‰ç¼“å­˜<br>- `None`: è‹¥å·²è®¾ç½®å…¨å±€ç¼“å­˜åˆ™ä½¿ç”¨ï¼Œå¦åˆ™ä¸ç¼“å­˜ã€‚**æ³¨æ„ï¼šä¸æ”¯æŒ streaming ç¼“å­˜ã€‚** |
| `verbose`              | `bool`                                 | `_get_verbosity()`             | âŒ `exclude=True`, ä¸åœ¨ `repr` ä¸­æ˜¾ç¤º | æ§åˆ¶æ˜¯å¦æ‰“å°æ¨¡å‹è¿”å›çš„æ–‡æœ¬ç»“æœï¼Œé€‚ç”¨äºè°ƒè¯•æ—¶æŸ¥çœ‹ç”Ÿæˆå†…å®¹ã€‚                                                                                                           |
| `callbacks`            | `Callbacks`                            | `None`                         | âŒ `exclude=True`                | ä¸º LLM è°ƒç”¨æ·»åŠ å›è°ƒæœºåˆ¶ï¼ˆå¦‚ tracingã€loggingã€streaming ç­‰ï¼‰ï¼Œè´¯ç©¿è°ƒç”¨ç”Ÿå‘½å‘¨æœŸã€‚                                                                                 |
| `tags`                 | `Optional[list[str]]`                  | `None`                         | âŒ `exclude=True`                | é™„åŠ åˆ° run trace çš„æ ‡ç­¾ï¼Œå¯ç”¨äºè¿è¡Œçš„æ ‡è®°å’Œè¿‡æ»¤ã€‚                                                                                                          |
| `metadata`             | `Optional[dict[str, Any]]`             | `None`                         | âŒ `exclude=True`                | é™„åŠ å…ƒæ•°æ®åˆ° run traceï¼Œæ”¯æŒè®°å½•è¿è¡Œçš„é¢å¤–ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚                                                                                                        |
| `custom_get_token_ids` | `Optional[Callable[[str], list[int]]]` | `None`                         | âŒ `exclude=True`                | è‡ªå®šä¹‰çš„ tokenizer å‡½æ•°ï¼Œç”¨äºç»Ÿè®¡ token æ•°ï¼ˆå¸¸ç”¨äº token é™åˆ¶æˆ–è´¹ç”¨è¯„ä¼°ï¼‰ã€‚                                                                                      |


BaseLanguageModel çš„ä¸»è¦ä½œç”¨æ˜¯å®šä¹‰æ¥å£:

```python
class BaseLanguageModel(
    RunnableSerializable[LanguageModelInput, LanguageModelOutputVar], ABC
):
    @abstractmethod
    def generate_prompt(
        self,
        prompts: list[PromptValue],
        stop: Optional[list[str]] = None,
        callbacks: Callbacks = None,
        **kwargs: Any,
    ) -> LLMResult:

    @abstractmethod
    async def agenerate_prompt(
        self,
        prompts: list[PromptValue],
        stop: Optional[list[str]] = None,
        callbacks: Callbacks = None,
        **kwargs: Any,
    ) -> LLMResult:
```

BaseLanguageModel ç»§æ‰¿è‡ª RunnableSerializableï¼Œä½†æ˜¯å¹¶æ²¡æœ‰æä¾› invoke çš„é»˜è®¤å®ç°ã€‚

æ‰€ä»¥ BaseLanguageModel ä¸»è¦æœ‰ä¸‰ä¸ªæ¥å£æ–¹æ³•:
1. `generate_prompt`
2. `agenerate_prompt`
3. `invoke`

BaseLanguageModel è¿˜æœ‰ä¸€äº›è¿‡æœŸçš„æ¥å£ï¼Œè¿™é‡Œæœªåˆ—å‡ºã€‚

### 2.2 BaseChatModel

```python
class BaseChatModel(BaseLanguageModel[BaseMessage], ABC):
    """Base class for chat models.
    """  # noqa: E501

    callback_manager: Optional[BaseCallbackManager] = deprecated(
        name="callback_manager", since="0.1.7", removal="1.0", alternative="callbacks"
    )(
        Field(
            default=None,
            exclude=True,
            description="Callback manager to add to the run trace.",
        )
    )

    rate_limiter: Optional[BaseRateLimiter] = Field(default=None, exclude=True)
    "An optional rate limiter to use for limiting the number of requests."

    disable_streaming: Union[bool, Literal["tool_calling"]] = False
    """Whether to disable streaming for this model.

    If streaming is bypassed, then ``stream()``/``astream()``/``astream_events()`` will
    defer to ``invoke()``/``ainvoke()``.

    - If True, will always bypass streaming case.
    - If ``'tool_calling'``, will bypass streaming case only when the model is called
      with a ``tools`` keyword argument. In other words, LangChain will automatically
      switch to non-streaming behavior (``invoke()``) only when the tools argument is
      provided. This offers the best of both worlds.
    - If False (default), will always use streaming case if available.

    The main reason for this flag is that code might be written using ``.stream()`` and
    a user may want to swap out a given model for another model whose the implementation
    does not properly support streaming.
    """

```

| å±æ€§å                 | ç±»å‹                                     | é»˜è®¤å€¼     | æ˜¯å¦åºåˆ—åŒ–            | è¯´æ˜                                                                                                                               |
| ------------------- | -------------------------------------- | ------- | ---------------- | -------------------------------------------------------------------------------------------------------------------------------- |
| `callback_manager`  | `Optional[BaseCallbackManager]`        | `None`  | âŒ `exclude=True` | **å·²åºŸå¼ƒ**ï¼Œè¯·ä½¿ç”¨ `callbacks`ã€‚ç”¨äºç®¡ç†å›è°ƒå‡½æ•°ç”Ÿå‘½å‘¨æœŸï¼Œå¦‚ tracingã€loggingã€token ç»Ÿè®¡ç­‰ã€‚                                                                |
| `rate_limiter`      | `Optional[BaseRateLimiter]`            | `None`  | âŒ `exclude=True` | è¯·æ±‚é€Ÿç‡é™åˆ¶å™¨ï¼Œå¯é™åˆ¶è°ƒç”¨é¢‘ç‡ï¼Œé¿å… API è¶…é¢ã€‚                                                                                                       |
| `disable_streaming` | `Union[bool, Literal["tool_calling"]]` | `False` | âœ…                | æ˜¯å¦å…³é—­æµå¼è¾“å‡ºï¼š<br> - `True`: å§‹ç»ˆå…³é—­æµå¼ï¼Œæ”¹ç”¨æ™®é€šè°ƒç”¨<br> - `"tool_calling"`ï¼šä»…å½“è°ƒç”¨åŒ…å« `tools` å‚æ•°æ—¶å…³é—­æµå¼<br> - `False`ï¼ˆé»˜è®¤ï¼‰ï¼šæ”¯æŒæµå¼æ—¶ä½¿ç”¨æµå¼è¿”å›ï¼ˆå¦‚ `.stream()`ï¼‰ |


BaseChatModel é‡è½½äº† Runnable çš„å¤§å¤šæ•°æ–¹æ³•ï¼Œå› æ­¤ä»£ç æ¯”è¾ƒé•¿ã€‚æˆ‘å†æ¬¡é—®äº† chatgptï¼Œåœ¨å›ç­”é‡Œæ‘˜å½•äº†ä¸‹é¢æœ‰å…³æ‰€æœ‰æ–¹æ³•çš„æ€»ç»“å›¾ã€‚ä½†æ˜¯è¿™ä¸ªè¿˜ä¸è¶³ä»¥å¸®æˆ‘ä»¬æ²¥é’æ–¹æ³•ä¹‹é—´çš„è°ƒç”¨å…³ç³»ã€‚æ‰€ä»¥æˆ‘ä»¬å…ˆæ¥çœ‹ BaseChatModel æŠ½è±¡äº†å“ªäº›æ–¹æ³•ã€‚

```python
BaseChatModel
â”œâ”€â”€ ğŸ”¹ æ ¸å¿ƒè°ƒç”¨æ–¹æ³•
â”‚   â”œâ”€â”€ invoke / ainvoke
â”‚   â”œâ”€â”€ generate / agenerate
â”‚   â”œâ”€â”€ batch / abatch
â”‚   â””â”€â”€ batch_as_completed / abatch_as_completed
â”‚
â”œâ”€â”€ ğŸ”¹ æµå¼è¾“å‡º
â”‚   â”œâ”€â”€ stream / astream
â”‚   â””â”€â”€ astream_events
â”‚
â”œâ”€â”€ ğŸ”¹ å£°æ˜å¼ç»„åˆ
â”‚   â”œâ”€â”€ bind_tools / with_structured_output
â”‚   â”œâ”€â”€ with_retry / with_fallbacks
â”‚   â””â”€â”€ configurable_fields / configurable_alternatives
â”‚
â”œâ”€â”€ ğŸ”¹ å­ç±»éœ€å®ç°
â”‚   â”œâ”€â”€ _generate / _agenerate
â”‚   â”œâ”€â”€ _stream / _astream
â”‚   â””â”€â”€ _llm_type / _identifying_params
â”‚
â””â”€â”€ ğŸ”¹ å±æ€§ä¸é…ç½®
    â”œâ”€â”€ rate_limiter / disable_streaming
    â””â”€â”€ callback_manager (deprecated)
```

é€šè¿‡æœç´¢ abstractmethod å’Œ NotImplementedError æˆ‘æ‰¾åˆ° BaseChatModel å®šä¹‰çš„å¦‚ä¸‹æ–¹æ³•ã€‚

```python
class BaseChatModel(BaseLanguageModel[BaseMessage], ABC):
    @property
    @abstractmethod
    def _llm_type(self) -> str:
        """Return type of chat model."""

    @abstractmethod
    def _generate(
        self,
        messages: list[BaseMessage],
        stop: Optional[list[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> ChatResult:
        """Top Level call."""

    def _stream(
        self,
        messages: list[BaseMessage],
        stop: Optional[list[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> Iterator[ChatGenerationChunk]:
        raise NotImplementedError

    def bind_tools(
        self,
        tools: Sequence[
            Union[typing.Dict[str, Any], type, Callable, BaseTool]  # noqa: UP006
        ],
        *,
        tool_choice: Optional[Union[str]] = None,
        **kwargs: Any,
    ) -> Runnable[LanguageModelInput, BaseMessage]:
        """Bind tools to the model.

        Args:
            tools: Sequence of tools to bind to the model.
            tool_choice: The tool to use. If "any" then any tool can be used.

        Returns:
            A Runnable that returns a message.
        """
        raise NotImplementedError
```

é€šè¿‡æœç´¢è¿™äº›æ–¹æ³•å¯ä»¥æ‰¾åˆ°å¦‚ä¸‹çš„è°ƒç”¨å…³ç³»:

```bash
invoke:
    generate_prompt
        generate
            _get_invocation_params
                dict
                    _llm_type
            _generate_with_cache
                _generate
stream:
    _stream

with_structured_output:
    bind_tools
```

### 2.3 BaseLLM
æˆ‘ä»¬å¯¹ BaseLLM é‡å¤ä¸Šè¿°è¿‡ç¨‹:

```python
class BaseLLM(BaseLanguageModel[str], ABC):
    """Base LLM abstract interface.

    It should take in a prompt and return a string.
    """

    callback_manager: Optional[BaseCallbackManager] = Field(default=None, exclude=True)
    """[DEPRECATED]"""

    model_config = ConfigDict(
        arbitrary_types_allowed=True,
    )

    @abstractmethod
    def _generate(
        self,
        prompts: list[str],
        stop: Optional[list[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> LLMResult:
        """Run the LLM on the given prompts."""

    @property
    @abstractmethod
    def _llm_type(self) -> str:
        """Return type of llm."""

    def _stream(
        self,
        prompt: str,
        stop: Optional[list[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> Iterator[GenerationChunk]:

        raise NotImplementedError
```

æŠ½è±¡æ–¹æ³•çš„è°ƒç”¨å…³ç³»:

```bash
invoke
    generate_prompt
        generate
            _generate_helper
                _generate
            dict
                _llm_type

stream:
    _stream
```

é€šè¿‡å¯¹æ¯”æˆ‘ä»¬å¯ä»¥å‘ç°:
1. BaseLLMã€BaseChatModel éƒ½å°† BaseLanguageModel çš„æ¥å£è½¬æ¢åˆ°äº† _generate
2. éƒ½è¦æ±‚å­ç±»å®ç° _stream æ–¹æ³•
3. ä½†æ˜¯ä¸¤è€…æ–¹æ³•æ¥å—çš„å‚æ•°ä¸åŒï¼ŒBaseLLM æ¥å— `list[str]`ï¼ŒBaseChatModel æ¥å— `list[BaseMessage]`
4. ä¸¤è€…çš„è¿”å›å€¼ä¸åŒï¼ŒBaseLLM è¿”å› `LLMResult`ï¼ŒBaseChatModel è¿”å› `ChatResult`

ä¸‹é¢æˆ‘ä»¬å…ˆæ¥çœ‹ BaseChatModel çš„ä»£ç ã€‚
## 3. BaseChatModel
### 3.1 invoke
BaseChatModel invoke éœ€è¦å…³æ³¨ä»¥ä¸‹å®ç°ç»†èŠ‚:
1. invoke å†…è°ƒç”¨ `self._convert_input` å°†è¾“å…¥æ ‡å‡†åŒ–ä¸º `list[PromptValue]`
2. generate_prompt å†…å°† `list[PromptValue]` è½¬åŒ–ä¸º `list[list[BaseMessage]]` å­ list ä»£è¡¨ä¸€ä¸ªå®Œæ•´çš„ Prompt
3. generate å®Œæˆå¤šä¸ªç‹¬ç«‹çš„ Prompt è°ƒç”¨è¿‡ç¨‹

```python
LanguageModelInput = Union[PromptValue, str, Sequence[MessageLikeRepresentation]]
LanguageModelOutput = Union[BaseMessage, str]
LanguageModelLike = Runnable[LanguageModelInput, LanguageModelOutput]
LanguageModelOutputVar = TypeVar("LanguageModelOutputVar", BaseMessage, str)

class BaseChatModel(BaseLanguageModel[BaseMessage], ABC):
    @override
    def invoke(
        self,
        input: LanguageModelInput,
        config: Optional[RunnableConfig] = None,
        *,
        stop: Optional[list[str]] = None,
        **kwargs: Any,
    ) -> BaseMessage:
        config = ensure_config(config)
        return cast(
            "ChatGeneration",
            self.generate_prompt(
                # è¾“å…¥æ ‡å‡†åŒ–
                [self._convert_input(input)],
                stop=stop,
                callbacks=config.get("callbacks"),
                tags=config.get("tags"),
                metadata=config.get("metadata"),
                run_name=config.get("run_name"),
                run_id=config.pop("run_id", None),
                **kwargs,
            ).generations[0][0],
        ).message

    @override
    def generate_prompt(
        self,
        prompts: list[PromptValue],
        stop: Optional[list[str]] = None,
        callbacks: Callbacks = None,
        **kwargs: Any,
    ) -> LLMResult:
        prompt_messages = [p.to_messages() for p in prompts]
        return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
```
### 3.2 generate
generate çš„è°ƒç”¨è¿‡ç¨‹åˆ†æˆäº†å¦‚ä¸‹å‡ ä¸ªéƒ¨åˆ†:
1. callback_manager çš„è°ƒç”¨ï¼Œè¿™ä¸€éƒ¨åˆ†è¯¦è§ ![CallBack Manager](./03_callback.md)
2. éå† messages å‚æ•°ï¼Œæ‰§è¡Œ `_generate_with_cache` è·å– ChatResult
3. åˆå¹¶ ChatResult ä¸º LLMResult è¿”å›
4. ä»£ç æ‰§è¡Œçš„è¿‡ç¨‹ä¸­æœ‰ä¸¤æ¬¡ message çš„è½¬æ¢ï¼Œè¯¦è§: ![CallBack Manager](./31_message_convert.md)

```python
    @abstractmethod
    def _generate(
        self,
        messages: list[BaseMessage],
        stop: Optional[list[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> ChatResult:
        """Top Level call."""

    def generate(
        self,
        messages: list[list[BaseMessage]],
        stop: Optional[list[str]] = None,
        callbacks: Callbacks = None,
        *,
        tags: Optional[list[str]] = None,
        metadata: Optional[dict[str, Any]] = None,
        run_name: Optional[str] = None,
        run_id: Optional[uuid.UUID] = None,
        **kwargs: Any,
    ) -> LLMResult:

        ls_structured_output_format = kwargs.pop(
            "ls_structured_output_format", None
        ) or kwargs.pop("structured_output_format", None)
        # ä» ls_structured_output_format è·å– schema
        ls_structured_output_format_dict = _format_ls_structured_output(
            ls_structured_output_format
        )
        # è·å–æœ‰å…³æ¨¡å‹çš„å‚æ•°ä¿¡æ¯
        params = self._get_invocation_params(stop=stop, **kwargs)
        options = {"stop": stop, **ls_structured_output_format_dict}
        inheritable_metadata = {
            **(metadata or {}),
            # è·å– LangSmithParams
            **self._get_ls_params(stop=stop, **kwargs),
        }

        callback_manager = CallbackManager.configure(
            callbacks,
            self.callbacks,
            self.verbose,
            tags,
            self.tags,
            inheritable_metadata,
            self.metadata,
        )

        messages_to_trace = [
            # å°†Messageè½¬æ¢ä¸º open api æ”¯æŒçš„æ ¼å¼ï¼Œä¾¿äºåœ¨ on_chat_model_start ä¸­ä½¿ç”¨
            _format_for_tracing(message_list) for message_list in messages
        ]
        run_managers = callback_manager.on_chat_model_start(
            self._serialized,
            messages_to_trace,
            invocation_params=params,
            options=options,
            name=run_name,
            run_id=run_id,
            batch_size=len(messages),
        )
        results = []
        input_messages = [
            # æ ¼å¼åŒ–ä¸º langchain æ ‡å‡†æ¶ˆæ¯æ ¼å¼
            _normalize_messages(message_list) for message_list in messages
        ]
        for i, m in enumerate(input_messages):
            try:
                results.append(
                    self._generate_with_cache(
                        m,
                        stop=stop,
                        run_manager=run_managers[i] if run_managers else None,
                        **kwargs,
                    )
                )
            except BaseException as e:
                if run_managers:
                    generations_with_error_metadata = _generate_response_from_error(e)
                    run_managers[i].on_llm_error(
                        e,
                        response=LLMResult(
                            generations=[generations_with_error_metadata]  # type: ignore[list-item]
                        ),
                    )
                raise
        flattened_outputs = [
            LLMResult(generations=[res.generations], llm_output=res.llm_output)  # type: ignore[list-item]
            for res in results
        ]
        llm_output = self._combine_llm_outputs([res.llm_output for res in results])
        generations = [res.generations for res in results]
        output = LLMResult(generations=generations, llm_output=llm_output)  # type: ignore[arg-type]
        if run_managers:
            run_infos = []
            for manager, flattened_output in zip(run_managers, flattened_outputs):
                manager.on_llm_end(flattened_output)
                run_infos.append(RunInfo(run_id=manager.run_id))
            output.run = run_infos
        return output
```

### 3.3 _generate_with_cache
generate çš„è°ƒç”¨è¿‡ç¨‹åˆ†æˆäº†å¦‚ä¸‹å‡ ä¸ªéƒ¨åˆ†:
1. æ£€ç´¢ç¼“å­˜ï¼Œé™æµ(rate_limiter)ï¼Œè¿™ä¸€éƒ¨åˆ†è¯¦è§ ![Cache And RateLimit](./32_cache_memory.md)
3. æ£€æŸ¥æ˜¯ä¸æ˜¯åº”è¯¥ä½¿ç”¨ stream æ–¹æ³•è·å–ç»“æœï¼Œå¦‚æœæ˜¯è°ƒç”¨ `self._stream`ï¼Œå¦åˆ™è°ƒç”¨ `self._generate`

```python
    def _generate_with_cache(
        self,
        messages: list[BaseMessage],
        stop: Optional[list[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> ChatResult:
        llm_cache = self.cache if isinstance(self.cache, BaseCache) else get_llm_cache()
        # We should check the cache unless it's explicitly set to False
        # A None cache means we should use the default global cache
        # if it's configured.
        check_cache = self.cache or self.cache is None
        if check_cache:
            if llm_cache:
                llm_string = self._get_llm_string(stop=stop, **kwargs)
                prompt = dumps(messages)
                cache_val = llm_cache.lookup(prompt, llm_string)
                if isinstance(cache_val, list):
                    return ChatResult(generations=cache_val)
            elif self.cache is None:
                pass
            else:
                msg = "Asked to cache, but no cache found at `langchain.cache`."
                raise ValueError(msg)

        # Apply the rate limiter after checking the cache, since
        # we usually don't want to rate limit cache lookups, but
        # we do want to rate limit API requests.
        if self.rate_limiter:
            self.rate_limiter.acquire(blocking=True)

        # If stream is not explicitly set, check if implicitly requested by
        # astream_events() or astream_log(). Bail out if _stream not implemented
        if self._should_stream(
            async_api=False,
            run_manager=run_manager,
            **kwargs,
        ):
            chunks: list[ChatGenerationChunk] = []
            for chunk in self._stream(messages, stop=stop, **kwargs):
                chunk.message.response_metadata = _gen_info_and_msg_metadata(chunk)
                if run_manager:
                    if chunk.message.id is None:
                        chunk.message.id = f"{_LC_ID_PREFIX}-{run_manager.run_id}"
                    run_manager.on_llm_new_token(
                        cast("str", chunk.message.content), chunk=chunk
                    )
                chunks.append(chunk)
            result = generate_from_stream(iter(chunks))
        elif inspect.signature(self._generate).parameters.get("run_manager"):
            result = self._generate(
                messages, stop=stop, run_manager=run_manager, **kwargs
            )
        else:
            result = self._generate(messages, stop=stop, **kwargs)

        # Add response metadata to each generation
        for idx, generation in enumerate(result.generations):
            if run_manager and generation.message.id is None:
                generation.message.id = f"{_LC_ID_PREFIX}-{run_manager.run_id}-{idx}"
            generation.message.response_metadata = _gen_info_and_msg_metadata(
                generation
            )
        if len(result.generations) == 1 and result.llm_output is not None:
            result.generations[0].message.response_metadata = {
                **result.llm_output,
                **result.generations[0].message.response_metadata,
            }
        if check_cache and llm_cache:
            llm_cache.update(prompt, llm_string, result.generations)
        return result
```

### 4.3 SimpleChatModel
SimpleChatModel å‡è®¾æ¨¡å‹åªè¿”å› strï¼Œå¹¶ç»™äº†ä¸€ä¸ª _generate çš„é»˜è®¤å®ç°ã€‚

```python
class SimpleChatModel(BaseChatModel):
    """Simplified implementation for a chat model to inherit from.

    **Note** This implementation is primarily here for backwards compatibility.
        For new implementations, please use `BaseChatModel` directly.
    """

    def _generate(
        self,
        messages: list[BaseMessage],
        stop: Optional[list[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> ChatResult:
        output_str = self._call(messages, stop=stop, run_manager=run_manager, **kwargs)
        message = AIMessage(content=output_str)
        generation = ChatGeneration(message=message)
        return ChatResult(generations=[generation])

    @abstractmethod
    def _call(
        self,
        messages: list[BaseMessage],
        stop: Optional[list[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        """Simpler interface."""
```

## 4. BaseLLM 
### 4.1 invoke
BaseLLM invoke éœ€è¦å…³æ³¨ä»¥ä¸‹å®ç°ç»†èŠ‚:
1. invoke å†…è°ƒç”¨ `self._convert_input` å°†è¾“å…¥æ ‡å‡†åŒ–ä¸º `list[PromptValue]`
2. generate_prompt å†…å°† `list[PromptValue]` è½¬åŒ–ä¸º `list[str]` è°ƒç”¨ generate
3. generate å®Œæˆå¤šä¸ªç‹¬ç«‹çš„ Prompt è°ƒç”¨è¿‡ç¨‹

```python
    @override
    def invoke(
        self,
        input: LanguageModelInput,
        config: Optional[RunnableConfig] = None,
        *,
        stop: Optional[list[str]] = None,
        **kwargs: Any,
    ) -> str:
        config = ensure_config(config)
        return (
            self.generate_prompt(
                [self._convert_input(input)],
                stop=stop,
                callbacks=config.get("callbacks"),
                tags=config.get("tags"),
                metadata=config.get("metadata"),
                run_name=config.get("run_name"),
                run_id=config.pop("run_id", None),
                **kwargs,
            )
            .generations[0][0]
            .text
        )

    @override
    def generate_prompt(
        self,
        prompts: list[PromptValue],
        stop: Optional[list[str]] = None,
        callbacks: Optional[Union[Callbacks, list[Callbacks]]] = None,
        **kwargs: Any,
    ) -> LLMResult:
        prompt_strings = [p.to_string() for p in prompts]
        return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
```

### 4.2 generate
generate:
1. å¦‚æœ callbacks æ˜¯å¤šä¸ªï¼Œé‚£ä¹ˆæ£€æŸ¥ callbacksã€tagsã€metadataã€run_name è¿™äº›å…ƒæ•°æ®å‚æ•°ï¼Œæ˜¯å¦ä¸ prompts æ•°é‡æ˜¯å¦ç›¸ç­‰ï¼Œå¹¶ä¸ºæ¯ä¸€ä¸ª prompt æ„é€ ä¸€ä¸ª CallbackManager
2. å¦‚æœ callbacks æ˜¯ä¸€ä¸ªæ„é€ ä¸€ä¸ª CallbackManager å¤šä¸ªå‰¯æœ¬
3. è°ƒç”¨ get_prompts å‡½æ•°ï¼Œä» cache ä¸­æ£€ç´¢
4. å¦‚æœæ²¡æœ‰ cache å¯¹æ‰€æœ‰è¾“å…¥ promts è°ƒç”¨ `_generate_helper`
5. æœ‰ç¼“å­˜ï¼Œå¯¹æœªå‘½ä¸­ç¼“å­˜è°ƒç”¨ `_generate_helper`
6. åˆå¹¶ç»“æœè¾“å‡º


```python
def generate(
        self,
        prompts: list[str],
        stop: Optional[list[str]] = None,
        callbacks: Optional[Union[Callbacks, list[Callbacks]]] = None,
        *,
        tags: Optional[Union[list[str], list[list[str]]]] = None,
        metadata: Optional[Union[dict[str, Any], list[dict[str, Any]]]] = None,
        run_name: Optional[Union[str, list[str]]] = None,
        run_id: Optional[Union[uuid.UUID, list[Optional[uuid.UUID]]]] = None,
        **kwargs: Any,
    ) -> LLMResult:
        """Pass a sequence of prompts to a model and return generations.

        This method should make use of batched calls for models that expose a batched
        API.

        Use this method when you want to:
            1. take advantage of batched calls,
            2. need more output from the model than just the top generated value,
            3. are building chains that are agnostic to the underlying language model
                type (e.g., pure text completion models vs chat models).

        Args:
            prompts: List of string prompts.
            stop: Stop words to use when generating. Model output is cut off at the
                first occurrence of any of these substrings.
            callbacks: Callbacks to pass through. Used for executing additional
                functionality, such as logging or streaming, throughout generation.
            tags: List of tags to associate with each prompt. If provided, the length
                of the list must match the length of the prompts list.
            metadata: List of metadata dictionaries to associate with each prompt. If
                provided, the length of the list must match the length of the prompts
                list.
            run_name: List of run names to associate with each prompt. If provided, the
                length of the list must match the length of the prompts list.
            run_id: List of run IDs to associate with each prompt. If provided, the
                length of the list must match the length of the prompts list.
            **kwargs: Arbitrary additional keyword arguments. These are usually passed
                to the model provider API call.

        Returns:
            An LLMResult, which contains a list of candidate Generations for each input
                prompt and additional model provider-specific output.
        """
        if not isinstance(prompts, list):
            msg = (
                "Argument 'prompts' is expected to be of type list[str], received"
                f" argument of type {type(prompts)}."
            )
            raise ValueError(msg)  # noqa: TRY004
        # Create callback managers
        if isinstance(metadata, list):
            metadata = [
                {
                    **(meta or {}),
                    **self._get_ls_params(stop=stop, **kwargs),
                }
                for meta in metadata
            ]
        elif isinstance(metadata, dict):
            metadata = {
                **(metadata or {}),
                **self._get_ls_params(stop=stop, **kwargs),
            }
        if (
            isinstance(callbacks, list)
            and callbacks
            and (
                isinstance(callbacks[0], (list, BaseCallbackManager))
                or callbacks[0] is None
            )
        ):
            # We've received a list of callbacks args to apply to each input
            if len(callbacks) != len(prompts):
                msg = "callbacks must be the same length as prompts"
                raise ValueError(msg)
            if tags is not None and not (
                isinstance(tags, list) and len(tags) == len(prompts)
            ):
                msg = "tags must be a list of the same length as prompts"
                raise ValueError(msg)
            if metadata is not None and not (
                isinstance(metadata, list) and len(metadata) == len(prompts)
            ):
                msg = "metadata must be a list of the same length as prompts"
                raise ValueError(msg)
            if run_name is not None and not (
                isinstance(run_name, list) and len(run_name) == len(prompts)
            ):
                msg = "run_name must be a list of the same length as prompts"
                raise ValueError(msg)
            callbacks = cast("list[Callbacks]", callbacks)
            tags_list = cast(
                "list[Optional[list[str]]]", tags or ([None] * len(prompts))
            )
            metadata_list = cast(
                "list[Optional[dict[str, Any]]]", metadata or ([{}] * len(prompts))
            )
            run_name_list = run_name or cast(
                "list[Optional[str]]", ([None] * len(prompts))
            )
            callback_managers = [
                CallbackManager.configure(
                    callback,
                    self.callbacks,
                    self.verbose,
                    tag,
                    self.tags,
                    meta,
                    self.metadata,
                )
                for callback, tag, meta in zip(callbacks, tags_list, metadata_list)
            ]
        else:
            # We've received a single callbacks arg to apply to all inputs
            callback_managers = [
                CallbackManager.configure(
                    cast("Callbacks", callbacks),
                    self.callbacks,
                    self.verbose,
                    cast("list[str]", tags),
                    self.tags,
                    cast("dict[str, Any]", metadata),
                    self.metadata,
                )
            ] * len(prompts)
            run_name_list = [cast("Optional[str]", run_name)] * len(prompts)
        run_ids_list = self._get_run_ids_list(run_id, prompts)
        params = self.dict()
        params["stop"] = stop
        options = {"stop": stop}
        (
            existing_prompts,
            llm_string,
            missing_prompt_idxs,
            missing_prompts,
        ) = get_prompts(params, prompts, self.cache)
        new_arg_supported = inspect.signature(self._generate).parameters.get(
            "run_manager"
        )
        if (self.cache is None and get_llm_cache() is None) or self.cache is False:
            run_managers = [
                callback_manager.on_llm_start(
                    self._serialized,
                    [prompt],
                    invocation_params=params,
                    options=options,
                    name=run_name,
                    batch_size=len(prompts),
                    run_id=run_id_,
                )[0]
                for callback_manager, prompt, run_name, run_id_ in zip(
                    callback_managers, prompts, run_name_list, run_ids_list
                )
            ]
            return self._generate_helper(
                prompts,
                stop,
                run_managers,
                new_arg_supported=bool(new_arg_supported),
                **kwargs,
            )
        if len(missing_prompts) > 0:
            run_managers = [
                callback_managers[idx].on_llm_start(
                    self._serialized,
                    [prompts[idx]],
                    invocation_params=params,
                    options=options,
                    name=run_name_list[idx],
                    batch_size=len(missing_prompts),
                )[0]
                for idx in missing_prompt_idxs
            ]
            new_results = self._generate_helper(
                missing_prompts,
                stop,
                run_managers,
                new_arg_supported=bool(new_arg_supported),
                **kwargs,
            )
            llm_output = update_cache(
                self.cache,
                existing_prompts,
                llm_string,
                missing_prompt_idxs,
                new_results,
                prompts,
            )
            run_info = (
                [RunInfo(run_id=run_manager.run_id) for run_manager in run_managers]
                if run_managers
                else None
            )
        else:
            llm_output = {}
            run_info = None
        generations = [existing_prompts[i] for i in range(len(prompts))]
        return LLMResult(generations=generations, llm_output=llm_output, run=run_info)
```

### 4.3 _generate_helper

_generate_helper å®ç°æ¯”è¾ƒç®€å•ï¼Œå”¯ä¸€éœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒBaseLLM çš„ `_generate` æ–¹æ³•ä¸ BaseChatModel æœ‰å¤šåŒºåˆ«:
1. `BaseLLM._generate` å¤„ç†çš„æ˜¯å¤šä¸ªç‹¬ç«‹ promptï¼Œæ‰€ä»¥ `_generate_helper` å¯¹ç»“æœè¿›è¡Œäº†å±•å¼€
2. `BaseChatModel._generate` åªå¤„ç†ä¸€ä¸ª promptï¼Œè¿™ä¸ª prompt åŒ…å«å¤šä¸ª message

```python
    def _generate_helper(
        self,
        prompts: list[str],
        stop: Optional[list[str]],
        run_managers: list[CallbackManagerForLLMRun],
        *,
        new_arg_supported: bool,
        **kwargs: Any,
    ) -> LLMResult:
        try:
            output = (
                self._generate(
                    prompts,
                    stop=stop,
                    # TODO: support multiple run managers
                    run_manager=run_managers[0] if run_managers else None,
                    **kwargs,
                )
                if new_arg_supported
                else self._generate(prompts, stop=stop)
            )
        except BaseException as e:
            for run_manager in run_managers:
                run_manager.on_llm_error(e, response=LLMResult(generations=[]))
            raise
        flattened_outputs = output.flatten()
        for manager, flattened_output in zip(run_managers, flattened_outputs):
            manager.on_llm_end(flattened_output)
        if run_managers:
            output.run = [
                RunInfo(run_id=run_manager.run_id) for run_manager in run_managers
            ]
        return output
```

### 4.4 LLM 
åœ¨ BaseLLM çš„åŸºç¡€ä¸Š langchain-core è¿˜å®ç°äº†ä¸€ä¸ª LLM ç±»ã€‚è¿™ä¸ªç±»å¯¹ BaseLLM åšäº†ä¸€ä¸ªç®€å•çš„æ¥å£è½¬æ¢ã€‚

```python
class LLM(BaseLLM):
    def _generate(
        self,
        prompts: list[str],
        stop: Optional[list[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> LLMResult:
        """Run the LLM on the given prompt and input."""
        # TODO: add caching here.
        generations = []
        new_arg_supported = inspect.signature(self._call).parameters.get("run_manager")
        for prompt in prompts:
            text = (
                self._call(prompt, stop=stop, run_manager=run_manager, **kwargs)
                if new_arg_supported
                else self._call(prompt, stop=stop, **kwargs)
            )
            generations.append([Generation(text=text)])
        return LLMResult(generations=generations)
```

### 5. Fake Model
æœ‰å…³ ChatGpt å…·ä½“çš„ Modelå®ç°åœ¨ langchain åº“å†…ï¼Œæˆ‘ä»¬ä¼šåœ¨å•ç‹¬çš„ç« èŠ‚é‡Œè¯¦ç»†è®²è§£ã€‚è¿™é‡Œæˆ‘ä»¬çœ‹ä¸€ä¸‹ langchain-core ç»™æˆ‘ä»¬æä¾›çš„æµ‹è¯•ç”¨ FakeModel ç±»ã€‚

FakeModel æœ‰å¦‚ä¸‹å‡ ä¸ª:
1. FakeListLLM
2. FakeStreamingListLLM
3. FakeMessagesListChatModel
4. FakeListChatModel
5. FakeChatModel
6. GenericFakeChatModel
7. ParrotFakeChatModel

è¿™äº› model éƒ½æ¯”è¾ƒç®€å•ï¼Œä¸åœ¨ä¸€ä¸€åˆ—å‡ºä»£ç ã€‚