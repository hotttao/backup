---
title: 5 主题和分区
date: 2020-04-05
categories:
    - 存储
tags:
    - Kafka
---
主题和分区
<!-- more -->

## 1. 主题与分区概述
主题是消息的集合，分区算是消息的二次分类。分区的划分不仅为Kafka提供了可伸缩性、水平扩展的功能，还通过多副本机制来为Kafka提供数据冗余以提高数据可靠性。

从底层来说，主题和分区都是逻辑概念。分区可以有一至多个副本，每个副本对应一个日志文件，每个日志文件对应一至多个日志分段（LogSegment），每个日志分段还可以细分为索引文件、日志存储文件和快照文件等。有关 Kafka 日志存储的内容我们将在下一节详述。

本节我们将聚焦于 Kafka 主题和分区的管理上，内容包括:
1. 主题的管理；
2. 初识KafkaAdminClient；
3. 分区的管理；
    - 优先副本的选举
    - 分区重分配
    - 复制限流
    - 修改副本因子

## 2. 主题的管理
主题的管理包括主题的增删改查。通过 Kafka提供的 kafka-topics.sh 脚本来执行这些操作，这个脚本位于$KAFKA_HOME/bin/目录下。其实质上是调用了kafka.admin.TopicCommand类来执行主题管理的操作。

除了 kafka-topics.sh 脚本，我们还可以通过KafkaAdminClient，这种方式实质上是通过发送 CreateTopicsRequest、DeleteTopicsRequest 等请求来实现。甚至我们还可以通过直接操纵日志文件和ZooKeeper节点来实现。

kafka 提供了如下命令行工具:

|序号|	脚本	|功能|
|:---|:---|:---|
|3	|kafka-topics.sh|topic管理脚本|
|3	|kafka-configs.sh|配置管理，包括主题，broker，客户端，用户|
|1	|kafka-perferred-replica-election.sh|对分区leader副本进行重新平衡|
|1	|kafka-reassign-partitions.sh|分区重分配/复制限流/修改副本因子|
|1	|kafka-server-start.sh|启动kafka服务|
|2	|kafka-server-stop.sh|停止kafka服务|
|4	|kafka-console-producer.sh|kafka生产者控制台|
|5	|kafka-replay-log-producer.sh|消费topic数据并转发到另外一个topic|
|5	|kafka-console-consumer.sh|kafka消费者控制台|
|7	|kafka-simple-consumer-shell.sh|获取指定consumer group的位移信息|
|8	|kafka-consumer-groups.sh|kafka消费者组相关信息|
|9	|kafka-producer-perf-test.sh|kafka生产者性能测试脚本|
|10|	kafka-consumer-perf-test.sh|kafka消费者性能测试脚本|
|11|	kafka-verifiable-consumer.sh|检验的kafka消费者|
|12|	kafka-verifiable-producer.sh|检验的kafka生产者|

#### kafka-topic.sh 命令
bin/kafka-topics.sh 
- 通用参数:
    --zookeeper: Zookeeper 连接地址，eg: node01:2181/kafka
    --topic: 主题名称 
- --create: 主题创建指令
    --partitions: 指定分区数
    --replication-factor: 指定副本数
    --replica-assignment: 手动指定分区副本的分配方案
    --config: 设置主题的配置参数
    --if-not-exists
    --broker.rack: 指定机架信息，可以让副本尽量分配到不同机架上的机器上
    --disable-rack-aware: 忽略机架信息
- --list
- --describe: 查看分区副本的分配细节
    - topics-with-overrides: 只会列出包含了与集群不一样配置的主题
    - under-replicated-partitions: 找出所有包含失效副本的分区
    - unavailable-partitions: 查看主题中没有 leader 副本的分区
- --alter
- --delete

### 2.1 创建主题
#### 主题的默认创建
当 `auto.create.topics.enable=true`，出现以下情况时会默认创建新的主题:
1. 生产者向一个尚未创建的主题发送消息时
2. 消费者开始从未知主题中读取消息时
3. 任意一个客户端向未知主题发送元数据请求时

而下面参数用于设置主题的分区和副本数:
- `num.partitions`: 默认分区数，默认值为 1
- `default.replication.factor`: 默认副本数，默认值为 1

自动创建主题的行为都是非预期的，通常 `auto.create.topics.enable` 都需要设置为 false。推荐的方式是使用 kafka-topics.sh

#### kafka-topics.sh 创建主题
kafka-topics.sh脚本在创建主题时还会检测是否包含`.`或`_` 字符。为什么要检测这两个字符呢？因为在Kafka的内部做埋点时会根据主题的名称来命名metrics的名称，并且会将点号“.”改成下画线“_”。假设遇到一个名称为“topic.1_2”的主题，还有一个名称为“topic_1.2”的主题，那么最后的metrics的名称都会为“topic_1_2”，这样就发生了名称冲突。

主题的命名同样不推荐（虽然可以这样做）使用双下画线`__`开头，因为以双下画线开头的主题一般看作Kafka的内部主题。

主题的名称必须由大小写字母、数字、点号“.”、连接线“-”、下画线“_”组成，不能为空，不能只有点号“.”，也不能只有双点号“..”，且长度不能超过249。

创建主题的命令示例如下:

```bash
# 1. 使用默认参数创建
bin/kafka-topics.sh --zookeeper node01:2181 --create --topic t_cdr --partitions 30  --replication-factor 2

# 2. 指定分区副本分配方案

# 3. 指定主题配置参数
```

#### 主题、分区、部分、日志之间的关系

在执行完 create 主题创建指令后，Kafka会在 log.dir 或 log.dirs 参数所配置的目录下创建相应的主题分区。

```
log.dir
    <topic>-<partition> # 文件夹
        .log            # 日志分段
        .index
        .timeindex
        ...
```

主题、分区、副本和 Log（日志）的关系入下图所示

![副本日志对应关系](/images/kafka/kafka_log.png)

主题和分区都是提供给上层用户的抽象，而在副本层面或更加确切地说是Log层面才有实际物理上的存在。

### 2.2 分区副本分配
在生产者和消费者中也都有分区分配的概念:
1. 生产者的分区分配是指为每条消息指定其所要发往的分区
2. 消费者中的分区分配是指为消费者指定其可以消费消息的分区

这里的分区分配是指为集群制定创建主题时的分区副本分配方案，即在哪个broker中创建哪些分区的副本。

kafka-topics.sh脚本创建主题时的内部分配逻辑按照机架信息划分成两种策略：未指定机架信息和指定机架信息。内部的分配逻辑还比较复杂，这里我们不在详述。

#### 主题创建时的 Zookeeper 操作
当创建一个主题时，无论通过kafka-topics.sh脚本，还是通过其他方式创建主题时，实质上是
1. 在ZooKeeper中的/brokers/topics节点下创建与该主题对应的子节点并写入分区副本分配方案
2. 并且在/config/topics/节点下创建与该主题对应的子节点并写入主题相关的配置信息（这个步骤可以省略不执行）
3. Kafka创建主题的实质性动作是交由控制器异步去完成的

### 2.3 主题查看
list和describe指令可以用来方便地查看主题信息。

```bash
# 查看所有主题
bin/kafka-topics.sh --zookeeper node01:2181 --list

# 查看指定主题信息
bin/kafka-topics.sh --zookeeper node01:2181 --describe --topic t_cdr
```

describe 指令的 under-replicated-partitions和unavailable-partitions参数都可以找出有问题的分区。

通过 under-replicated-partitions 参数可以找出所有包含失效副本的分区。包含失效副本的分区可能正在进行同步操作，也有可能同步发生异常，此时分区的ISR集合小于 AR 集合。对于通过该参数查询到的分区要重点监控，因为这很可能意味着集群中的某个broker已经失效或同步效率降低等。

通过 unavailable-partitions 参数可以查看主题中没有 leader 副本的分区，这些分区已经处于离线状态，对于外界的生产者和消费者来说处于不可用的状态。

### 2.4 修改主题
alter指令可以修改主题的配置和分区数。

#### 增加分区数

```bash
bin/kafka-topics.sh --zookeeper node01:2181  --alter --topic t_cdr --partitions 10
```

增加分区数，根据key计算分区的行为就会受到影响。如此还会影响既定消息的顺序，对于基于key计算的主题而言，建议在一开始就设置好分区数量，避免以后对其进行调整。

#### 减少分区数
目前Kafka只支持增加分区数而不支持减少分区数。为什么不支持与实现复杂度和语义保证有关。

比如删除的分区中的消息该如何处理？
1. 如果随着分区一起消失则消息的可靠性得不到保障；如果需要保留则又需要考虑如何保留
2. 直接存储到现有分区的尾部，消息的时间戳就不会递增，如此对于Spark、Flink这类需要消息时间戳（事件时间）的组件将会受到影响
3. 如果分散插入现有的分区，那么在消息量很大的时候，内部的数据复制会占用很大的资源，而且在复制期间，此主题的可用性又如何得到保障？与此同时，顺序性问题、事务性问题，以及分区和副本的状态机切换问题都是不得不面对

#### 更改配置
更改配置，使用的是与 create 相同的 --config 参数，还可以通过 --delete-config 参数来删除之前覆盖的配置，使其恢复原有的默认值。

使用kafka-topics.sh脚本的alter指令来变更主题配置的功能已经过时（deprecated），将在未来的版本中删除，推荐使用kafka-configs.sh脚本来实现相关功能。

### 2.5 删除主题
delete 指令用来删除主题。必须将delete.topic.enable参数配置为true才能够删除主题，这个参数的默认值就是true，如果配置为false，那么删除主题的操作将会被忽略。在实际生产环境中，建议将这个参数的值设置为true。

内部主题 __consumer_offsets和__transaction_state 是无法被删除的。

#### 主题删除时的 Zookeeper 操作
使用kafka-topics.sh脚本删除主题的行为本质上只是在ZooKeeper中的 **/admin/delete_topics** 路径下创建一个与待删除主题同名的节点，以此标记该主题为待删除的状态。与创建主题相同的是，真正删除主题的动作也是由Kafka的控制器负责完成的。

### 2.6 配置管理
kafka-configs.sh 脚本是专门用来对配置进行操作。其包含变更配置alter和查看配置describe这两种指令类型。

kafka-configs.sh脚本不仅可以支持操作主题相关的配置，还可以支持操作broker、用户和客户端这3个类型的配置。

kafka-configs.sh
- 操作对象指定:
    - entity-type: 指定操作配置的类型，可选值包括 topics、brokers、clients和users
    - entity-name: 指定操作配置的名称，不指定，会查看 entity-type 的所有配置
- alter: 变更指令
    - add-config: 实现配置增、改
    - delete-config: 实现配置删除，即恢复默认配置

```bash
bin/kafka-configs.sh --zookeeper localhost:2181/kafkacluster --alter --entity-type topics --entity-name topicName  --add-config 'max.message.bytes=50000000' --add-config 'flush.messages=50000'

bin/kafka-configs.sh --zookeeper localhost:2181/kafkacluster --entity-type topics --entity-name topicName --describe

```

#### 配置更改时的 Zookeeper 操作
使用kafka-configs.sh脚本来变更（alter）配置时，会在ZooKeeper中创建一个命名形式为`/config/<entity-type>/<entity-name>`的节点，，并将变更的配置写入这个节点。

变更配置时还会在ZooKeeper中的`/config/changes/`节点下创建一个以`config_change_`为前缀的持久顺序节点（PERSISTENT_SEQUENTIAL），节点命名形式可以归纳为`/config/changes/config_change_<seqNo>` seqNo是一个单调递增的10位数字的字符串，不足位则用0补齐。

查看（describe）配置时，就是从`/config/<entity-type>/<entity-name>` 节点中获取相应的数据内容。


### 2.7 主题端配置参数
与主题相关的所有配置参数在 broker 层面都有对应参数。如果没有修改过主题的任何配置参数，那么就会使用broker端的对应参数作为其默认值。

|主题端参数|描述|对应的broker端参数|
|:---|:---|:---|
|cleanup.policy	|日志压缩策略。默认值为delete，还可以配置为compact|	log.cleanup.policy|
|compression.type	|消息的压缩类型。默认值是producer，表示保留生产者中所使用的原始压缩类型。还可以配置为uncompressed、snappy、lz4、gzip	|compression.type|
|delete.retention.ms	|被标识为删除的数据能够保留多久。默认值是86400000，即1天|	log.clear.delete.retention.ms|
|file.delete.delay.ms	|清理文件之前可以等待多长时间，默认值是60000，即1分钟|	log.segment.delete.delay.ms|
|flush.messges	|需要收集多少消息才会将它们强制刷新到磁盘，默认值是Long.MAX_VALUE,即让操作系统来决定。建议不要修改此参数的默认值|	log.flush.interval.messges|
|flush.ms	|需要等待多久才会将消息强制刷新到磁盘，默认值是Long.MAX_VALUE,即让操作系统来决定。建议不要修改此参数的默认值|	log.flush.interval.ms|
|follower.replication.throttled.replicas	|用来配置被限制速率的主题所对应的follower副本列表|	follower.replication.throttled.replicas|
|index.interval.bytes	|用来控制添加索引项的频率。每超过这个参数所设置的消息字节数时就可以添加一个新的索引项，默认值是4096|	log.index.interval.bytes|
|leader.replication.throttled.replicas	|用来配置被限制速率的主题所对应的leader副本列表|	leader.replication.throttled.replicas|
|max.message.byte	|消息的最大字节数，默认值是1000012|	max.message.byte|
|message.format.version	|消息格式的版本，默认值为2.0-IV1|	log.message.format.version|
|message.timestamp.difference.max.ms	|消息中自带的时间戳与broker收到消息时的时间戳之间的最大差值，默认值为Long.MAX_VALUE。此参数只有在message.timestamp.type参数设置为CreteTime时才有效|	log.message.timestamp.difference.max.ms|
|message.timestamp.type	|消息的时间戳类型。默认值是CreteTime，还可以设置为LogAppendTime|	log.message.timestamp.type|
|min.cleanable.dirty.ratio	|日志清理时的最小污浊率，默认值是0.5|	log.cleaner.min.cleanable.ratio|
|min.compaction.lag.ms	|日志再被清理前的最小保留时间，默认值为0|	log.cleaner.min.compaction.lag.ms|
|min.insync.replicas	|分区ISR集合中至少有多少个副本，默认值为1|	min.insync.replicas|
|preallocate	|在创建日志分段的时候是否要预分配空间，默认值为false|	log.preallocate|
|retention.bytes	|分区中所能保留的消息总量，默认值为-1，即没有限制|	log.retention.bytes|
|retention.ms	|使用delete的日志清理策略时消息能够保留多长时间，默认值为604800000，即7天。如果设置为-1，则表示没有限制|	log.retention.ms|
|segment.bytes	|日志分段的最大值，默认值为1073741824，即1GB|	log.segment.bytes|
|segment.index.bytes	|日志分段索引的最大值，默认值为10485760，即10MB|	log.index.size.max.bytes|
|segment.jitter.ms	|滚动日志分段时，在segment.ms的基础之上增加的随机数，默认为0|	log.roll.jitter.ms|
|segment.ms	|最多多久滚动一次日志分段，默认值为604800000，即7天|	log.roll.ms|
|unclean.leader.election.enable	|是否可以从非ISR集合中选举leader副本，默认值为false，如果设置为true，则可能造成数据丢失|	unclean.leader.election.enable|

## 3. KafkaAdminClient

## 4. 分区的管理
分区相关的知识和操作，包括优先副本的选举、分区重分配、复制限流、修改副本因子等内容。

### 4.1 kafka 负载均衡与优先副本的选举
分区使用多副本机制来提升可靠性，但只有leader副本对外提供读写服务，而follower副本只负责在内部进行消息的同步。如果一个分区的leader副本不可用，此时Kafka就从剩余的follower副本中挑选一个新的leader副本来继续对外提供服务。

按照主从副本所在节点，可以将节点分为:
1. leader 节点: 将leader副本所在的broker节点叫作分区的leader节点
2. follower节点: follower副本所在的broker节点

虽然不够严谨，但是 leader 节点是否分布均匀决定了 kafka 负载是否均衡。

为了能够有效地治理负载失衡的情况，Kafka引入了优先副本（preferred replica）的概念:
1. 所谓的优先副本是指在 AR 集合列表中的第一个副本。
2. 理想情况下，优先副本就是该分区的leader副本 所以也可以称之为preferred leader
3. Kafka要确保所有主题的**优先副本在Kafka集群中均匀分布**，这样就保证了所有分区的leader均衡分布
4. 所谓的**优先副本的选举**是指通过一定的方式**促使优先副本选举为leader副本**，以此来促进集群的负载均衡，这

需要注意的是，优先副本选举只能保证 leader 分配均衡，但这并不意味着Kafka集群的负载均衡，kafka 集群的负载均衡与如下因素有关:
1. **分区分配均衡**
2. **leader 分配均衡**
3. **每个 leader 分区的负载**

#### 自动平衡功能
由于 Kafka 集群的 broker 节点不可避免地会遇到宕机或崩溃从而导致 kafka 失衡，因此需要自动平衡功能。与此对应的 broker 端参数是 `auto.leader.rebalance.enable`，用于控制是否启用kafka 自动平衡功能，默认为 True 启用。

如果开启分区自动平衡的功能，则 Kafka 的控制器会启动一个定时任务，这个定时任务会轮询所有的 broker节点，计算每个broker节点的分区不平衡率（broker中的不平衡率=**非优先副本的leader个数/分区总数**）是否超过leader.imbalance.per.broker.percentage参数配置的比值，默认值为 10%，如果超过设定的比值则会自动执行优先副本的选举动作以求分区平衡。执行周期由参数 **leader.imbalance.check.interval.seconds** 控制，默认值为300秒，即5分钟。

不过在生产环境中不建议将auto.leader.rebalance.enable设置为默认的true。因为自动均衡会引起客户端一定时间的阻塞，可能在业务关键时期造成重大影响，而且是非预期不可控的。对于 kafka 集群一定的负载不均衡是可以接受的，重要的不平衡的程序，需要针对此类相关的埋点指标设置相应的告警，并超过预警时手动执行分区平衡。

#### 手动均衡
`kafka-perferred-replica-election.sh` 脚本提供了对分区leader副本进行重新平衡的功能(即执行优先副本的选举，保证所有的优先副本就是该分区的 leader副本)。优先副本的选举过程是一个安全的过程，Kafka客户端可以自动感知分区leader副本的变更。

```bash
bin/kafka-perferred-replica-election.sh --zookeeper localhost:2181/kafka
```
上面示例中的这种使用方式会将集群上所有的分区都执行一遍优先副本的选举操作，leader 副本的转移也是一项高成本的工作，如果执行的分区数很多，势必会造成影响，如果集群包含大量分区，则可能失效。

#### 优先副本选举过程
在优先副本的选举过程中，具体的元数据信息会被存入 ZooKeeper的 **/admin/preferred_replica_election** 节点，如果这些数据超过了ZooKeeper节点所允许的大小，那么选举就会失败。默认情况下ZooKeeper所允许的节点数据大小为1MB。生产环境中一般分批、手动地执行优先副本的选举操作。同时，优先副本的选举操作也要注意避开业务高峰期，以免带来性能方面的负面影响。

kafka-perferred-replica-election.sh脚本中还提供了path-to-json-file参数来小批量地对部分分区执行优先副本的选举操作。通过path-to-json-file参数来指定一个JSON文件，这个JSON文件里保存需要执行优先副本选举的分区清单。

```bash
cat election.json
{
    "partitions":[
        {
            "partition": 0,
            "topic": "topicName"   
        },
        {
            "partition": 1,
            "topic": "topicName"   
        },
        {
            "partition": 2,
            "topic": "topicName"   
        }
    ]
}

bin/kafka-perferred-replica-election.sh --zookeeper localhost:2181/kafka \
                --path-to-json-file election.json
```

### 4.2 分区重分配
当集群中的一个节点突然宕机下线时，kafka 会自动对此节点上的 leader 副本转交到集群的其他follower副本中。但 Kafka 并不会将这些失效的分区副本自动地迁移到集群中剩余的可用broker节点上，因此需要手动进行分区副本的迁移。

当集群中新增broker节点时，只有新创建的主题分区才有可能被分配到这个节点上，而之前的主题分区并不会自动分配到新加入的节点中。

为了解决上述问题，需要让分区副本再次进行合理的分配，也就是所谓的分区重分配。Kafka提供了 `kafka-reassign-partitions.sh` 脚本来执行分区重分配的工作，它可以在集群扩容、broker节点失效的场景下对分区进行迁移。优先副本选举就属于分区重分配。

#### 分区重分配的执行
kafka-reassign-partitions.sh 脚本的使用分为 3 个步骤：
1. 首先创建需要一个包含主题清单的JSON 文件
2. 其次根据主题清单和 broker 节点清单生成一份重分配方案
3. 最后根据这份方案执行具体的重分配动作

```bash
# 1. 创建一个JSON文件（resign.json），文件内容为要进行分区重分配的主题清单
cat resign.json
{
    "topics": [
        {
            "topic": "topic-ressign"
        }
    ],
    "version": 1
}

# 2. 生成重分配方案
bin/kafka-reassign-partitions.sh  --zookeeper localhost:2181/kafka \
    --generate \                               # 一种指令类型
    --topics-to-move-json-file resign.json \   # 执行要执行分区重分配的主题
    --broker-list 0,2                          # 指定要分配的节点列表
 # 输出
 Current partition replica assignment...       # 
 {}                                            # 当前的分区副本分配情况，需要备份，以便后续的回滚操作

 Proposed partition reassignment configuration # 
 {}                                            # 所对应的JSON 内容为重分配的候选方案，将其保存为 project.json

# 3. 执行分区重分配
bin/kafka-reassign-partitions.sh  --zookeeper localhost:2181/kafka \
    --execute  # 指令类型，表示执行分区重分配
    --reassignment-json-file project.json # 指定分区重分配方案的路径

# 4. 查看分区重分配的的进度
bin/kafka-reassign-partitions.sh  --zookeeper localhost:2181/kafka \
    --verify  # 指令类型，查看分区重分配的进度
    --reassignment-json-file project.json # 指定分区重分配方案的路径
```

对于分区重分配而言，这里还有可选的第四步操作，即验证查看分区重分配的进度。只需将上面的execute替换为verify即可。


#### 分区重分配的基本原理
分区重分配的基本原理是先通过控制器为每个分区添加新副本（增加副本因子），新的副本将从分区的leader副本那里复制。在复制完成之后，控制器将旧副本从副本清单里移除（恢复为原先的副本因子数）。

分区重分配对集群的性能有很大的影响，需要占用额外的资源，比如网络和磁盘。在实际操作中，我们将**降低重分配的粒度，分成多个小批次来执行**，以此来将负面的影响降到最低，这一点和优先副本的选举有异曲同工之妙。

还需要注意的是，如果要将某个broker下线，那么在执行分区重分配动作之前最好先关闭或重启broker。这样这个broker就不再是任何分区的leader节点了，它的分区就可以被分配给集群中的其他broker。这样可以减少broker间的流量复制，以此提升重分配的性能，以及减少对集群的影响。

###  4.3 复制限流
分区重分配本质在于数据复制，先增加新的副本，然后进行数据同步，最后删除旧的副本来达到最终的目的。数据复制会占用额外的资源，这时就需要有一个限流的机制，可以对副本间的复制流量加以限制来保证重分配期间整体服务不会受太大的影响。

副本间的复制限流有两种实现方式：kafka-config.sh脚本和kafka-reassign-partitions.sh脚本。

#### 通过 kafka-configs.sh 配置复制限流 
kafka-config.sh脚本主要以动态配置的方式来达到限流的目的，在broker级别有两个与复制限流相关的配置参数：`
1. follower.replication.throttled.rate`: 设置follower副本复制的速度，单位都是B/s
2. `leader.replication.throttled.rate`: 设置leader副本传输的速度，单位都是B/s

在主题级别也有两个相关的参数来限制复制的速度：`leader.replication.throttled.replicas` 和 `follower.replication.throttled.replicas`，它们分别用来配置被限制速度的主题所对应的leader副本列表和follower副本列表。

```bash
# 对 broker 设置复制限流
bin/kafka-configs.sh --zookeeper localhost:2181/kafka \
    --entity-type broker   \
    --entity-name 1        \
    --alter                \
    --add-config follower.replication.throttled.rate=1024,leader.replication.throttled.rate=1024

# 对主题设置复制限流
bin/kafka-configs.sh --zookeeper localhost:2181/kafka \
    --entity-type topics   \
    --entity-name topic-throttle        \
    --alter                \
    --add-config leader.replication.throttled.replicas=[0:0,1:1,2:2], \
                  follower.replication.throttled.replicas=[0:1, 2:2, 2:0]
```

接下来我们看一个带限流的分区重分配方案。因为分区重分配会引起某个分区AR集合的变更，那么这个分区中与leader有关的限制会应用于重分配前的所有副本，因为任何一个副本都可能是leader，而与follower有关的限制会应用于所有移动的目的地。我们举一个例子。

首先看一下重分配前和分配后的分区副本布局对比，这里我们假设有 3 个 broker，一个主题有 3 个分区，每个分区两个副本。现在要将 broker 下架。

```bash
partition 重分配前的AR 重分配之后的AR 
  0          0,1           0,2
  1          1,2           0,2
  2          2,0           0,2
```

对上面的布局对比而言，分区0重分配的AR为`[0，1]`，重分配后的AR为`[0，2]`，那么这里的目的地就是新增的2。也就是说，对分区0而言，`leader.replication.throttled.replicas`配置为`[0：0，0：1]`，`follower.replication.throttled.replicas` 配置为`[0：2]`。分区 leader和 follower 则相应为 `[1:1,1:2]`, `[1:0]` 接下来我们就可以执行具体操作:

```bash
# 1. 为主题设置复制限流
bin/kafka-configs.sh --zookeeper localhost:2181/kafka \
    --entity-type topics   \
    --entity-name topic-throttle        \
    --alter                \
    --add-config leader.replication.throttled.replicas=[1:1,1:2,0:0,0:1], \
                  follower.replication.throttled.replicas=[0:2, 1:0]
# 2. 设置 broker2 的复制限流
bin/kafka-configs.sh --zookeeper localhost:2181/kafka \
    --entity-type broker   \
    --entity-name 2        \
    --alter                \
    --add-config follower.replication.throttled.rate=10,leader.replication.throttled.rate=10

# 3. 执行正常的分区重分配
bin/kafka-reassign-partitions.sh  --zookeeper localhost:2181/kafka \
    --execute  # 指令类型，表示执行分区重分配
    --reassignment-json-file project.json # 指定分区重分配方案的路径

# 4. 查看分区重分配的进度
bin/kafka-reassign-partitions.sh  --zookeeper localhost:2181/kafka \
    --verify  # 指令类型，查看分区重分配的进度
    --reassignment-json-file project.json # 指定分区重分配方案的路径
```

为了不影响Kafka本身的性能，往往对临时设置的一些限制性的配置在使用完后要及时删除。

#### kafka-reassign-partitions.sh 配置复制限流
kafka-reassign-partitions.sh脚本本身也提供了限流的功能，只需一个throttle参数即可。

```bash
# 3. 执行正常的分区重分配，同时限流
bin/kafka-reassign-partitions.sh  --zookeeper localhost:2181/kafka \
    --execute  # 指令类型，表示执行分区重分配
    --reassignment-json-file project.json # 指定分区重分配方案的路径
    --throttle 10
```
需要特别注意的是，使用这种方式的限流同样需要显式地**在重分配完成之后手动删除限流的设置**。如果想在重分配期间修改限制来增加吞吐量，以便完成得更快，则可以重新运行 kafka-reassign-partitions.sh脚本的execute命令，只需更改 throttle 的值即可。

kafka-reassign-partitions.sh脚本提供的限流功能背后的实现原理就是配置与限流相关的那4个参数而已，没有什么太大的差别。不过使用 kafka-config.sh 脚本的方式来实现复制限流的功能比较烦琐，并且在手动配置限流副本列表时也比较容易出错，推荐大家使用kafka-reassign-partitions.sh脚本配合throttle参数的方式，方便快捷且不容易出错。

### 4.4 修改副本因子
kafka-reassign-partition.sh 还可以实现修改副本因子的功能。修改副本因子与分区重分配有一样，首先需要准备一份带修改的主题清单:

```bash
cat project,json
{
    "topics": [
        {
            "topic": "topic-ressign",
            "partition": 1,
            "replicas": [ # 指定副本数
                0,  # brokerid，表示副本所在的节点
                1,  # 增加的
                2
            ],
            "log_dirs": [ # 指定副本所在的日志目录
                "any",    # any 表示使用默认配置
                "any",    # 增加的
                "any"
            ]
        }
    ],
    "version": 1
}
```

然后执行增加副本的操作:

```bash
bin/kafka-reassign-partitions.sh  --zookeeper localhost:2181/kafka \
    --execute  # 指令类型，表示执行分区重分配
    --reassignment-json-file project.json # 指定分区重分配方案的路径
```

在配置文件中我们需要决定，每个分区的副本所在的节点，如果在众多 broker 中进行合理的分配是一个关键的问题。我们可以将我们的分配策略写成程序自动完成。

### 4.4 分区数选择
分区数的选择没有固定答案，要根据实际的业务场景、软件条件、硬件条件、负载情况等来做具体的考量，并结合测试来最终决定。

#### 性能测试工具
Kafka 提供了如下测试工具:
1. kafka-producer-perf-test.sh: 用于生产者性能测试
2. kafka-consumer-perf-test.sh: 用于消费者性能测试

#### 分区数越大越好么？
消息中间件的性能一般是指吞吐量（广义来说还包括延迟）。

分区是Kafka 中最小的并行操作单元，对生产者而言，每一个分区的数据写入是完全可以并行化的；对消费者而言，Kafka 只允许单个分区中的消息被一个消费者线程消费，一个消费组的消费并行度完全依赖于所消费的分区数。看上去好像分区数越多，吞吐量越大。

分区数不是越多越好，大量分区会导致下面这些问题:
1. 分区进行 leader 角色切换的过程会变得不可用，如果集群某个 broker 节点宕机，那么会有大量分区同时进行 leader 角色切换，就会耗费可观的时间，并且在这个时间窗口内这些分区也会变得不可用。
2. 让 kafka 启动和关闭变得更慢
3. 增加日志清理的耗时，而且在被删除时也会耗费更多的时间

从吞吐量方面考虑，增加合适的分区数可以在一定程度上提升整体吞吐量，但超过对应的阈值之后吞吐量不升反降。如果应用对吞吐量有一定程度上的要求，则建议在投入生产环境之前对同款硬件资源做一个完备的吞吐量相关的测试，以找到合适的分区数阈值区间。

抛开硬件资源的影响，消息写入的吞吐量还会受到消息大小、消息压缩方式、消息发送方式（同步/异步）、消息确认类型（acks）、副本因子等参数的影响，消息消费的吞吐量还会受到应用逻辑处理速度的影响。

在设定完分区数，或者更确切地说是创建主题之后，还要对其追踪、监控、调优以求更好地利用它。

#### 文件描述限制
一味增加分区数并不能使吞吐量一直得到提升，并且如果分区数超过默认的配置值，还会引起kafka 奔溃。原因是 kafka 打开的文件描述符超过了系统配置的上线。对于线上环境，将文件描述符调至最大 65536 足以应对大多数情况。

文件描述符等资源限制可在 /etc/security/limits.conf 配置。也可以在/etc/profile 中通过 ulimit 命令配置。在选择合适的分区数之前，最好再考量一下当前Kafka进程中已经使用的文件描述符的个数。

#### 分区数量的修改
在创建主题之后，虽然我们还能够增加分区的个数，但基于key计算的主题需要严谨对待。如果分区的数量发生变化，那么有序性就得不到保证。在创建主题时最好能确定好分区数，尤其对于 key 高关联的应用，在创建主题时可以适当地多创建一些分区，以满足未来的需求。

有些应用场景会要求主题中的消息都能保证顺序性，这种情况下在创建主题时可以设定分区数为1，通过分区有序性的这一特性来达到主题有序性的目的。