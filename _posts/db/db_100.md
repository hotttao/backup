---
title: 12. 数据系统的未来
date: 2019-04-12
categories:
    - 分布式
tags:
    - 数据密集型应用
---

如何构建现代数据系统

<!-- more -->

## 1. 构建现代系统
前面我们讨论当前流行的技术，接下来我们综合之前所说的所有知识，来谈谈未来的系统应该是什么样子。

### 1.1 数据集成
首先每个软件，即使所谓的通用数据库，也是针对特定的使用模式而设计，第一个挑战就是弄清楚软件产品与他们适合运行环境之间的关系。

其次在复杂的应用中，数据通常以多种不同的方式使用，不太可能存在适用于所有不同环境的软件，因此我们需要数据继承。

当同样的数据的多个副本需要保存在不同的存储系统，以满足不同的访问模式需求时。我们需要弄清楚数据的输入和输出。通过变更数据捕获我们可以保证异构数据系统的一致性。

允许应用程序同时向搜索索引和数据库写入会带来问题。两个客户端同时发送冲突的写操作，两个存储系统以不同的顺序执行它们。

**如果可以通过单个系统来决定所有输入的写入顺序**，那么以相同的顺序处理写操作就可以更容易的派生出数据的其他形式。无论是使用变更数据捕获还是事件获取日志，**都不如简化总体顺序的原则重要**。

根据事件日志来更新一个派生系统通常会比较好实现，并且可以实现确定性和幂等性，也因此使系统很容易从故障中恢复。

### 1.1 派生数据与分布式事务
保证异构数据系统的一致性，经典的方法是通过分布式事务。与分布式事务相比，上面描述的派生数据系统怎么样呢？

抽象点说，它们通过不同的方式达到类似的目标:
1. 分布式事务
    - 通过使用锁机制进行互斥来决定写操作的顺序
    - 使用原子提交保证更改只生效一次
2. CDC 和事件溯源
    - 使用日志进行排序
    - 基于确定性重试和幂等保证更改只生效一次

最大的不同在于事务系统通常提供线性化，这意味着它可以保证读自己的写等一致性。派生系统通常是异步封信的，所以默认情况下无法提供类似级别的保证。

分布式事务的容错性和性能不尽如意，考虑到好的分布式事务协议未得到广泛支持，基于日志的派生数据是集成不同数据系统最有前途的方法。但是一些保证仍是非常有用的，后面我们会讨论在异步派生系统之上实现更强保证的一些方法。

## 1.2 全序的局限
非常小的系统，构建一个完全有序的事件日志是完全可行的，但是当负载增加时，瓶颈就会出现:
1. 大多数情况下，构建一个完全有序的日志需要所有事件都通过一个主节点来决定排序。如果事件吞吐量大于单节点处理上限，则需要将其分区到多台节点上，这样就是的不同分区的事件顺序变得不明确。
2. 如果服务部署在多个地理位置不同的数据中心，为了高可用，每个数据中心都有自己的主节点。这意味着来自两个不同数据中心的事件顺序不确定
3. 微服务的设计目的是将每个服务与其持久化状态作为独立单元部署，服务之间不共享持久化状态。当两个事件来自不同服务时，事件顺序不确定

从形式上讲，决定事件的全序关系被称为**全序关系广播**(性能瓶颈)，它等价于共识。大多数共识算法是针对当节点吞吐量足以处理整个事件流而设计的，这些算法不提供支持多节点共享事件排序的机制。

### 1.3 排序事件以捕获因果关系
排序事件是为了捕获因果关系，但是这个问题没有简单的答案。

## 2. 围绕数据流设计应用程序
### 2.1 应用程序代码与状态分离
现在大多数Web 应用程序都被部署为无状态服务，状态势必保存在某个地方，通常是数据库。在这种 Web应用程序模型中，数据库充当一种可以通过网络同步访问的可变共享变量。应用程序如果想知道数据的内容是否变化唯一的选择就是轮询。

从数据流的角度思考应用意味着**重新协调应用代码和状态管理之间的关系**。我们不是简单将数据库视为被应用程序所操纵的被动变量，而是更多的考虑状态、状态变化以及处理代码之间的相互作用和协调关系。应用程序代码在某个地方会触发状态变化，而在另一个地方有队状态变化做出响应。变更数据捕获和基于日志的事件流可以让我们做到这一点。

维护派生数据与异步作业执行不同:
1. 当维护派生数据时，**状态更改的顺序通常很重要**
2. 容错性是派生数据的关键，丢失单个消息都将导致派生数据集与数据源不同步。消息传递和派生状态更新都必须可靠。

对**稳定的消息排序**和**可容错的消息处理**的要求都非常严格，但是它们比分布式事务代价要小得多，并且在操作上更加稳健。现代流式处理可以对大规模环境提供排序和可靠性保证，并允许应用代码作为stream operator 运行。

订阅变化的流，而不是在需要时去查询状态，使得我们更接近电子表格那样的计算模型: 当某些数据发生更改时，依赖于此的所有派生数据都可以快速更新。

### 2.2 观察派生状态
从数据源到生成派生数据系统，我们把这个过程称为写路径。当用户访问时，所做的数据读取，我们成为读路径。写路径和读路径涵盖了数据的整个过程: 从数据收集到数据使用。读写路径在派生数据上交会，某种程度上，我们所做的系统架构设计就是在写入时需要完成的工作量和读取时需要完成的工作量之间进行的一种权衡。从这个角度看，缓存、索引、实体化视图的角色主要是调整读、写路径之间的边界。

![读路径和写路径](/images/db/read_write.png)

随着 WebSocket 的出现，使得服务器可以主动推送消息至Web应用。就我们的读写路径模型而言，主动推送状态至客户端意味着将写路径一直延伸至终端用户。因此状态变化可以通过端到端的写路径流动: 某个设备上交互行为触发了状态变化，通过事件日志，派生数据系统和流式处理等，一直到另一台设备上用户观察到状态。这种状态变化传播可以做到很低的延迟。

那为什么我们看到的大多数应用都不是这种实现方式呢。这源于目前数据库、开发框架、交互协议对**无状态客户端**和**请求/响应交互**根深蒂固的假设。许多数据存储系统的读写操作都是一个请求对应一个响应，**很少支持订阅更改**。

为了将写路径扩展到最终用户，我们需要从根本上重新思考构建这些系统的方式: 从**请求/响应**转向**发布/订阅数据流**。更具响应性的用户界面和更好的离线支持是绝对值得尝试的。

#### 读也是事件
我们讨论了当流式处理将派生数据写入存储时，以及请求查询存储时，存储充当了上述写路径和读路径之间的一种可调边界。对存储的写入是通过事件日志进行的，而读取则是即时的网络请求，查询直接路由到那些存储数据节点。这样的流程是合理的，但不是唯一的设计方案。

也可以将读请求表示为事件流，发送至流处理系统，流处理系统则将读结果发送至输出流来响应读事件。这是一种设想，原书接下来的内容已经看不太懂了。

## 3. 端到端的正确性
