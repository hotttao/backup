---
title: 10. 一致性与共识
date: 2019-04-10
categories:
    - 分布式
tags:
    - 数据密集型应用
---

一致性与共识

<!-- more -->

## 1. 共识算法的概述
本节我们将讨论构建容错式分布式系统的相关算法和协议。这里我们假设第 8 章中所有的故障都可能发生: 网络会丢失、顺序紊乱、重复发送或延迟；时钟也有一定偏差，节点可能发生暂停甚至随时崩溃。为了构建容错系统，最好先建立一套**通用的抽象机制**和**与之对应的技术保证**，这样只需实现一次，其上的各种应用程序都可以安全的信赖底层的保证。这与引入事务是一样的道理。

接下来我们将沿着这个思路，尝试建立**让分布式应用忽略内部各种问题的抽象机制**，比如**共识**，即所有的节点就某一项提议达成一致。我们将研究解决共识问题的相关算法，以及讨论**分布式系统可提供的若干保证和抽象机制**。

我们需要了解系统能力的边界，在什么情况下，系统可以容忍故障并继续工作。我们将探索分布式系统下比最终一致性更强的一致性模型(之前介绍复制的内容时，我们已经介绍了一部分)。

分布式一致性模型与之前讨论的多种事务隔离级别有相似之处，但总体上他们有着显著的不同:
1. 事务隔离主要是为了处理**并发执行事务时**的各种临界条件
2. 分布式一致性则主要是**针对延迟和故障等问题**来**协调副本之间的状态**

分布式一致性包括:
1. 写后读
2. 单调读
3. 前缀一致读
4. 线性化
5. 顺序一致性，满足线性化的写，不满足线性化的读
5. 因果一致性

前面三个我们已经介绍过了，接下来我们将讨论:
1. **线性化**: 这是最强的一致性模型
2. 分布式系统中**事件顺序问题**，特别是**因果关系**和**全局顺序**
3. 如果自动提交分布式事务，并最终解决共识问题

## 2. 可线性化
可线性化的基本想法是**让一个系统看起来好像只有一个数据副本**，且**所有的操作都是原子的**。有了这个保证，应用程序就不需要多副本带来的复制延迟问题，每个客户端都拥有相同的数据视图。这种看似单一副本的假设意味着，它可以保证**读取最近最新值**，而不是过期的缓存。换句话说,**可线性化是一种就近保证**。

可线性化的基本思想很简单，但是它有更多的含义:
1. 一旦某个读操作返回了新值，之后所有的读(**包括相同和不同的客户端**)都必须返回新值
2. 某个客户端读取了新值，即使**写操作尚未提交**，那么所有后续的读取也必须全部返回新值(当然尚未提交，数据库完全可以不让客户端读取到新值)

对于可线性化的系统要格外注意**时序依赖关系**，在下面的读写时序中:
1. 客户端 A 读取到新值 1，在 A 读取返回之后，B 开始读取，由于 **B 的读取严格在 A 的读取发生之后**，因此即使 C 的写入仍在进行之中，也必须返回 1。

![一个可线性化的系统](/images/db/line.png)

可以进一步细化时序图来可视化每部操作具体在哪个时间点生效，如下图所示。这里我们引入了第三种类型的操作: `Cas(x, V-old, V-new)`

![细化读、写操作的生效时间，客户端B的最后读取不满足可线性化](/images/db/line_detail.png)

在上图中每个操作都有一条竖线，表示实际的执行时间点，这些标记**以前后关系依次连接起来**，最终的结果必须是一个有效的寄存器读写顺序，即每个读操作返回最近写操作所设置的值。

可线性化要求，如果连接这些标记的竖线，它们必须总是按时间箭头(从左往右)向前移动，而不能向后移动。这个要求确保了之前所讨论的**就近性保证**: **一旦新值被写入或读取**，所有后续的读都看到的是最新的值，直到被再次覆盖。

上图中有一些细节值得仔细研究:
1. **模型没有假定事务间的隔离**，即另一个并发客户端可能随时会修改值。我们可以使用原子比较和设置(Cas)操作来检查是否被其他并发客户端修改
2. 客户端 B 的最后一次读取不满足线性化。该操作与 C 的 Cas 写操作同时发生，后者将 x 从 2 变成 4。在没有其他请求时，B读取可以返回 2。但在 B 读取开始之前，A 已经读取了新值 4，所以不允许 B 读取到比A更老的值。
3. 通过记录所欲请求和响应的时序，然后检查他们是否可以顺序排列，可以用来测试系统是否可线性化

### 2.1 可线性化与可串行化(重要)
注意可线性化与可串行化非常容易混淆，但它们完全不同:
1. 可串行化
    - 是事务的隔离属性，每个事务可以读写多个对象
    - 用来确保事务执行的结果与串行执行(每次执行一个事务)的结果完全相同，即使串行执行的顺序与事务实际执行的顺序不同
2. 可线性化:
    - 是**读写寄存器(单个对象)**的最新值保证
    - 不要求将操作组合到事务中，因此无法避免写倾斜等问题，除非采取其他额外措施

数据库可以同时支持可串行化和线性化，这种组合被称为**严格的可串行化**或**强的单副本可串行化**。基于两阶段加锁或者实际以串行执行都是典型的可线性化。但是可串行化的快照隔离则不是线性化的: 按照设计，它可以从一致性快照中读取，以避免读写之间的竞争。一致性快照的要点在于它里面不包括快照点创建时刻之后的写入数据，因此从快照读取肯定不满足线性化。


### 2.2 线性化的依赖条件
那么什么时候需要线性化呢？在下列场景中，线性化对于保证系统正确工作至关重要:
1. 主节点的选举:
    - 选举主节点通常的方式是使用锁，谁获取锁谁就是主节点
    - 不管锁如何实现，它必须满足可线性化: 所有节点都必须同意那个节点持有锁
    - **线性化存储服务是所有协调服务的基础**
2. 约束与唯一性保证: 与加锁类似，都要求所有节点就某个最新值达成一致
3. 跨通道的时间依赖
    - 线性化违例之所以被注意到，是因为**系统中存在其他的通信渠道**

什么叫系统中存在其他的通信渠道，我们看下面这个示例:
1. 用户上传图片至 Web 服务器，Image resizer 用于产生缩略图方便快速加载
2. Web 服务通过消息队列通知调整器，因为消息队列通常不适合大数据流而照片可能数兆大小，Web 服务会先将图片写入文件存储服务，写入完成后发消息通知调整器

![多个通信渠道](/images/db/channel_multi.png)

注意这里 Web 服务器和图片调整器之间存在两个不同的通信通道: 文件存储器和消息队列(注: 文件存储服务这里是分布式的)

如果没有线性化的就近保证，这两个通道之间存在竞争条件: 消息队列(步骤3)可能比**存储服务内部的复制**(步骤4)执行更快，这种情况下步骤5 可能会看到图片的旧版本或根本读不到任何内容。如果文件存储服务是可线性化的系统将可以正常工作。

线性化并非避免这种竞争的唯一方法，但却是最容易理解的。例如可以控制某一个通信通道(消息队列)"读自己写"，但会引入额外的复杂性。

### 2.3 实现线性化系统
由于线性化的本质意味着"表现得好像只有一个数据副本 + 所有操作都是原子的"。所以最贱的方案自然是只用一个数据副本，但是这样无法容错。系统容错最常见的方法是采用复制机制。前面我们介绍了多种复制机制，那么它们符合线性化要求么?
1. 主从复制: 读写必须都从主节点才满足线性化要求，但此时无法容错
2. 多主复制: 无法满足
3. 无主复制: 无法满足

直觉上对于 Dynamo 风格的复制模型(无主复制)，如果读写遵从严格的 quorum，应该是可线性化的，然而如果遭遇不确定的网络延迟，就会出现竞争条件，如下图所示:

![严格的 quorum 仍然不满足线性化](/images/db/line_quorum.png)

1. x 初始值为 0，写客户端向所有三个副本发送写请求将 x 置为 1
2. A 从两个节点读取到新值 1
3. B 在 A 之后，但在写请求同步到副本 2/3 之前读取，读到旧值 0

显然这不符合线性化要求。可以通过牺牲性能为代价来满足线性化: 读操作在返回结果给应用之前，必须同步执行读修复；而写操作在发送结果之前，必须读取 quorum 节点以获取最新值。当然这个的前提是数据不会采用最终写入这获胜的方法来处理写冲突。此外这种方法**只能支持线性化读、写操作**，但不能支持线性化的比较和设置操作。

总而言之最安全的假设是类似 Dynamo 风格的无主复制系统无法保证线性化。线性化的唯一实现方式是共识算法。

### 2.4 CAP 理论的误解
无论是主从复制还是多主复制，任何可线性化的数据库都存在这样的问题:
1. 如果应用要求线性化，但由于网络的问题，某些副本与其他副本断开连接后无法继续处理请求，就必须等待网络修复，服务不可用
2. 如果应用不要求线性化，那么断开连接之后，每个副本可独立处理请求，此时服务可用，但结果行为不符合线性化

因此，**不要求线性化的应用更能容忍网络故障**，这被称为 CAP 理论。CAP 代表**一致性、可用性、分区容错性**系统只能支持其中两个特性。但是这种理解存在误导性:
1. **网络分区是一种故障**，无法选择。网络正常时可以同时保证可用性和一致性，一旦发生网络故障，要么选择一致性，要么选择可用性。
2. 一致性指的是**线性化**，很多博客和文章在讲解 CAP 时都没有明确说明过
3. 可用性存在争议，其形式化定理中的可用性与通常意义上的理解有些差别(记得没错的话，CAP 的可用性指的是可访问的到的所有节点中，每一个都可用)。许多所谓的"高可用性"(**容错**)系统实际上并不符合 CAP 对可用性的特殊定义。

总之避免使用 CAP。

### 2.5 可线性化与网络延迟
实际上很少有系统真正满足线性化。例如现代多核 CPU 上的内存就是非线性化的(这个就是为什么类似 Go 语言有所谓的内存模型)。如果某个 CPU 核上运行的线程修改了一个内存地址，紧接着另一个 CPU 核上的线程尝试读取，则系统无法保证可以读到刚刚写入的值，除非使用了内存屏障或 fence 指令。

出现这种现象的原因是每个 CPU 都有自己独立的 cache 和寄存器。内存访问受限进入 cache系统，所有修改默认会异步刷新到主存。由于访问 cache 比访问主存快得多，所以这样的异步刷新对于现代 CPU 性能至关重要。但是，这就导致出现了多个数据副本(主存+几个不同级别cache)，而副本更新是异步的，无法保证线性化。

CAP 理论不适用于当今的**多核-内存一致性模型**: 在计算机内部，我们通常假设通信是可靠的，我们会假设一个 CPU核在于其他核断开之后操作系统还没down机。之所以放弃线性化的原因是**性能**，而不是为了容错。

许多分布式数据库也是类似，他们选择不支持线性化是为了提高性能，而不是为了保住容错特性。**无论是否发生故障，线性化对性能的影响都是巨大的**。

是否存在一个更有效的线性化实现方案呢？目前来看是否定的，已经有证明: **如果想要满足线性化，读、写请求的响应时间至少要与网络延迟成正比**。考虑到网络高度不确定的网络延迟，线性化读写的性能势必非常差。

虽然没有足够快的线性化算法，但弱一致性模型的性能则快得多，这种取舍对于延迟敏感的系统非常重要。后面我们将讨论一些避免线性化但有可以保证正确性的方法。

所以**可线性化的缺点就是读写性能必然非常差**。


## 1.共识算法实现的逻辑
因果一致性可以认为是，不会由于网络延迟而显著影响性能，又能对网络故障提供容错的最强的一致性模型。
1. 共识算法，比如 Raft，Zab 都是通过全序关系广播来实现
2. 全序关系广播通常需要一个主节点
3. 主节点的选取通常需要共识算法，这就导致了奇怪的循环
4. 解决的办法是，每个主节点都有一个单调递增的版本号，又称为世代编号(epoch)
5. 主节点在做出任何决定时:
    - 首先必须将提议发送给其他所有节点，等待多数节点的响应
    - 节点只有在没有发现更高 epoch 主节点存在时，才会对当前提议(带 epoch 号码)进行投票
    - 这里实际存在两轮不同的投票，首先投票决定谁是主节点，然后对节点的提议进行投票
    - 最关键一点是参与两轮的 quorum 必须由重叠: 如果某个提议获得通过，那么其中参与投票的节点必须至少有一个也参加了最近一次的主节点选举
    - 换言之，如果在针对提议的投票中没有出现更高 epoch 号码，那么可以得出这样的结论: 因为没有发生更高 epoch 的主节点选举，当前主节点地位没有改变，所以可以安全就提议进行投票

## 2. 为什么要使用 Zookeeper
1. 要在多节点集群中达成共识，可以自己实现一个共识算法(很难)，所以可以直接使用 Zookeeper 提供的服务
2. Zookeeper 通常在固定数量的节点上运行投票，相比于在 1000 甚至更多的节点上达成共识效率更高


因此我们需要某种算法对写入的最终顺序进行确定，即达成共识。这就涉及到两个问题:
1. 如何达成共识
2. 如何让客户端感知到共识发生了变化

如果我们假设网络始终稳定，且不存在单点故障(类似单机情况下)通过锁就可以解决并发问题。但是这些问题无法避免单点故障可能导致数据丢失，这就需要多台机器提供冗余，此时就需要对同一项配置达成共识，我们需要共识算法。

共识算法需要解决的另一个问题是，当配置发生变化时，如何让客户端感知到共识发生了变化。我们可以让所有节点抖注册监听器，但是这样会引起惊群效应。因此我们只能让少量节点，即所谓的主节点注册监听器。然后通过版本矢量，让与主节点同步的从节点感知配置的变化。

kafka 为什么要引入 Zookeeper 是因为如果对所有 kafka 的节点都通过共识算法解决数据一致性问题，效率将非常低。数据一致性是以吞吐量为代价的。