---
title: 10. 一致性与共识
date: 2019-04-10
categories:
    - 分布式
tags:
    - 数据密集型应用
---

一致性与共识

<!-- more -->

## 1. 共识算法的概述
本节我们将讨论构建容错式分布式系统的相关算法和协议。这里我们假设第 8 章中所有的故障都可能发生: 网络会丢失、顺序紊乱、重复发送或延迟；时钟也有一定偏差，节点可能发生暂停甚至随时崩溃。为了构建容错系统，最好先建立一套**通用的抽象机制**和**与之对应的技术保证**，这样只需实现一次，其上的各种应用程序都可以安全的信赖底层的保证。这与引入事务是一样的道理。

接下来我们将沿着这个思路，尝试建立**让分布式应用忽略内部各种问题的抽象机制**，比如**共识**，即所有的节点就某一项提议达成一致。我们将研究解决共识问题的相关算法，以及讨论**分布式系统可提供的若干保证和抽象机制**。

我们需要了解系统能力的边界，在什么情况下，系统可以容忍故障并继续工作。我们将探索分布式系统下比最终一致性更强的一致性模型(之前介绍复制的内容时，我们已经介绍了一部分)。

分布式一致性模型与之前讨论的多种事务隔离级别有相似之处，但总体上他们有着显著的不同:
1. 事务隔离主要是为了处理**并发执行事务时**的各种临界条件
2. 分布式一致性则主要是**针对延迟和故障等问题**来**协调副本之间的状态**

分布式一致性包括:
1. 写后读
2. 单调读
3. 前缀一致读
4. 线性化
5. 顺序一致性，满足线性化的写，不满足线性化的读
5. 因果一致性

前面三个我们已经介绍过了，接下来我们将讨论:
1. **线性化**: 这是最强的一致性模型
2. 分布式系统中**事件顺序问题**，特别是**因果关系**和**全局顺序**
3. 如果自动提交分布式事务，并最终解决共识问题

## 2. 可线性化
可线性化的基本想法是**让一个系统看起来好像只有一个数据副本**，且**所有的操作都是原子的**。有了这个保证，应用程序就不需要多副本带来的复制延迟问题，每个客户端都拥有相同的数据视图。这种看似单一副本的假设意味着，它可以保证**读取最近最新值**，而不是过期的缓存。换句话说,**可线性化是一种就近保证**。

可线性化的基本思想很简单，但是它有更多的含义:
1. 一旦某个读操作返回了新值，之后所有的读(**包括相同和不同的客户端**)都必须返回新值
2. 某个客户端读取了新值，即使**写操作尚未提交**，那么所有后续的读取也必须全部返回新值(当然尚未提交，数据库完全可以不让客户端读取到新值)

对于可线性化的系统要格外注意**时序依赖关系**，在下面的读写时序中:
1. 客户端 A 读取到新值 1，在 A 读取返回之后，B 开始读取，由于 **B 的读取严格在 A 的读取发生之后**，因此即使 C 的写入仍在进行之中，也必须返回 1。

![一个可线性化的系统](/images/db/line.png)

可以进一步细化时序图来可视化每部操作具体在哪个时间点生效，如下图所示。这里我们引入了第三种类型的操作: `Cas(x, V-old, V-new)`

![细化读、写操作的生效时间，客户端B的最后读取不满足可线性化](/images/db/line_detail.png)

在上图中每个操作都有一条竖线，表示实际的执行时间点，这些标记**以前后关系依次连接起来**，最终的结果必须是一个有效的寄存器读写顺序，即每个读操作返回最近写操作所设置的值。

可线性化要求，如果连接这些标记的竖线，它们必须总是按时间箭头(从左往右)向前移动，而不能向后移动。这个要求确保了之前所讨论的**就近性保证**: **一旦新值被写入或读取**，所有后续的读都看到的是最新的值，直到被再次覆盖。

上图中有一些细节值得仔细研究:
1. **模型没有假定事务间的隔离**，即另一个并发客户端可能随时会修改值。我们可以使用原子比较和设置(Cas)操作来检查是否被其他并发客户端修改
2. 客户端 B 的最后一次读取不满足线性化。该操作与 C 的 Cas 写操作同时发生，后者将 x 从 2 变成 4。在没有其他请求时，B读取可以返回 2。但在 B 读取开始之前，A 已经读取了新值 4，所以不允许 B 读取到比A更老的值。
3. 通过记录所欲请求和响应的时序，然后检查他们是否可以顺序排列，可以用来测试系统是否可线性化

### 2.1 可线性化与可串行化(重要)
注意可线性化与可串行化非常容易混淆，但它们完全不同:
1. 可串行化
    - 是事务的隔离属性，每个事务可以读写多个对象
    - 用来确保事务执行的结果与串行执行(每次执行一个事务)的结果完全相同，即使串行执行的顺序与事务实际执行的顺序不同
2. 可线性化:
    - 是**读写寄存器(单个对象)**的最新值保证
    - 不要求将操作组合到事务中，因此无法避免写倾斜等问题，除非采取其他额外措施

数据库可以同时支持可串行化和线性化，这种组合被称为**严格的可串行化**或**强的单副本可串行化**。基于两阶段加锁或者实际以串行执行都是典型的可线性化。但是可串行化的快照隔离则不是线性化的: 按照设计，它可以从一致性快照中读取，以避免读写之间的竞争。一致性快照的要点在于它里面不包括快照点创建时刻之后的写入数据，因此从快照读取肯定不满足线性化。


### 2.2 线性化的依赖条件
那么什么时候需要线性化呢？在下列场景中，线性化对于保证系统正确工作至关重要:
1. 主节点的选举:
    - 选举主节点通常的方式是使用锁，谁获取锁谁就是主节点
    - 不管锁如何实现，它必须满足可线性化: 所有节点都必须同意那个节点持有锁
    - **线性化存储服务是所有协调服务的基础**
2. 约束与唯一性保证: 与加锁类似，都要求所有节点就某个最新值达成一致
3. 跨通道的时间依赖
    - 线性化违例之所以被注意到，是因为**系统中存在其他的通信渠道**

什么叫系统中存在其他的通信渠道，我们看下面这个示例:
1. 用户上传图片至 Web 服务器，Image resizer 用于产生缩略图方便快速加载
2. Web 服务通过消息队列通知调整器，因为消息队列通常不适合大数据流而照片可能数兆大小，Web 服务会先将图片写入文件存储服务，写入完成后发消息通知调整器

![多个通信渠道](/images/db/channel_multi.png)

注意这里 Web 服务器和图片调整器之间存在两个不同的通信通道: 文件存储器和消息队列(注: 文件存储服务这里是分布式的)

如果没有线性化的就近保证，这两个通道之间存在竞争条件: 消息队列(步骤3)可能比**存储服务内部的复制**(步骤4)执行更快，这种情况下步骤5 可能会看到图片的旧版本或根本读不到任何内容。如果文件存储服务是可线性化的系统将可以正常工作。

线性化并非避免这种竞争的唯一方法，但却是最容易理解的。例如可以控制某一个通信通道(消息队列)"读自己写"，但会引入额外的复杂性。

### 2.3 实现线性化系统
由于线性化的本质意味着"表现得好像只有一个数据副本 + 所有操作都是原子的"。所以最贱的方案自然是只用一个数据副本，但是这样无法容错。系统容错最常见的方法是采用复制机制。前面我们介绍了多种复制机制，那么它们符合线性化要求么?
1. 主从复制: 读写必须都从主节点才满足线性化要求，但此时无法容错
2. 多主复制: 无法满足
3. 无主复制: 无法满足

直觉上对于 Dynamo 风格的复制模型(无主复制)，如果读写遵从严格的 quorum，应该是可线性化的，然而如果遭遇不确定的网络延迟，就会出现竞争条件，如下图所示:

![严格的 quorum 仍然不满足线性化](/images/db/line_quorum.png)

1. x 初始值为 0，写客户端向所有三个副本发送写请求将 x 置为 1
2. A 从两个节点读取到新值 1
3. B 在 A 之后，但在写请求同步到副本 2/3 之前读取，读到旧值 0

显然这不符合线性化要求。可以通过牺牲性能为代价来满足线性化: 读操作在返回结果给应用之前，必须同步执行读修复；而写操作在发送结果之前，必须读取 quorum 节点以获取最新值。当然这个的前提是数据不会采用最终写入这获胜的方法来处理写冲突。此外这种方法**只能支持线性化读、写操作**，但不能支持线性化的比较和设置操作。

总而言之最安全的假设是类似 Dynamo 风格的无主复制系统无法保证线性化。线性化的唯一实现方式是共识算法。

### 2.4 CAP 理论的误解
无论是主从复制还是多主复制，任何可线性化的数据库都存在这样的问题:
1. 如果应用要求线性化，但由于网络的问题，某些副本与其他副本断开连接后无法继续处理请求，就必须等待网络修复，服务不可用
2. 如果应用不要求线性化，那么断开连接之后，每个副本可独立处理请求，此时服务可用，但结果行为不符合线性化

因此，**不要求线性化的应用更能容忍网络故障**，这被称为 CAP 理论。CAP 代表**一致性、可用性、分区容错性**系统只能支持其中两个特性。但是这种理解存在误导性:
1. **网络分区是一种故障**，无法选择。网络正常时可以同时保证可用性和一致性，一旦发生网络故障，要么选择一致性，要么选择可用性。
2. 一致性指的是**线性化**，很多博客和文章在讲解 CAP 时都没有明确说明过
3. 可用性存在争议，其形式化定理中的可用性与通常意义上的理解有些差别(记得没错的话，CAP 的可用性指的是可访问的到的所有节点中，每一个都可用)。许多所谓的"高可用性"(**容错**)系统实际上并不符合 CAP 对可用性的特殊定义。

总之避免使用 CAP。

### 2.5 可线性化与网络延迟
实际上很少有系统真正满足线性化。例如现代多核 CPU 上的内存就是非线性化的(这个就是为什么类似 Go 语言有所谓的内存模型)。如果某个 CPU 核上运行的线程修改了一个内存地址，紧接着另一个 CPU 核上的线程尝试读取，则系统无法保证可以读到刚刚写入的值，除非使用了内存屏障或 fence 指令。

出现这种现象的原因是每个 CPU 都有自己独立的 cache 和寄存器。内存访问受限进入 cache系统，所有修改默认会异步刷新到主存。由于访问 cache 比访问主存快得多，所以这样的异步刷新对于现代 CPU 性能至关重要。但是，这就导致出现了多个数据副本(主存+几个不同级别cache)，而副本更新是异步的，无法保证线性化。

CAP 理论不适用于当今的**多核-内存一致性模型**: 在计算机内部，我们通常假设通信是可靠的，我们会假设一个 CPU核在于其他核断开之后操作系统还没down机。之所以放弃线性化的原因是**性能**，而不是为了容错。

许多分布式数据库也是类似，他们选择不支持线性化是为了提高性能，而不是为了保住容错特性。**无论是否发生故障，线性化对性能的影响都是巨大的**。

是否存在一个更有效的线性化实现方案呢？目前来看是否定的，已经有证明: **如果想要满足线性化，读、写请求的响应时间至少要与网络延迟成正比**。考虑到网络高度不确定的网络延迟，线性化读写的性能势必非常差。

虽然没有足够快的线性化算法，但弱一致性模型的性能则快得多，这种取舍对于延迟敏感的系统非常重要。后面我们将讨论一些避免线性化但有可以保证正确性的方法。

所以**可线性化的缺点就是读写性能必然非常差**。

## 3. 顺序保证
线性化寄存器对外呈现的好像只有一份数据拷贝，并且每一个操作都是原子性生效。这意味着**操作是按照某种顺序执行的**。顺序是一个非常重要的概念，我们已经多次提及:
1. 主从复制中主节点的主要作用就是**确定复制日志的写入顺序**，正因为有主节点确定写入顺序，主从复制才不会出现并发写冲突
2. 可串行化是确保事务的执行结果与**按照某种顺序方式**执行一样
3. 分布式系统的时间戳和时钟，试图将顺序引入无序的操作，比如确定两个写操作哪一个先发生。

**排序、可线性化与共识**存在着某种深刻的联系。理解它们对于理解系统能做什么不能做什么非常有帮助。

### 3.1 顺序与因果关系
之所以反复出现**顺序问题**，其中一个原因是它有助于**保持因果关系**。因果关系对所发生的事件施加了某种顺序: 发送消息先于收到消息，问题出现在答案之前等等。**这些因果关系的依赖链条**定义了系统中的**因果顺序**，即某件事应该发生在另一件事情之前。

如果系统服从因果关系所规定的顺序，我们称之为**因果一致性**。例如快照隔离级别提供了因果一致性: 当从数据库中读数据时，如果查询到了某些数据，也一定能看到触发该数据的前序事件(假设期间没有发生删除操作)。

### 3.2 因果顺序并非全序
全序关系支持任何两个元素之间进行比较，但是某些集合并不符合全序，例如集合{a,b} 和 {b,c} 无法直接比较，数据集合只能是偏序，即某些情况下一个集合可以包含另一个，否则无法比较。

**全序和偏序**的差异也体现在不同的数据库一致性模型中:
1. 可线性化: **存在全序操作关系**，总是可以指出哪个操作在先
2. 因果关系: 
    - 如果两个操作都没有发生在对方之前，这两操作是并发关系，Happen-before
    - 如果两个事件是因果关系，那么这两个事件可以被排序
    - 并发的事先无法排序比较
    - 因果关系至少可以定位为偏序，而非全序

因此根据这个定义，可线性化数据存储中不存在并发操作，一定有一个时间线将所有操作都全序执行，所以单个时间轴，单个数据副本，没有并发。可能存在多个请求，但出于等待状态。

并发意味着时间线会出现分支和合并，而不同分支上的操作无法直接比较。

### 3.3 可线性化与因果一致性
可线性化一定意味着因果关系，特别是如果系统存在多个通信通道，可线性化确保了因果关系会自动全部保留，而不需要额外的工作。

但是线性化会显著降低性能和可用性，尤其是在严重网络延迟的情况下。

**线性化并非是保证因果关系的唯一途径**。还有其他方法使得系统可以满足因果一致性而免于线性化所带来的的性能问题。**因果一致性可以认为是，不会由于网络延迟而显著影响性能，又能对网络故障提供容错的最强一致性模型**。

许多情况，看似需要线性化的系统实际真正需要的是因果一致性。这也正是当下数据库发展的方向: 新的数据库保证因果关系，性能与可用性与最终一致性类似。

### 3.4 捕获因果依赖关系
为保持有听过关系，需要知道哪一个操作发生在前，如果一个操作发生在另一个操作之前，那么每个副本都应该按照相同的顺序处理。我们需要一些手段来描述系统中节点所知道的"知识"。与前面介绍的检测并发写的方法类似:
1. 无主复制中的因果关系，需要去检查对同一个主键的并发写请求，从而避免更新丢失
2. 因果一致性则要更近异步，需要跟踪整个数据库请求的 因果关系，而不仅仅是某个主键，版本向量可以推广为一种通用的解决方案。
3. 为了确定因果关系，数据库需要知道应用程序读取的是哪个版本的数据

因果关系很重要，但实际上跟踪所有的因果关系不切实际。客户端可能在写之前读取大量数据，系统无法了解写入究竟是依赖于全部读取内容内容，还仅仅是其中一部分。更好的办法是可以使用序列号或时间戳来排序时间。

在主从复制中，主节点可以简单为每个操作递增某个计数器，从而为复制日中的每个操作赋值一个单调递增的序列号。但如果系统不存在唯一的主节点，序列号的产生就不简单了。序列号要保证与因果关系一致，特别是保证跨节点操作的顺序。

#### Lamport 时间戳
有一种简单方法可以产生与因果关系一致的序列号，叫做兰伯特时间戳。下面是 Lamport 时间戳的示例:

![Lamport时间戳](/images/db/lamport.png)

1. 每一个节点都有一个唯一的标识符，且每个节点都有一个计数器来记录各自已处理的请求总数
2. Lamport 时间戳是一个值对(计数器，节点ID)
3. 给定两个 Lamport 时间戳，计数器较大那个时间戳大，如果计数器值相等，节点 ID 越大，时间戳越大

Lamport 时间戳的核心亮点在于使它们与因果关系保持一致:
1. 每个节点以及每个客户端都跟踪迄今为止所见到的最大计数器值
2. 并在每个请求中附带该最大计数器值
3. 当节点收到某个请求或回复时，如果发现请求内嵌的最大计数器值大于节点自身的计数器值，则立即把自己的计数器修改为改最大值

**只要把最大计数器值嵌入到每一个请求中**，该方案可以确保 Lamport 时间戳与因果关系一致性，而**请求的因果依赖性一定会保证后发生的请求得到更大的时间戳**

Lamport 时间戳与版本向量存在相似之处，但它们的目的不同:
1. 版本向量用以区分两个操作是并发还是因果依赖
2. Lamport 时间戳则主要用于确保全序关系，但是即便 Lamport 时间戳与因果序一致，但**根据其全序关系却无法区分两个操作属于并发关系，还是因果依赖关系**。(Lamport 时间戳中，并发操作也被排序)


#### 时间戳排序依然不够
虽然Lamport时间戳定义了与因果序一致的全序关系，但还不足以解决实际分布式系统中许多常见的问题，例如账号的全局唯一性。乍看总是选择时间戳最小的作为获胜者是可行的，但是这个方法有一个前提条件: 需要收集系统中所有的用户创建请求，然后才可以比较它们的时间戳。当节点刚刚收到用户的创建请求时，它无法当时就做出决定该请求应该成功还是失败。此时节点根本不知道是否有另一个节点在同时创建相同的用户名。

为了获取以上两点信息，系统必须检查每个节点，如果因为节点故障或网络问题无法连接，这个方法就无法正常运转。

这个问题的关键是: **只有收集了所有的请求信息之后**才能清楚这些请求之间的全序关系。如果另一个节点执行了某些操作，但你无法知道那是什么，就无法构造出最终的序列。也许，来自该位置操作确实需要插入到全序集合中才能正确评估出下一步。

总而言之，为了实现像用户名唯一性约束，仅仅**对操作进行全序排序还是不够的，还需要知道这些操作是否发生、何时确定等**。假如能够在创建用户时，已经确定知道了没有其他节点正在执行相同用户名的创建，大可直接安全返回成功。

要想知道**什么时候全序关系已经确定**就需要**全序关系广播**。

## 4. 全序关系广播
在分布式系统上，让所有节点就全序关系达成一致面临巨大挑战。在分布式系统文献中，这些问题被称为**全序关系广播**或者原子广播。

全序关系广播通常指**节点之间交换信息的某种协议**。下面是一个非正式的定义:
1. 可靠发送: 没有消息丢失，如果消息发送到某一节点，一定发送到所有节点
2. 严格有序: 消息总以相同的顺序发送给每个节点

### 4.1 使用全序关系广播
想 Zookeeper 和 etcd 这样的共识服务实际上就实现了全序关系广播，这也暗示了全序关系广播与共识之间有着密切联系。

全序关系广播:
1. 正是数据库复制所需要的，如果每条消息代表数据库写请求，并且每个副本都按相同的顺序处理这些写请求，那么所有副本可以保持一致(或许有滞后)，该原则也被称为**状态机复制**
2. 可以实现**串行化事务**: 每条消息表示一个确定性事务，并且作为存储过程来执行，且每个节点遵从相同的执行顺序，可以保证数据库各分区以及各副本的一致性
3. 另一个要点是: **顺序在发送消息时已经确定**，**如果消息发送成功，节点不允许追溯地将某条消息插入到先前的某个位置上**(重要)。这一点是的全序关系广播比基于时间戳排序要求更强。

全序关系广播中，消息就像**追加方式更新日志**。

### 4.2 采用全序关系广播实现线性化存储(非常重要)
一个可线性化的系统中有全序操作集合。这是否意味着可线性化与全序关系广播是完全相同的呢？不完全是，但它们有着密切的联系:
1. 全序关系广播是基于异步模型: **保证消息以固定的顺序可靠发送**，但是**不保证消息何时发送成功**。某个接收者可能明显落后于其他接收者。可线性化则强调就近性: **读取时保证能够看到最新的写入值**
2. 如果有了全序关系广播，就可以在其上构建线性化的存储系统。

#### 基于全序关系广播实现线性化存储
通过使用全序关系广播以追加日志的方式来实现**线性化的原子比较-设置**:
    - 在日志中追加一条消息，并指明想要的用户名
    - 读取日志，将其广播给所有节点，并等待回复
    - 检查是否有任何消息生成用户名已被占用，如果没有其他节点回复已占用，可以提交该获取声明并返回客户端。反之则中止操作

由于日志条目以相同的顺序发送到所有节点，如果存在多个并发写入，则所有节点将首先决定哪个请求在先，选择第一个写请求作为获胜者，并中止其他请求，以确保所有节点同意一个写请求最终要么提交成功要么中止。类似的方法还可以用来在日志之上实现可串行化的多对象事务。

虽然此过程**可以确保线性化写入**，但它**无法保证线性化读取**。即从异步日志更新的存储中读取数据时，可能是旧值。具体来说，这里**只提供了顺序一致性**，也称为**时间线一致性**，它弱于线性化保证。

为了同时满足线性化读取，有以下几个方案:
1. 采用**追加的方式**把读请求排序、广播，然后各个节点获取该日志，当本节点收到消息时才执行真正的读操作。**消息在日志中的位置**已经决定了读取发生的时间点。etcd 的 quorum 读取和这个思路有相似之处
2. 如果可以**以线性化的方式获取当前最新日志中消息的位置**，则查询位置，等待直到该位置之前的所有条目都已经发送给你，接下来再执行读取。这与 Zookeeper的 sync() 操作思想相同。
3. 可以从同步更新的副本上读取，这样确保总是读取最新值

#### 采用线性化存储实现全序关系广播
最简单的方法是假设有一个线性化的寄存器来存储一个计数，然后使其支持原子自增-读取操作或者原子比较-设置操作。

思路很简单: 对于每个要通过全序关系广播的消息，**原子递增并读取**该线性化的计数，然后将其作为序列号附加到消息中。接下来广播到所有节点，接收者也严格按照序列化来发送回复消息。

这与 Lamport 时间戳不同，通过递增线性化寄存器获得的数字不会存在任何间隙。因此节点完成了消息 4 的发送，且收到了序列化 6 的消息，那么**在它对消息 6 回复之前必须等待消息 5**。Lamport时间戳不是这样的，这样是区别**全序关系广播**和**基于时间戳排序**的关键。

原子自增操作的线性化整数有多难？如果没有失效很容易，难点在于如何处理网络故障和节点失效。事实上，如果对**线性化的序列号发生器**深入思考之后所得到的的最终结果，毫无意外的指向了共识算法。

可以证明(非常重要):
1. **线性化的原子比较-设置(或自增)寄存器**与**全序关系广播**二者都等价于共识问题
2. 如果你能解决其中的一个问题，那么就可以把方案用于解决其他问题

接下来我们就正式进入共识问题。

#### 概念的递推关系总结(重要)
在可线性化，因果关系一致性，全序关系广播他们之间存在这样的关系:
1. 可线性化是分布式系统中最强的一致性模型，但是不能容错
2. 退而求次，大多数系统需要的是因果关系一致性，因果关系一致性可以容忍网络异常，提供和最终一致性一样的性能和容错性
3. 要捕获全局因果关系是不现实的，系统转而去捕获因果依赖关系，通过确定事件顺序来间接实现因果关系一致性
4. 通过 Lamport 时间戳可以获取**全序关系**，但是仅仅对操作进行全序排列还是不够的，需要确切知道操作是否发生、何时确定
5. 要想获取操作的全序关系，同时让所有节点达成共识就是全序关系广播。
6. 基于全序关系广播就可以实现线性化存储，反之亦然

## 1.共识算法实现的逻辑
因果一致性可以认为是，不会由于网络延迟而显著影响性能，又能对网络故障提供容错的最强的一致性模型。
1. 共识算法，比如 Raft，Zab 都是通过全序关系广播来实现
2. 全序关系广播通常需要一个主节点
3. 主节点的选取通常需要共识算法，这就导致了奇怪的循环
4. 解决的办法是，每个主节点都有一个单调递增的版本号，又称为世代编号(epoch)
5. 主节点在做出任何决定时:
    - 首先必须将提议发送给其他所有节点，等待多数节点的响应
    - 节点只有在没有发现更高 epoch 主节点存在时，才会对当前提议(带 epoch 号码)进行投票
    - 这里实际存在两轮不同的投票，首先投票决定谁是主节点，然后对节点的提议进行投票
    - 最关键一点是参与两轮的 quorum 必须由重叠: 如果某个提议获得通过，那么其中参与投票的节点必须至少有一个也参加了最近一次的主节点选举
    - 换言之，如果在针对提议的投票中没有出现更高 epoch 号码，那么可以得出这样的结论: 因为没有发生更高 epoch 的主节点选举，当前主节点地位没有改变，所以可以安全就提议进行投票

## 2. 为什么要使用 Zookeeper
1. 要在多节点集群中达成共识，可以自己实现一个共识算法(很难)，所以可以直接使用 Zookeeper 提供的服务
2. Zookeeper 通常在固定数量的节点上运行投票，相比于在 1000 甚至更多的节点上达成共识效率更高


因此我们需要某种算法对写入的最终顺序进行确定，即达成共识。这就涉及到两个问题:
1. 如何达成共识
2. 如何让客户端感知到共识发生了变化

如果我们假设网络始终稳定，且不存在单点故障(类似单机情况下)通过锁就可以解决并发问题。但是这些问题无法避免单点故障可能导致数据丢失，这就需要多台机器提供冗余，此时就需要对同一项配置达成共识，我们需要共识算法。

共识算法需要解决的另一个问题是，当配置发生变化时，如何让客户端感知到共识发生了变化。我们可以让所有节点抖注册监听器，但是这样会引起惊群效应。因此我们只能让少量节点，即所谓的主节点注册监听器。然后通过版本矢量，让与主节点同步的从节点感知配置的变化。

kafka 为什么要引入 Zookeeper 是因为如果对所有 kafka 的节点都通过共识算法解决数据一致性问题，效率将非常低。数据一致性是以吞吐量为代价的。