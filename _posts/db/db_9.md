---
title: 9. 分布式系统的挑战
date: 2019-04-09
categories:
    - 分布式
tags:
    - 数据密集型应用
---

分布式系统面临的挑战

<!-- more -->

## 1. 分布式系统中的故障
在分布式系统中，故障来自于下面的方方面面:
1. 网络分区不可避免 -- 网络不可靠
2. 时钟和时序问题，时钟无法精确同步  --- 时钟不可靠
3. 分布式系统中的一个节点必须假设，执行过程中的任何时刻都可能被暂停相当长一段时间，包括运行在某个函数中间。暂停期间，整个集群其他部分都照常运行，甚至会一致将暂停的节点宣告为故障节点，最终暂停的节点可能会回来继续执行，除非再次检查时钟，否则对刚刚过去的暂停毫无意识

因此我们将探讨如何认清分布式系统的状态本质，并据此来评估所发生的各种故障。

## 2. 故障与部分失效
在单节点上开发应用程序，通常是以一种确定性的方式运行: 要么工作，要么出错，相同的操作通常总会产生相同的结果(即确定性)，而不应该出现模棱两可的现象。这背后涉及一个非常慎重的选择: 如果发生了某种内部错误，我们宁愿使计算机全部奔溃，而不是返回一个错误的结果，错误的结果往往更难处理。因此计算机隐藏了一切模糊的物理世界，呈现出一个理想的系统模型。

然而对于分布式系统，这种理想化的标准正确模型不再适用，分布式系统中可能会出现系统的一部分工作正常，但其他某些部分出现难以预测的故障，我们称之为"部分失效"。问题的难点就在于**这种部分失效是不确定的**。正是由于这种**不确定性**和**部分失效**大大提高了分布式系统的复杂度。

故障处理是软件设计的重要组成部分，作为系统运维者，需要知道在发生故障时，系统的预期行为是什么。不能假定故障不可能发生而总是期待理想情况。可以说，**在分布式系统中，怀疑、悲观和偏执狂才能生存**。

## 3. 不可靠的网络
我们关注的是分布式无共享系统，即通过网络连接的多个节点。网络是跨节点通信的唯一方式。但网络并不保证它神秘时候到达，甚至它是否一定到达。发送之后等待响应过程中，有很多事情可能会出错:

![请求响应可能在很多地方法神错误](/images/db/network_error.png)

如上图所示，请求响应可能在很多地方法神错误:
1. 请求丢失
2. 请求在队列中，无法马上发送
3. 远程节点失效
3. 远程节点无法立即响应
4. 响应丢失
6. 请求处理已经完成，但回复被延迟处理

如果**请求没有得到响应，无法区分是(a) 请求丢失 (b)远程节点关闭 (c)响应丢失**

### 3.1 现实中的网络故障
现实中可能因为各种原因出现各种硬件故障，这些原因处理物理原因外，很大一部分是认为造成的。当网络的一部分由于网络故障而与其他部分断开，称之为网络分区。

处理网络故障并不意味着总是需要复杂的容错机制，一种简单的方法是对用户提示错误信息。但前提是**必须非常清楚接下来软件会如何应对网络故障，以确保系统最终可以恢复**。推荐有计划的人为触发网络问题，来测试系统的反应情况。

### 3.2 故障检测
系统通常都需要自动检测节点失效，然而由于网络的不确定性很难准确判断节点是否确实失效。如果超时是故障检测唯一可行办法，那**超时应该设置多长时间呢？**:
1. 较长的时间意味着更长时间才能宣告节点失效，在此期间用户只能等待或拿到错误信息
2. 较短的时间可以快速检测故障，但是很可能出现误判(只是网络波动或者性能波动而非故障)

节点宣告失效，需要转移负载，会给网络和其他节点带来额外的负担，如果系统已经处于高负载装填，很可能导致失效扩散。

#### 网络拥塞与排队
计算机网络上数据包延时的变化根源往往在于排队:
1. 高流量下，在网络交换机出现排队
2. CPU高负载下，在操作系统出现排队
3. 虚拟化下，入向的包可能会被虚拟机管理器排队缓存
4. TCP 执行流量控制意味着数据甚至在进入网络之前，已经在发送方开始排队
5. TCP 重传也会引入额外的延迟

![网络包排队](/images/db/waiter_queue.png)

以上因素都会导致网络延时的变化和不确定性，特别是当系统接近最大设计上限时，**负载过高，队列深度就会显著加大**，排队对延时的影响变得特别明显。

网络超时设置最好的办法不是设置一个不变的常量，而是持续测量响应时间及其变化，然后根据最新的响应时间分布来自动调整。可以使用 **Phi Accrual 故障检测器**，改检测器已在 Akka 和 Cassandra 中使用。

## 4. 不可靠的时钟
时钟和计时非常重要，但由于网络的不确定延迟，精确测量面临很多挑战，这使得多节点通信时**很难确定事情发生的先后顺序**。而且每台机器都维护自己本地的时间版本，可能比其他机器稍快或更慢。NTP通常用来同步机器之间的时钟，一定程度上可以同步机器之间的时钟。

### 4.1 单调时钟和墙上时钟
现在计算机内部至少有两种不同的时钟，本质上他们是服务于不同目的的:
1. 墙上时钟: 
    - 钟表时钟，可以与 NTP 同步
    - 因为时钟同步会导致时间出现突然的跳跃，特别是跳回到先前的某个时间点，导致其不适合测量时间间隔 
2. 单调时钟:
    - 名字来源于其保证总是向前，不会出现回拨，更适合测量持续时间段(时间间隔)
    - 绝对值没有任何意义，有意义的时钟的间隔
    - 如果服务器有多路CPU，则每个 CPU 可能有单独的计时器，其不与其他CPU同步，由于应用程序的线程可能会调度到不同的 CPU上，操作系统会补偿多个计时器之间的偏差，从而为应用程序提供统一的**单调递增计时**，不过最好还是对这种偏差补偿持谨慎态度
    - NTP 不会直接调整单调时钟

分布式系统中可以采用单调时钟测量一段任务的持续时间，它不假设节点间有任何的时钟同步，且可以容忍轻微测量误差。

### 4.2 时钟同步与准确性
墙上时钟需要根据 NTP服务器或其他外部时间做必要的同步，但硬件时钟和 NTP 可能会出现一些莫名其妙的现象:
1. 计算机中的石英钟不够精确，存在漂移现象，即速度加快或减慢
2. 时钟与 NTP 时间差别太大，可能会出现拒绝同步，或者同步后出现时间的突变
3. 网络延时会影响 NTP 同步的准确性
4. NPT 本身的故障
5. 闰秒
6. 虚拟机中，硬件时钟也是虚拟化的，时间可能出现跳跃

如果应用需要精确同步的时钟，最好仔细监控所有节点上的时钟偏差。如果某个节点的时钟漂移超出上限，应该将其宣告失效，并从集群中移除。

### 4.3 时间戳与事件顺序
跨节点的事件顺序，如果高度依赖时钟计时，就存在一定的技术风险，因为各个节点的时间戳很可能不能对所有事件正确排序。由于时钟精度限制，两个节点可能产生了相同的时间戳。

依赖时间戳确定的事件顺序，无法区分连续快速发生的连续写和并发写入，需要额外的**因果关系跟踪机制(版本向量)**来防止因果冲突。

因此通过保持**最新值**并丢弃其他值的**最后写入获胜(LWW)**冲突解决策略看起来不错，但是最新的定义如果取决于墙上时钟就会引入偏差。

对于排序来说，基于递增计数器而不是震荡石英晶体的**逻辑时钟**是更可靠的方式。逻辑时钟不测量一天的某个时间点或时间间隔，而是事件的相对顺序(**事件发生的相对前后顺序**)。相应的墙上时钟和单调时钟都属于**物理时钟**。

#### 4.4 分布式全局快照的同步时钟(重要)
常见的快照隔离需要单调递增事务ID，单节点上，一个简单的计数器足以生成事务ID。但是当数据分布在多台机器，跨越多个数据中心时，由于需要复杂的协调以产生全局的单调递增的事务 ID(跨所有分区)。

**事务ID要求必须反映因果关系**: 事务B要读取事务A写入的值，B的事务ID，必须大于A的事务ID

考虑到大量，频繁的小包，在分布式系统中创建事务ID通常会引入瓶颈。那能否**用同步后的墙上时钟作为事务ID呢？如果时钟足够可靠其同步，自然符合事务ID属性要求: 后发生的事务具有更大的时间戳。然而问题还是时钟精度不精确。

Googgle Spanner以这种方式实现跨数据中心的快照隔离。它使用TrueTime API报告的时钟置信区间，并基于以下观察结果：如果有两个置信区间，每个置信区间包含最早和最近可能的时间戳（ $A = [A{earliest}, A{latest}]$， $B=[B{earliest}, B{latest}] $），这两个区间不重叠（即：$A{earliest} < A{latest} < B{earliest} < B{latest}$），那么B肯定发生在A之后——这是毫无疑问的。只有当区间重叠时，我们才不确定A和B发生的顺序。

​ 为了**确保事务时间戳反映因果关系**，Spanner在提交读写事务之前故意等待置信区间长度的时间。通过这样，它可以确保任何可能读取数据的事务足够晚发生，避免与先前的事务的置信区间产生重叠。为了保持尽可能短的等待时间，Spanner需要使时钟的误差范围尽可能小，为此，Google在每个数据中心都部署了一个GPS接收器或原子钟，保证所有时钟同步在约 7ms 内完成。

**借助时钟同步来处理分布式事务语义**，除了 Google 以外，目前主流数据库都没有更多的实现。
